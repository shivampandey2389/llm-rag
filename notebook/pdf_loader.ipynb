{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "81a93a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3448b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files\n",
      "Loading 2509.18094v3.pdf\n",
      "Loaded 23 pages from 2509.18094v3.pdf\n",
      "Loading 2410.17725v1.pdf\n",
      "Loaded 9 pages from 2410.17725v1.pdf\n",
      "Total documents loaded: 32\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_documents(pdf_directory):\n",
    "  all_docs=[]\n",
    "  pdf_dir = Path(pdf_directory)\n",
    "  pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "  print(f\"Found {len(pdf_files)} PDF files\")\n",
    "  for pdf_file in pdf_files:\n",
    "    print(f\"Loading {pdf_file.name}\")\n",
    "    try:\n",
    "      loader = PyPDFLoader(pdf_file)\n",
    "      document = loader.load()\n",
    "      for doc in document:\n",
    "        doc.metadata['source_file'] = pdf_file.name\n",
    "        doc.metadata['file_type'] = 'pdf'\n",
    "      \n",
    "      all_docs.extend(document)\n",
    "      print(f\"Loaded {len(document)} pages from {pdf_file.name}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading {pdf_file.name}: {e}\")\n",
    "    \n",
    "  return all_docs\n",
    "\n",
    "pdf_documents = load_pdf_documents(\"../data/pdf\")\n",
    "print(f\"Total documents loaded: {len(pdf_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "80a81940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='UniPixel: Unified Object Referring and\\nSegmentation for Pixel-Level Visual Reasoning\\nYe Liu1,2, Zongyang Ma2,3, Junfu Pu2, Zhongang Qi4, Yang Wu5,\\nYing Shan2, Chang Wen Chen1∗\\n1 The Hong Kong Polytechnic University 2 ARC Lab, Tencent PCG\\n3 Institute of Automation, Chinese Academy of Sciences 4 vivo Mobile Communication Co.\\n5 MindWingman Technology (Shenzhen) Co., Ltd.\\ncoco.ye.liu@connect.polyu.hk\\nhttps://polyu-chenlab.github.io/unipixel/\\nReasoning Segmentation (ReasonSeg)\\nReferring Expression Segmentation (RES)\\nInteractive Segmentation (IS)\\nReasoning Video Object Segmentation (ReVOS)\\nReferring Video Object Segmentation (RVOS)\\nMotion-Grounded Video Reasoning\\nReferred Video Description\\nReferred Video Question-Answering\\nFind the empty chair that is to the left of the main sitting down.Segment the zebra standing in the middle of the frame.Locate the area that a cyclist uses to navigate in the city.Find the place where patients lie down to receive examination.Please segment and track the marked <region>.\\nPlease segment and track the hopping rabbit that leaped from the other one in the video.Where are the utensils used for drinking? Answer with masks.Q: Who shook off and scored?A: The man in red pants.\\nA: This police officer is a middle-aged man with a beard, wearing a blue uniform shirt, a black hat, and glasses. He stopped a blue vintage car and talked to the driver sitting inside.Q: Please describe the <region>. Q: If <region> continues his breakdance routine, what is a likely future event?A: He will perform more complex and varied breakdance moves.\\nPixel-Level Video Question Answering (PixelQA) — Joint Referring + Segmentation + QA in Videos\\nQ: How does the behavior of [1] differ from that of [2]? Why?\\n1\\n2\\n 3\\n4\\nA: [1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering food to [2] and [3]. This might because [1] doesn’t like the food from [4].\\n1\\n2\\nFigure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding\\ntasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and\\nreferred video description & question answering. It can also handle a novelPixelQAtask that jointly\\nrequires object-centric referring, segmentation, and question answering in videos.\\nAbstract\\nRecent advances in Large Multi-modal Models (LMMs) have demonstrated their\\nremarkable success as general-purpose multi-modal assistants, with particular\\nfocuses on holistic image- and video-language understanding. Conversely, less\\nattention has been given to scaling fine-grained pixel-level understanding capabili-\\nties, where the models are expected to realize pixel-level alignment between visual\\n∗Corresponding author.\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2509.18094v3  [cs.CV]  27 Oct 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='signals and language semantics. Some previous studies have applied LMMs to\\nrelated tasks such as region-level captioning and referring expression segmentation.\\nHowever, these models are limited to performing either referring or segmentation\\ntasks independently and fail to integrate these fine-grained perception capabilities\\ninto visual reasoning. To bridge this gap, we proposeUniPixel, a large multi-modal\\nmodel capable of flexibly comprehending visual prompt inputs and generating\\nmask-grounded responses. Our model distinguishes itself by seamlessly integrating\\npixel-level perception with general visual understanding capabilities. Specifically,\\nUniPixel processes visual prompts and generates relevant masks on demand, and\\nperforms subsequent reasoning conditioning on these intermediate pointers during\\ninference, thereby enabling fine-grainedpixel-level reasoning. The effectiveness\\nof our approach has been verified on 10 benchmarks across a diverse set of tasks,\\nincluding pixel-level referring/segmentation and object-centric understanding in\\nimages/videos. A novelPixelQAtask that jointly requires referring, segmentation,\\nand question answering is also designed to verify the flexibility of our method.\\n1 Introduction\\nLarge Multi-modal Models (LMMs) have been the de facto standard for developing general-purpose\\nassistants. By effectively aligning multi-modalities with language, their significance has been\\ndemonstrated across various applications, including multi-modal analysis [ 59, 19, 107, 108, 48],\\nautonomous driving (AD) [16, 79, 106, 11], and Embodied AI [111, 22, 30, 99].\\nIn the field of visual-language understanding, efforts have been dedicated to developingholistic\\nunderstanding models, where simple projection layers between visual encoders and LLMs are utilized\\nto bridge vision and language modalities. Supported by large-scale alignment pre-training and\\nvisual instruction tuning, such a straightforward paradigm achieves strong performance in holistic\\nunderstanding tasks such as captioning [ 40, 6, 114] and general question answering [ 36, 24, 54,\\n49]. However, these models exhibit two fundamental limitations in fine-grained scenarios.First,\\ntheir interactions with users are limited to text format, lacking support for more intuitive forms of\\ncommunication such as drawing points/boxes as references or grounding model responses with key\\nregions represented by masks.Second, the internal reasoning process of these models predominantly\\noperates at a coarse level, directly perceiving the entire content rather than reasoning over specific\\nobjects/regions, making them hard to understand fine-grained details. Some previous studies have\\nexplored the application of LMMs to related tasks such as region-level captioning [ 12, 102, 103],\\nreferring expression segmentation [ 29, 55, 41, 21, 71, 62], and reasoning segmentation [ 32, 27,\\n96, 4, 112]. Nevertheless, their models are limited to performing either referring or segmentation\\ntasks independently via rigidly defined input/output templates (e.g., “It’s <SEG>.” in LISA [ 32]),\\nlacking the flexibility to comprehend user-referred concepts and generate mask-grounded responses\\nsimultaneously. More importantly, these methods cannot integrate such fine-grained perception\\ncapabilities with their original human-like [ 90, 89, 88, 92, 91] multi-modal reasoning abilities,\\nresulting in degraded performance on general visual understanding benchmarks [100, 93, 28].\\nIn this work, we seek to bridge this gap by introducingUniPixel, a large multi-modal model that\\ncan flexibly comprehend visual prompt inputs (i.e., points, boxes, and masks) and generate mask-\\ngrounded responses. Our model significantly differentiates itself from existing ones by unifying\\nthe internal representations of referred and segmented objects via a novelobject memory bank,\\nwhich is a hashmap storing the spatial-temporal information of object-of-interests. During inference,\\nUniPixel initializes the object memory bank and updates it on demand by adding object-centric\\ninformation according to the context. The model responses are then generated conditioning on the\\nfine-grained object memory. Benefits from such unification, UniPixel is able to perform not only basic\\nreferring/segmentation tasks, but also flexiblepixel-level reasoningtasks that require simultaneous\\nvisual prompt comprehension and mask prediction. As illustrated in Fig. 1 (the last row), given a\\nvideo2, a question, and optionally a visual prompt (e.g., a point specified by a click on an object in\\nany frame), UniPixel can (1) infer the mask for the referred object in the corresponding frame, (2)\\npropagate it to all video frames containing the same instance, (3) extract the mask-grounded object\\nfeatures, and finally (4) answer the question conditioning on both the video-level and object-centric\\n2Images are treated as single-frame videos, thus we do not explicitly differentiate them in this work.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='LLM\\nBox/Mask Encoder\\nPoint/Box/Mask Prompt\\nMask Decoder\\nIt shows a...\\nLLM\\nImage/Video\\nSure. It’s <SEG>.\\nPrompt Encoder\\n[3] is less interested in [2] compared with [1].\\nLLM\\nMasks\\nMask Decoder\\nHow does the behavior of [3] differ from [1]?\\n12\\n3\\nDescribe this region.\\nSegment the bunny.\\n(a) Referring-only Models(b) Segmentation-only Models(c) Unified Pixel-level Reasoning Model\\nUpdate\\n(e.g., Osprey, GPT4RoI, VideoRefer) (e.g., LISA, PixelLM, VISA)\\nObject Memory Bank\\nFigure 2:Schematic comparison between UniPixel and its counterparts.To the best of our knowl-\\nedge, UniPixel is the first unified method supporting simultaneous object referring and segmentation.\\ninformation. All these operations are seamlessly conductedwithin a single model, eliminating the\\nneed for external frame samplers [96], mask generators [102, 103], or object trackers [4].\\nWe evaluate the effectiveness of UniPixel from two aspects,i.e., basic referring/segmentation ca-\\npabilities and flexible pixel-level reasoning capabilities. For the first aspect, we conduct extensive\\nexperiments on 10 public benchmarks across 9 image/video referring/segmentation tasks. Our method\\nachieves state-of-the-art performance in diverse scenarios. Notably, on the challenging video reason-\\ning segmentation and referred video QA tasks, our 3B model obtains62.1 J&F on ReVOS [96] and\\n72.8%Acc on VideoRefer-Bench Q [103], surpassing strong counterparts with 7B ∼13B parameters.\\nFurther ablation studies also demonstrate the mutual reinforcement effect of referring and segmenta-\\ntion. For the second aspect, we introduce a novelPixelQAtask that jointly requires object-centric\\nreferring, segmentation, and QA in videos, which cannot be handled by existing methods. UniPixel\\nestablishes a strong baseline for this novel setting. Our contributions are summarized below:\\n1. We proposeUniPixel, a unified large multi-modal model that supports flexible object referring\\nand segmentation in images and videos, via a novelobject memory bankdesign.\\n2. Our model achieves state-of-the-art performance on 10 public benchmarks across 9 refer-\\nring/segmentation tasks, verifying themutual reinforcement effectof such unification.\\n3. We also introduce a novelPixelQAtask that jointly requires object-centric referring, segmen-\\ntation, and QA in videos, where UniPixel establishes a strong baseline for this setting.\\n2 Related Work\\nLarge Multi-modal ModelsThe remarkable success of large multi-modal models (LMMs) has\\nshifted the paradigm of visual-language understanding from close-ended experts to open-ended task\\nsolvers. Early attempts [43, 42, 17, 115] involve an MLP projector or Q-Former [34] to align visual\\nencoders to LLMs, enabling open-ended tasks such as visual question answering. With advanced\\ndesigns such as dynamic resolution and data augmentation, open-source models,e.g., Qwen-VL\\n[2, 77, 3] and InternVL [14, 74, 13] series, have narrowed the gap with advanced proprietary models\\nlike the GPT [58, 59] and Gemini families [67, 18]. Recent studies [60, 25, 51, 37, 48] also explore\\ntest-time scaling on visual-language understanding. However, these methods are spatially coarse-\\ngrained. UniPixel can also be regarded as an object-centric test-time scaling approach, where key\\nobjects are first segmented then encoded to facilitate the subsequent reasoning process.\\nVisual Referring and SegmentationTo meet the growing demand for fine-grained visual under-\\nstanding [50, 46, 47, 45, 84], recent efforts have focused on enhancing LMMs with object referring\\nand segmentation capabilities, as compared in Fig. 2. LISA [32] is a representative model that enables\\nLMM-based segmentation by integrating SAM [ 31] as its decoder. They also introduced a novel\\nreasoning segmentation task, requiring models to perform segmentation based on implicit queries.\\nOther works in this direction [104, 69, 110, 65, 27] have explored advanced mask decoders, more\\nflexible tasks, and larger-scale datasets. Recent studies have also extended these capabilities to videos\\n[4, 96, 101]. Additionally, some research has examined regional understanding through boxes [12]\\nand masks [102, 103]. While recent approaches attempt to unify these two capabilities, they either\\nsupport only images [65] or rely on sub-optimal, tool-based pipelines [23]. To the best of knowledge,\\nUniPixel is the first end-to-end method unifying object referring and mask prediction.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='LLM\\nMask Decoder\\nVisual Encoder\\nPoint Encoder\\nBox Encoder\\nMask Encoder\\n Object Memory Bank\\nPrompt Encoder\\nMemory Injection\\nIn this video, how does the behavior of [1] differ from [2]and[3]? [1] appears disinterested and focuses on nibbling on the ground, while [2]is engaging with [4], who is offering some food to [2]and [3].\\nMasked Crop\\nVisual Tokens\\n Update Memory\\n<REF>Tokens\\n<SEG>Tokens\\n<MEM>Tokens\\nVisual TokenText Token\\nTokenizer\\nFigure 3:The architecture of UniPixel.Given a video, a question, and visual prompts, the model\\nencodes them into tokens via the visual encoder, prompt encoder, and tokenizer, respectively, then\\npredicts a spatial-temporal mask for each visual prompt via the mask decoder. The masks are updated\\ninto the object memory bank, and subsequently injected into the prompt for pixel-level reasoning.\\n3 Method\\nProblem FormulationWe provide a unified definition for pixel-level reasoning tasks. Formally,\\nthe inputs are an image or a video X, a text prompt T , and N optional visual prompts {Pi}N\\ni=1 where\\neach Pi could be a point, box, or mask on a specific frame. The outputs are textual responses to\\nthe prompt with K grounded spatial-temporal masks {Mi}K\\ni=1. Here, both N and K could be zero\\n(degenerating to a normal visual understanding task) and K is not necessarily equal to N, as the\\nmodel may segment extra objects/regions that are not specified by the visual prompts.\\nOverviewFig. 3 presents an overview of UniPixel. It is built upon the Qwen2.5-VL [ 3] framework,\\nconsisting of an LLM backbone and a ViT-based visual encoder that supports dynamic resolution\\ninputs. Given a video and a text prompt, the model first tokenizes them via the visual encoder and\\ntext tokenizer, then sends them into the LLM for response generation. To boost this framework from\\nholistic-level to pixel-level, we introduce (1) aprompt encoder(Sec.3.1) supporting three types of\\nvisual prompts, (2) anobject memory bank(Sec.3.2) for storing object information and injecting it\\ninto the response generation process, and (3) amask decoder(Sec.3.3) for generating spatial-temporal\\nmasks. We also extend the LLM’s vocabulary by adding <REF>, <MEM>, and <SEG> tokens. The\\nformer two serve as placeholders in the input prompt that would be replaced by visual prompt and\\nmemory tokens, respectively, while the<SEG> token is utilized to trigger and guide the mask decoding\\nprocess. Detailed designs and interactions among these components are illustrated as follows.\\n3.1 Prompt Encoder\\nT (0.3)\\nX1Y1(0.3, 0.2)\\nLinear\\nLinear\\nGELU + Linear\\nX2Y2(0.8, 0.5)\\n Linear\\n1D\\n2D\\n2D\\nTemporal EmbPositional EmbType EmbFourier EmbElement-wise AddChannel-wise Cat\\nFigure 4:Joint positional & temporal encodingfor\\npoint (X1Y1T) and box (X1Y1X2Y2T) prompts.\\nThis module aims to effectively encode each\\nvisual prompt into a single token that can be\\nprocessed by the LLM. We denote a point\\nprompt as a tuple (x, y, t)containing its spa-\\ntial coordinates (x, y)and the corresponding\\nframe indext. For box prompts, it is extended\\nto (x1, y1, x2, y2, t)containing the positions\\nof top-left and bottom-right corners. A mask\\nprompt is densely represented by a 2D binary\\nmask mij ∈ {0,1}with the same shape as\\nthe encoded target frame.\\nFor sparse prompts (points and boxes), as shown in Fig. 4, we encode each position(xi, yi) as the sum\\nof a 2D Fourier embedding [73] and a learnable type embedding (indicating whether it is a single point,\\ntop-left corner, or bottom-right corner). For box prompts, we merge the two positional embeddings\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='by concatenating them along the channel dimension and linearly projecting them back to the original\\nsize. Frame indices are also encoded similarly with 1D Fourier embeddings. The resulting positional\\nand temporal embeddings are concatenated again, and then projected to the LLM’s embedding space\\nvia a GELU→Linear block, such that the sparse coordinates in a point/box are encoded into a\\ncompact high-dimensional token. This design is inspired by [31, 66] with two key differences: (1) the\\nspatial-only embeddings are extended to include temporal information, and (2) the negative points\\nare discarded. For dense prompts (masks), we directly resize the binary masks and apply masked\\npooling on the outputs of the visual encoder. An M →L projector (Linear→GELU→Linear ) is\\nleveraged to project the pooled visual features to the LLM’s embedding space.\\n3.2 Object Memory Bank\\nAlthough sparse prompts contain rich positional and temporal information indicating the objects that\\nusers are referring to, it is still hard for the model to focus on these important regions. Previous studies\\n[12, 102, 103] also confirm that direct region cropping can generally provide better object awareness\\ncompared to positional pointers. To seamlessly integrate such a mechanism while preserving the\\nflexibility of visual prompts (e.g., allow pointing on a single frame instead of drawing complete\\nmasks on all frames), we propose an object memory bank to bridge sparse visual prompts and dense\\nobject masks. This is a hashmap where the keys are object IDs and the values are the corresponding\\nspatial-temporal masks. It is initialized as an empty storage for every chat session, and is dynamically\\nupdated on demand. We define two operations for the object memory bank, namelymemory pre-filling\\nandmemory injection. Below is an example of memory-enhanced multi-round conversation.\\nPrompt 1:How does the behavior of [1]<REF>differ from [2]<REF>and [3]<REF>?\\n<REF>detected, enhancing the prompt with object memory.\\nMemory Pre-filling Response:\\nThe relevant regions for this question are [1] <SEG> [2] <SEG> [3] <SEG> [4] <SEG>. ← 4 objects saved into the memory\\nMemory Injected Prompt:\\nHere is a video with 4 frames denoted as <1> to <4>. The highlighted regions are as follows:\\n[1]: <1><MEM><2><MEM><3><MEM>←This object cannot be seen in the last frame\\n[2]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[3]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[4]: <1><MEM><2><MEM><3><MEM><4><MEM>\\nHow does the behavior of [1] differ from [2] and [3]?\\nResponse 1:[1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering\\nsome food to [2] and [3].\\nPrompt 2:What food is [4] offering?←Users can directly refer to objects in the memory\\nResponse 2:[4] is offering carrots.\\nMemory Pre-fillingThis operation is triggered upon the detection of <REF> tokens in the input\\nprompt, aiming to thoroughly analyze the referred objects and predict their corresponding masks. In\\nthis stage, the model responds with object IDs and <SEG> tokens for the relevant objects according to\\nthe context, and predicts their spatial-temporal masks accordingly. These object-mask pairs are then\\nsaved into the object memory bank.\\nMemory InjectionWe inject the features of the saved objects into the prompt to enhance object-\\nawareness. Similar to the mask prompt encoder described in Sec. 3.1, each frame-level object mask\\nis downsampled to match the resolution of visual tokens. We then apply masked pooling to aggregate\\nobject-centric features. Each frame-level mask is condensed into a single feature token, projected\\nthrough the mask projector, and subsequently utilized to replace the corresponding <MEM> token\\nin the memory-injected prompt. Through thispre-filling and injectionmechanism, object-centric\\ninformation is effectively integrated into the model inference process.\\nWhy using object memory bank?An alternative is directly appending a <SEG> token to each\\n<REF> token, followed by masked pooled features obtained during inference. However, we do not\\nadopt this approach for two reasons: (1) During mask prediction, the <SEG> tokens, due to the\\nunidirectional nature of causal self-attention, are unable to aggregate the full context of the prompt,\\nthereby compromising the quality of predicted masks. (2) By utilizing the object memory bank, we\\ncan effectively decouple regional understanding and mask prediction, allowing each to benefit from\\nreferring and segmentation data during training, thus enhancing both capabilities.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison with state-of-the-art methods on ReVOS [96] val split. The best and second-\\nbest results are markedboldand underlined , respectively.\\nMethod Size\\nReferring Reasoning Overall\\nR\\nJ F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nMTTR [5] – 29.8 30.2 30.0 20.4 21.5 21.0 25.1 25.9 25.5 5.6\\nLMPM [21] – 29.0 39.1 34.1 13.3 24.3 18.8 21.2 31.7 26.4 3.2\\nReferFormer [85] – 31.2 34.3 32.7 21.3 25.6 23.4 26.2 29.9 28.1 8.8\\nLLM-based Generalists\\nLISA [32] 13B 45.2 47.9 46.6 34.3 39.1 36.7 39.8 43.5 41.6 8.6\\nTrackGPT [72] 13B 48.3 50.6 49.5 38.1 42.9 40.5 43.2 46.8 45.0 12.8\\nVISA [96] 13B 55.6 59.1 57.4 42.0 46.7 44.3 48.8 52.9 50.9 14.5\\nHyperSeg [81] 3B 56.0 60.9 58.5 50.2 55.8 53.0 53.1 58.4 55.7 –\\nInstructSeg [82] 3B 54.8 59.2 57.0 49.2 54.7 51.9 52.0 56.9 54.5 –\\nGLUS [39] 7B 56.0 60.7 58.3 48.8 53.9 51.4 52.4 57.3 54.9 17.9\\nViLLa [112] 6B – – – – – – 54.9 59.1 57.0 –\\nSa2V A [101] 4B – – – – – – – – 53.2 –\\nUniPixel(Ours) 3B 62.3 66.7 64.5 57.1 62.1 59.6 59.7 64.4 62.1 19.0\\nUniPixel(Ours) 7B 63.9 67.8 65.8 59.4 63.7 61.5 61.7 65.7 63.7 19.4\\n3.3 Mask Decoder\\nWe adopt SAM 2.1 [ 66] as the mask decoder to disentangle the discrete language modeling and\\ncontinuous mask prediction capabilities. For each <SEG> token, we extract its last-layer hidden\\nstates, downsample them via an L→M projector (architecturally identical to the M→L projector),\\nand reshape them into two tokens. Using two tokens ensures better preservation of object information\\nwhen downsampling from high- to low-dimensional channel space. These tokens prompt the mask\\ndecoder to predict the mask on the first frame, which is then propagated to the other frames.\\n3.4 Model Training\\nThe training loss for UniPixel is a linear combination of language modeling loss and mask decoding\\nlosses [66], including a focal loss and dice loss for mask prediction, a mean-absolute-error (MAE)\\nloss for IoU prediction, and a cross-entropy loss for objectness prediction. The loss weights are set\\nto 1, 100, 5, 5, and 5, respectively. We train the model through a three-stage progressive alignment\\nrecipe. The datasets are listed in Tab. 12. In the first stage, we pre-train the sparse prompt encoder\\nusing 851K regional captioning data. Then, we align the LLM and mask decoder by training the\\nL→M projector on 87K referring segmentation data. In the last stage, we further unfreeze the M→L\\nprojector and mask decoder, and apply LoRA [26] on the visual encoder and LLM. The model is\\njointly trained on a large-scale corpus with around 1M samples for diverse tasks.\\n4 Experiments\\nWe evaluate the effectiveness of UniPixel by conducting extensive experiments across a diverse set of\\nbenchmarks. Specifically, we study the following research questions.\\nQ1. Whether UniPixel is flexible and effective on basic image/video referring and segmentation\\ntasks compared to the corresponding representative methods?\\nQ2. How does it perform on the more challenging PixelQA task, which requires joint referring,\\nsegmentation, and question answering in videos?\\nQ3. What effects does each architectural design contribute? More importantly, does the unified\\nmodeling of referring and segmentation lead to a mutual reinforcement effect?\\nDetailed information about the benchmarks, evaluation metrics, implementation details, and more\\nexperimental results can be found in the appendix.\\n4.1 Q1: Comparison with State-of-the-Arts on Referring and Segmentation Tasks\\nReasoning Video Object SegmentationWe begin with the most challenging ReVOS [ 96] dataset,\\nwhich requires models to predict masks based on implicit text queries demanding complex reasoning\\nabilities based on world knowledge. The results are shown in Tab. 1. Our 3B variant outperforms all\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 2: Comparison with state-of-the-art methods on referring video object segmentation (RVOS)\\nand motion-grounded video reasoning datasets, including MeViS [21] (val), Ref-YouTube-VOS [71]\\n(val), Ref-DA VIS17 [62] (val), and GroundMoRe [20] (test). The best and second-best results are\\nmarkedboldand underlined , respectively.\\nMethod Size\\nMeViS Ref-YouTube-VOS Ref-DA VIS17 GroundMoRe\\nJ F J&F J F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nReferFormer [85] – 29.8 32.2 31.0 61.3 64.6 62.9 58.1 64.1 61.1 11.2 14.3 12.7\\nLMPM [21] – 34.2 40.2 37.2 – – – – – – 12.7 14.0 13.3\\nOnlineRefer [83] – – – – 61.6 65.5 63.5 61.6 67.7 64.8 – – –\\nLLM-based Generalists\\nPixelLM [69] 7B 36.3 41.1 38.7 54.3 55.7 55.0 63.4 70.0 66.7 9.9 10.0 10.0\\nLISA [32] 13B 35.8 40.0 37.9 54.0 54.8 54.4 63.2 68.8 66.0 6.3 6.7 6.5\\nVISA [96] 13B 41.8 47.1 44.5 61.4 64.7 63.0 67.0 73.8 70.4 5.3 4.7 5.9\\nVideoLISA [4] 3.8B 41.3 47.6 44.4 61.7 65.7 63.7 64.9 72.7 68.8 – – –\\nVideoGLaMM [56] 3.8B 42.1 48.2 45.2 65.4 68.2 66.8 73.3 65.6 69.5 – – –\\nViLLa [112] 6B 46.5 52.3 49.4 64.6 70.4 67.5 70.6 78.0 74.3 – – –\\nGLUS [39] 7B 48.5 54.2 51.3 65.5 69.0 67.3 – – – – – –\\nSa2V A [101] 4B – – 46.2 – – 70.0 – – 73.8 – – –\\nMoRA [20] 7B – – – – – – – – – 27.4 26.9 27.2\\nUniPixel(Ours) 3B 50.4 55.7 53.1 68.6 72.3 70.5 70.7 77.8 74.2 36.0 38.7 37.4\\nUniPixel(Ours) 7B 53.2 58.3 55.8 69.5 72.4 71.0 72.7 80.1 76.4 36.5 39.1 37.8\\nTable 3: Comparison with state-of-the-art methods on image referring expression segmentation (RES)\\nand reasoning segmentation datasets, including RefCOCO/+/g [29, 55] and ReasonSeg [32] (val).\\nThe best and second-best results are markedboldand underlined , respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg ReasonSeg\\nval testA testB val testA testB val(U) test(U) gIoU cIoU\\nNon-LLM-based Specialists\\nReLA [41] – 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0 – –\\nX-Decoder [116] – – – – – – – 64.6 – 22.6 17.9\\nSEEM [117] – – – – – – – 65.7 – 25.5 21.2\\nLLM-based Image Generalists\\nNExT-Chat [104] 7B 74.7 78.9 69.5 65.1 71.9 56.7 67.0 67.0 – –\\nPixelLM [69] 7B 73.0 76.5 68.2 66.3 71.7 58.3 69.3 70.5 – –\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6 61.3 62.9\\nGroundhog [110] 7B 78.5 79.9 75.7 70.5 75.0 64.9 74.1 74.6 56.2 –\\nLaSagnA [80] 7B 76.8 78.7 73.8 66.4 70.6 60.1 70.6 71.9 48.8 47.2\\nM2SA [27] 13B 74.6 77.6 71.0 64.0 68.1 57.6 69.0 69.3 – –\\nLLM-based Video Generalists\\nVideoLISA [4] 3.8B 73.8 76.6 68.8 63.4 68.8 56.2 68.3 68.8 61.4 67.1\\nVISA [96] 7B 72.4 75.5 68.1 59.8 64.8 53.1 65.5 66.4 52.7 57.8\\nVitron [23] 7B 75.5 79.5 72.2 66.7 72.5 58.0 67.9 68.9 – –\\nSa2V A [101] 4B 78.9 – – 71.7 – – 74.1 – – –\\nUniPixel(Ours) 3B 80.5 82.6 76.9 74.3 78.9 68.4 76.3 77.0 64.0 56.2\\nUniPixel(Ours) 7B 80.8 83.0 77.4 75.3 80.1 70.0 76.4 77.1 60.5 58.7\\nexisting methods with larger LLMs (including Sa2V A-4B [101] also with SAM 2 decoder), achieving\\n62.1 overall J&F . The 7B model further boosts the performance to 64.0 J&F – an improvement\\nof 12% over the previous state-of-the-art – demonstrating that UniPixel can effectively understand\\nimplicit queries based on its world knowledge, and accurately generate masks as responses.\\nReferring Video Object SegmentationThe performance comparisons on MeViS [ 21], Ref-\\nYouTube-VOS [71], and Ref-DA VIS17 [62] datasets are presented in Tab. 2. UniPixel consistently\\nachieves the best performance among its counterparts. Its advantage is particularly evident on the\\nmore challenging MeViS dataset, where our 3B model outperforms GLUS-7B [39] by around 3.5%,\\nas well as the similarly sized VideoGLaMM-3.8B [56] by 17%. More experimental results on MeViS\\n[21] valu set and Ref-SA V [101] val set are provided in Tab. 4 and Tab. 5, respectively. Ref-SA V\\nfeatures long referring descriptions, large object motion, large camera motion, and heavy occlusion\\ncompared with existing datasets. Given these complex descriptions and video content, our method\\nconsistently performs better than counterparts, including those fine-tuned on the target dataset.\\nMotion-Grounded Video ReasoningWe also evaluate our method on GroundMoRe [20] dataset\\n(results shown in Tab. 2), which highlights visual answer generation that requires joint spatial and\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 4: Experimental results on MeViS [21] valu\\nset. Post means applying post optimization.\\nMethod Size FT J F J&F\\nLMPM [21] –✗ 36.5 43.9 40.2\\nLISA [32] 7B✗ 39.9 46.5 43.2\\nLISA [32] + XMem [15] 7B✗ 41.9 49.3 45.6\\nVideoLISA [4] 7B✗ 48.4 54.9 51.7\\nVideoLISA [4] + Post 7B✗ 50.9 58.1 54.5\\nSa2V A [101] 4B✗ – – 52.1\\nSa2V A [101] 8B✗ – – 57.0\\nUniPixel(Ours) 3B ✗ 56.1 63.2 59.7\\nUniPixel(Ours) 7B ✗ 58.4 65.0 61.7\\nTable 5: Comparison on Ref-SA V [101] val set.\\nFT means fine-tuning after pre-/co-training.\\nMethod Size FT J F J&F\\nUniRef++ [86] –✗ 11.6 9.5 10.5\\nUNINEXT [95] –✗ 8.8 6.4 7.6\\nLMPM [21] –✗ 12.2 9.8 10.3\\nVISA [96] 7B✗ 13.2 11.3 11.8\\nSa2V A [101] 8B✗ 39.6 43.0 41.3\\nUniRef++ [86] –✓ 15.8 13.4 14.6\\nSa2V A [101] 8B✓ 48.3 51.7 50.0\\nUniPixel(Ours) 3B ✗ 66.9 67.6 67.2\\nUniPixel(Ours) 7B ✗ 68.5 69.6 69.0\\nTable 6: Fine-tuned performance on referring expression segmentation (RES) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6\\nGSV A [87] 7B 77.2 78.9 73.5 65.9 69.6 59.8 72.7 73.3\\nOMG-LLaV A [109] 7B 78.0 80.3 74.1 69.1 73.1 63.0 72.9 72.9\\nGLaMM [65] 7B 79.5 83.2 76.9 72.6 78.7 64.6 74.2 74.9\\nSa2V A [101] 4B 80.4 – – 74.3 – – 75.7 –\\nUniPixel(Ours) 3B 81.9 83.5 78.6 75.3 80.3 70.6 77.2 78.5\\nUniPixel(Ours) 7B 83.0 84.9 80.4 77.8 82.3 72.7 78.7 79.7\\nTable 7: Experimental results on referring expression comprehension (REC) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nOFA [78] – 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6\\nShikra [9] 7B 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2\\nMiniGPT-v2 [8] 7B 88.7 91.6 85.3 79.9 85.1 74.4 84.4 84.6\\nVitron [23] 7B 90.9 93.289.3 83.7 89.1 76.9 86.4 87.0\\nUniPixel(Ours) 3B 91.8 93.8 87.5 86.3 90.8 80.3 88.0 88.2\\nUniPixel(Ours) 7B 92.0 94.4 88.1 87.2 91.9 82.1 88.6 88.7\\ntemporal grounding. Note that we mainly compare the results with MoRA [20], which is fine-tuned\\non GroundMoRe while other methods are evaluated under the zero-shot setting. Benefit from the\\nstrong pixel-level reasoning capability, UniPixel significantly performs better than the baseline.\\nReferring Expression Segmentation and Reasoning SegmentationTab. 3 compares the image\\nsegmentation capabilities using explicit and implicit queries. We evaluate our co-trained model on\\nRefCOCO/+/g [29, 55] and ReasonSeg [32]. While state-of-the-art performance has been achieved\\non RES datasets, we observe that the reasoning segmentation data (239 samples) can be easily\\noverwhelmed by the other samples during training due to its limited size. Tab. 6 presents the RES\\nperformance after fine-tuning. We follow the common practice that jointly fine-tunes the model on\\nRefCOCO/+/g datasets [29, 55], and then evaluate on them separately. These results demonstrate the\\ngeneralizability of UniPixel when facing both explicit and implicit queries.\\nReferring Expression ComprehensionOur method also supports referring expression compre-\\nhension by inferring the bounding boxes from predicted masks. Its performance (accuracy with IoU\\n⩾ 0.5) is compared with representative methods in Tab. 7. Benefiting from the high-quality mask\\nprediction, UniPixel can also achieve very competitive performance on this simpler task.\\nReferred Video Description and Question AnsweringWe study UniPixel’s regional understand-\\ning capabilities on VideoRefer-Bench [103], which contains two subsets for description and question\\nanswering tasks. The comparisons are in Tab. 8 and Tab. 9. BQ, SQ, RQ, CQ, and FP denote basic\\nquestions, sequential questions, relational questions, reasoning questions, and future predictions,\\nrespectively. Both tasks leverage mask prompts as inputs, where single-frame and multi-frame modes\\ndenote applying the masks only on a specific frame and on all frames, respectively. UniPixel can\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 8: Comparison with state-of-the-art methods on VideoRefer-Bench D [103]. The best and\\nsecond-best results are markedboldand underlined , respectively.\\nMethod Size\\nSingle-Frame Multi-Frame\\nSC AD TD HD Avg. SC AD TD HD Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B 2.62 1.58 2.19 2.07 2.12 3.09 1.94 2.50 2.41 2.48\\nQwen2-VL [77] 7B 2.97 2.24 2.03 2.31 2.39 3.30 2.54 2.22 2.12 2.55\\nInternVL2 [74] 26B 3.55 2.99 2.57 2.25 2.84 4.08 3.353.08 2.28 3.20\\nGPT-4o-mini [59] – 3.56 2.85 2.87 2.38 2.92 3.89 3.18 2.62 2.50 3.05\\nGPT-4o [59] – 3.34 2.96 3.01 2.50 2.95 4.15 3.31 3.11 2.43 3.25\\nImage Referring LMMs\\nFerret [98] 7B 3.08 2.01 1.54 2.14 2.19 3.20 2.38 1.97 1.38 2.23\\nOsprey [102] 7B 3.19 2.16 1.54 2.45 2.34 3.30 2.66 2.10 1.58 2.41\\nVideo Referring LMMs\\nElysium [75] 7B 2.35 0.30 0.02 3.59 1.57 – – – – –\\nArtemis [63] 7B – – – – – 3.42 1.34 1.39 2.90 2.26\\nVideoRefer [103] 7B 4.41 3.273.03 2.973.42 4.443.27 3.10 3.043.46\\nUniPixel(Ours) 3B 4.04 3.15 3.10 3.37 3.42 4.08 3.13 3.13 3.42 3.44\\nUniPixel(Ours) 7B 3.83 3.07 2.96 3.62 3.37 3.82 3.05 3.01 3.57 3.36\\nTable 9: Comparison with state-of-the-art methods on\\nVideoRefer-BenchQ [103] (mask prompts). MF denotes\\nmulti-frame mode. Full question types are in Sec. 4.1.\\nMethod Size MF BQ SQ RQ CQ FP Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B✗ 58.7 62.9 64.7 87.4 76.3 67.4\\nQwen2-VL [77] 7B✗ 62.0 69.6 54.9 87.3 74.6 66.0\\nInternVL2 [74] 26B✗ 58.5 63.5 53.4 88.0 78.9 65.0\\nGPT-4o-mini [59] –✗ 57.6 67.1 56.5 85.9 75.4 65.8\\nGPT-4o [59] –✗ 62.374.5 66.088.0 73.7 71.3\\nImage Referring LMMs\\nFerret [98] 7B✗ 35.2 44.7 41.9 70.4 74.6 48.8\\nOsprey [102] 7B✗ 45.9 47.1 30.0 48.6 23.7 39.9\\nVideo Referring LMMs\\nVideoRefer [103] 7B✗ 75.468.6 59.389.478.1 71.9\\nUniPixel(Ours) 3B ✗ 73.6 70.3 60.7 88.8 78.0 72.2\\nUniPixel(Ours) 7B ✗ 68.9 73.1 64.7 88.8 83.3 73.4\\nVideoRefer [103] 7B✓ – 70.6 60.5 – – 72.1\\nUniPixel(Ours) 3B ✓ 75.3 70.7 62.3 87.4 77.2 72.8\\nUniPixel(Ours) 7B ✓ 70.6 74.6 64.7 88.8 82.5 74.1\\nTable 10: Evaluation results on our newly\\nintroduced PixelQA task. All the visual\\nprompts are applied in a single frame. See\\nSec. 4.2 for detailed settings.\\nMethod Size J F J&F Acc\\nPoint Prompts\\nInternVL2 [74] 26B – – – 60.8\\nQwen2-VL [77] 72B – – – 69.3\\nUniPixel(Ours) 3B 57.3 64.4 60.9 71.1\\nUniPixel(Ours) 7B 42.1 47.1 44.6 71.4\\nBox Prompts\\nInternVL2 [74] 26B – – – 61.3\\nQwen2-VL [77] 72B – – – 69.0\\nUniPixel(Ours) 3B 57.8 64.7 61.3 70.3\\nUniPixel(Ours) 7B 41.1 46.4 43.8 71.4\\nMixed (50% Points + 50% Boxes)\\nInternVL2 [74] 26B – – – 60.9\\nQwen2-VL [77] 72B – – – 69.1\\nUniPixel(Ours) 3B 57.2 64.1 60.6 70.8\\nUniPixel(Ours) 7B 42.3 47.5 44.9 71.4\\neffectively comprehend both types of prompts, and accurately respond with object-centric descriptions\\nor answers, surpassing strong models including GPT-4o [59] and VideoRefer [103].\\n4.2 Q2: Pixel-Level Video Question Answering (PixelQA)\\nWe design the new PixelQA task based on VideoRefer-Bench Q [103], where the original mask\\nprompts are replaced with more challenging point or box prompts. Given these ambiguous visual\\ncues, models are expected to correctly identify the target object according to the question and the\\nvisual prompt, then respond withboth the textual answer and the corresponding object masks.\\nWe report the mask prediction J&F and MCQ accuracy in Tab. 10. Note that none of the existing\\nmethods supports this scenario. Thus, we apply set-of-mark prompts [97] directly on video frames,\\nand evaluate the QA accuracies of two strong LMMs [77, 74] as our baselines. Aside from point- or\\nbox-only prompts, we also explore a more flexible setting that randomly chooses different prompts\\nfor different objects. The results verify that ourmemory pre-filling & injectionparadigm effectively\\nenhances the model’s reasoning capabilities. Visualizations of this task are shown in Fig. 5.\\n4.3 Q3: Key Ablation Studies\\nEffect of Task UnificationWe study the effect of task unification in Tab. 11 (a). Unifying referring\\nand segmentation capabilities into a single model and training them jointly leads to better results\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='If [1] continues moving forward, what is a likely future event?(A) The bear will encounter other animals (B) The bear will find a place to rest (C) The bear will start running (D) The bear will climb the stone wall\\nWhat is a likely future event with [1]?(A) He will stop and rest (B) He will start walking slowly (C) He will continue to navigate through more obstacles (D) He will sit down and take a break\\nHow is [4] related to [2]?(A) [4] is holding [2] (B) [4] is controlling [2] with a leash (C) [4] is walking away from [2] (D) [4] is ignoring [2]\\nFigure 5:Visualization of the outputs from UniPixel on PixelQA task. Star marks and boxes\\nrefer to point and box prompts, respectively. The boxed frames denote where the visual prompts are\\napplied. Given different types of visual prompts on a single frame, our method can flexibly infer the\\nrelevant object, track it across the entire video, and involve its features in reasoning.\\nTable 11: Key ablation studies with UniPixel-3B on PixelQA (mixed). See Sec. 4.3 for explanations.\\n(a) Task Unification\\nRefer Segment Memory J&F Acc\\n✓ – 64.6\\n✓ 47.5 –\\n✓ ✓ 48.2 67.4\\n✓ ✓ ✓ 49.0 68.5\\n(b) Object Memory Bank\\nReferring Method J&F Acc\\n①<REF> 46.8 64.5\\n②<REF><SEG> 47.8 64.9\\n③<REF><SEG>+ Pooling 47.5 66.3\\n④Object Memory Bank 49.0 68.5\\n(c) Prompt Encoder & Mask Decoder\\nEncoder Decoder J&F Acc\\nw/o Time – 44.3 63.7\\nw/ Time – 49.0 68.5\\n– Independent 46.1 66.2\\n– Propagation 49.0 68.5\\non both tasks (first three rows), demonstrating themutual reinforcement effectof such unification.\\nIncorporating memory pre-filling as an auxiliary task (last row) brings extra improvements.\\nEffect of Object Memory BankTab. 11 (b) verifies the effectiveness of object memory bank. ①\\nmeans using a single token for each referred object. ② means adding an extra segmentation token to\\nsegment it as an auxiliary task. ③ further appends masked-pooled visual tokens after it. The results\\nshow that (1) both adding auxiliary segmentation task and masked-pooled features help regional\\nunderstanding, and (2) decoupling them via object memory bank can further boost the performance.\\nDesign Space of Prompt Encoder & Mask DecoderWe compare different prompt encoder\\nand mask decoder designs in Tab. 11 (c). The performance significantly drops when the temporal\\nencoding in the prompt encoder is removed (first two rows). For the mask decoder (last two rows),\\nwe explore an alternative strategy that treats video frames independently (as batched images), which\\ncould largely accelerate inference but lead to sub-optimal accuracies. We hypothesize that this is\\nbecause the LLM-generated <SEG> token cannot well-capture the object information in all frames,\\nthus disentangling the segmentation and tracking capabilities to an external module is reasonable.\\n5 Conclusion\\nIn this work, we proposedUniPixel, a large multi-modal model that supports flexible pixel-level\\nvisual reasoning. It unifies the internal representations of referred and segmented objects through\\na novelobject memory bank. We observe that by such unification, the performance of object\\nreferring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Appendix\\nIn this appendix, we provide more details about the training data, model implementation, and experi-\\nmental settings to complement the main paper. Additional analysis, ablation studies, visualizations,\\nand discussions are also incorporated. Below is the table of contents.\\nA.Model\\n1.Implementation Details\\n2.Training Recipe\\nB.Experiments\\n1.Tasks and Benchmarks\\n2.Evaluation Metrics\\n3.More Experimental Results\\n4.Ablation Studies\\n5.Qualitative Results\\nC.Discussions\\n1.Limitations & Future Work\\n2.Potential Societal Impacts\\nD.Licenses\\nA Model\\nA.1 Implementation Details\\nWe instantiate our base models with 3B and 7B versions of Qwen2.5-VL [3]. Both variants employ\\npre-trained SAM 2.1 [66] with Hiera Base+ [70] backbone as the mask decoder. The M→L projector\\nis initialized with the weights from the V→L projector of Qwen2.5-VL. The hidden size inside the\\nprompt encoder is 256. To reduce GPU memory and accelerate training, we randomly sample 8\\nframes per video, with each frame resized to 3162 ∼448 2 pixels (128∼256 tokens per frame). The\\nframe sampling strategies follow the specifications of each benchmark during inference. The mask\\ndecoder has a fixed resolution of 768 ×768. For each segmentation sample, up to 5 objects are\\nrandomly selected to compute the mask prediction losses. During training, LoRA adapters [26] with\\nrank=128 and alpha=256 are applied to all QKVO layers in the visual encoder and LLM. The input\\nsequences are restricted to 4K tokens. We train the model with 8 RTX A6000 Ada (48G) GPUs, with\\na global batch size of 256 for stages 1 and 2, and 32 for stage 3. In the first two stages, the learning\\nrates are set to 1e-3. In the last stage, it is set to 5e-6 for the mask decoder and 2e-5 for all the other\\nparameters, respectively. A linear warmup in the first 3% steps followed by cosine decay is adopted\\nin all stages. The configurations of datasets are introduced in the following section.\\nA.2 Training Recipe\\nThe detailed distribution of training datasets for UniPixel is shown in Tab. 12. Within the three-stage\\ntraining recipe, we first pre-train the sparse prompt encoder using short caption samples from Inst-IT\\n[61] and VideoRefer [103]. For each sample, we randomly select a point inside the ground truth\\nmask (50%) or generate an augmented box from it (50%). This stage aims to enable the model with\\nsimple visual prompt comprehension and regional captioning capabilities on images and videos. In\\nthe second stage, we align the LLM and mask decoder using referring object segmentation datasets\\n[29, 55, 71]. We use short caption/query samples for the first two stages to focus on alignment rather\\nthan knowledge learning. For the last stage, we collect a large-scale, high-quality corpus called\\nUniPixel-SFT-1M3 to jointly train the model on diverse pixel-level tasks. The original annotations\\nhave been rewritten using task-specific templates to incorporate instructions. All the repurposed\\ndatasets and pre-processing pipelines will be publicly available to facilitate future research.\\n3 https://huggingface.co/datasets/PolyU-ChenLab/UniPixel-SFT-1M\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 12: The distribution of training datasets for UniPixel. We use different background colors to\\ndenote object referring , object segmentation , regional understanding , memory pre-filling , and\\ngeneral video understanding data, respectively.\\nStage Dataset\\nInputs Outputs\\n#Samples #Repeat Ratio\\nText Image Video Point Box Mask Text Mask\\n1 Inst-IT-Image-Short-Caption [61] ✓ ✓ ✓ ✓ ✓ 351K 1 41.2%\\nVideoRefer-Short-Caption [103] ✓ ✓ ✓ ✓ ✓ 500K 1 58.8%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\n2 RefCOCOg [55] ✓ ✓ ✓ ✓ 22K 5 26.8%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 5 22.0%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 3 9.5%\\n3\\nOsprey-Conversation [102] ✓ ✓ ✓ ✓ 1.4K 5 0.1%\\nOsprey-Detail-Description [102] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nOsprey-Pos-Neg [102] ✓ ✓ ✓ ✓ 20K 5 1.7%\\nVideoRefer-Detailed-Caption [103] ✓ ✓ ✓ ✓ 120K 5 10.1%\\nVideoRefer-QA [103] ✓ ✓ ✓ ✓ 69K 5 5.8%\\nInst-IT-Video-QA [61] ✓ ✓ ✓ ✓ 159K 5 13.4%\\nVideoRefer-QA-Memory [103] ✓ ✓ ✓ ✓ ✓ ✓ 69K 3 3.5%\\nInst-IT-QA-Memory [61] ✓ ✓ ✓ ✓ ✓ ✓ 158K 3 8.0%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCOg [55] ✓ ✓ ✓ ✓ 22K 10 3.7%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 10 3.0%\\nReasonSeg [32] ✓ ✓ ✓ ✓ 1.6K 10 0.3%\\nADE20K [113] ✓ ✓ ✓ ✓ 20K 3 1.0%\\nCOCOStuff [7] ✓ ✓ ✓ ✓ 118K 3 6.0%\\nMapillary Vistas [57] ✓ ✓ ✓ ✓ 18K 3 0.9%\\nPACO-LVIS [64] ✓ ✓ ✓ ✓ 46K 3 2.3%\\nPASCAL-Part [10] ✓ ✓ ✓ ✓ 4.4K 3 0.2%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 5 1.1%\\nRef-DA VIS17 [62] ✓ ✓ ✓ ✓ 0.6K 10 0.1%\\nRef-SA V [101] ✓ ✓ ✓ ✓ 56K 3 2.8%\\nMeViS [21] ✓ ✓ ✓ ✓ 23K 5 1.9%\\nLV-VIS [76] ✓ ✓ ✓ ✓ 11K 3 0.6%\\nViCaS [1] ✓ ✓ ✓ ✓ 41K 3 2.1%\\nReVOS [96] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nGroundMoRe [20] ✓ ✓ ✓ ✓ 5.6K 3 0.3%\\nLLaV A-1.5-Mix-665K [42] ✓ ✓ ✓ 647K 1 10.9%\\nVideoGPT+ Instruct [52] ✓ ✓ ✓ 573K 1 9.7%\\nB Experiments\\nB.1 Tasks and Benchmarks\\nOur method is extensively evaluated across 9 fine-grained image/video understanding tasks. The\\nbenchmark(s) used for each task are listed as follows:\\n1.Reasoning Video Object Segmentation:ReVOS [96]\\n2. Referring Video Object Segmentation:MeViS [ 21], Ref-YouTube-VOS [71], Ref-DA VIS17 [62], Ref-SA V [101]\\n3.Motion-Grounded Video Reasoning:GroundMoRe [20]\\n4.Referring Expression Segmentation:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n5.Reasoning Segmentation:ReasonSeg [32]\\n6.Referring Expression Comprehension:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n7.Referred Video Description:VideoRefer-Bench D [103]\\n8.Referred Video Question Answering:VideoRefer-Bench Q [103]\\n9.Flexible Pixel-Level Understanding:PixelQA (Ours)\\nB.2 Evaluation Metrics\\nFor video segmentation tasks, we adopt J&F as the main metric to jointly consider region similarity\\nJ and contour accuracy F. Image segmentation is evaluated using cIoU (the cumulative intersection\\nover the cumulative union) and gIoU (the average of all per-image IoUs) following existing work. For\\nreferred video description and question answering tasks, we follow the official evaluation protocols to\\nreport GPT-4o [59] scores and MCQ accuracy, respectively. For referring expression comprehension,\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 13: Performance comparison on general video question answering (VideoQA) on MVBench\\n[36]. Note that UniPixel is the only model supporting pixel-level referring & segmentation.\\nModel Size AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg.\\nGPT-4V [58] – 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.059.011.0 43.5\\nVideo-ChatGPT [53] 7B 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 39.5 48.5 29.0 33.0 29.5 26.0 35.5 32.7\\nVideo-LLaMA [105] 7B 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1\\nVideoChat [35] 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5\\nVideo-LLaV A [38] 7B 46.0 42.5 56.5 39.0 53.5 53.0 48.0 41.0 29.0 31.5 82.5 45.0 26.0 53.0 41.5 33.5 41.5 27.5 38.5 31.5 43.0\\nTimeChat [68] 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5\\nPLLaV A [94] 7B 58.0 49.0 55.5 41.0 61.0 56.0 61.0 36.0 23.5 26.0 82.0 39.5 42.0 52.0 45.0 42.0 53.5 30.5 48.0 31.0 46.6\\nST-LLM [44] 7B 66.0 53.584.044.0 58.5 80.5 73.5 38.5 42.5 31.0 86.5 36.5 56.5 78.5 43.0 44.5 46.5 34.5 41.5 58.5 54.9\\nVideoGPT+ [52] 4B 69.0 60.0 83.0 48.5 66.5 85.575.536.0 44.0 34.0 89.5 39.5 71.0 90.5 45.0 53.0 50.0 29.5 44.0 60.0 58.7\\nVideoChat2 [36] 7B 75.558.0 83.5 50.560.5 87.5 74.5 45.047.544.082.5 37.0 64.5 87.5 51.0 66.547.035.037.0 72.5 60.4\\nUniPixel(Ours) 3B 69.5 62.5 83.0 48.5 76.5 86.5 66.5 38.0 49.0 40.5 87.0 49.0 74.0 95.0 49.0 45.0 63.5 34.5 58.0 73.5 62.5\\nUniPixel(Ours) 7B 71.0 68.0 84.0 45.0 78.0 91.5 66.5 35.5 57.5 43.0 91.5 47.0 73.5 92.5 58.0 53.0 74.0 37.5 49.0 69.0 64.3\\nTable 14: Effectiveness justification of multi-stage training. The best and second-best results are\\nmarkedboldand underlined , respectively. The three-stage recipe leads to optimal performance.\\nStage 1 Stage 2 Stage 3\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ 58.3 63.6 61.0 54.8 61.9 58.4 71.1 71.5\\n✓ ✓ 59.0 63.4 61.2 55.2 62.1 58.7 71.8 72.3\\n✓ ✓ 59.6 63.5 61.6 55.7 62.5 59.1 71.2 71.6\\n✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nwe leverage mean accuracies, where a predicted bounding box is considered correct when it has the\\nintersection over union (IoU) with the ground truth no less than 0.5.\\nB.3 More Experimental Results\\nGeneral Video Question AnsweringWe also evaluate UniPixel on MVBench [ 36] to compare its\\ngeneral video understanding capabilities with existing methods. The results are illustrated in Tab. 13.\\nNote that our method is the only one in the table that supports referring and segmentation. By jointly\\ntraining on holistic-level and pixel-level data, UniPixel can effectively balance the capabilities under\\nboth scenarios, demonstrated by the strong performance compared with holistic-level models.\\nB.4 Ablation Studies\\nEffect of Multi-stage TrainingWe investigate the effectiveness of multi-stage training in Tab. 14.\\nAs shown in the first line, directly training the model using large-scale data only leads to sub-optimal\\nperformance, due to the unaligned representations among prompt encoder, LLM, and mask decoder.\\nWe observe that pre-training either the sparse prompt encoder or the L→M projector (the second and\\nthird lines) brings performance gains on both tasks (referring and segmentation). We hypothesize that\\nthis is because pre-aligning either of them can alleviate the burden of joint-task learning in stage 3.\\nThe last row verifies that the performance can be further boosted by pre-aligning both of them.\\nNumber of Hidden Tokens for Mask DecoderAs mentioned in the main paper, there is a huge\\ngap between the feature dimensions of the LLM and the mask decoder, thus splitting the<SEG> token\\ninto more hidden tokens can better preserve the object information from the LLM. We ablate this\\nmechanism in Tab. 15. According to the results, using only 1 hidden token cannot fully preserve the\\nobject information, as the mask prediction performance is sub-optimal. However, we also observe\\nthat using more than 2 hidden tokens (e.g., 4 or 8) only brings negligible performance gain. Therefore,\\nwe choose 2 hidden tokens per object in our final model.\\nTraining Strategy for the M→L projectorThe M →L projector aims to project the masked-pooled\\nobject-centric features to the LLM’s embedding space. Since the object features originate from the\\nvisual encoder, it is possible to re-use the pre-trained weights of the original V →L projector in\\nQwen2.5-VL. Its effects are studied in Tab. 16. We investigated two strategies: 1) re-using the\\nweights and 2) adding an extra pre-training stage for better alignment. The comparison shows that\\ndirectly re-using weights without extra pre-training can achieve the best results.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 15: Ablation study on the number of hid-\\nden tokens for each <SEG>. Performance gains\\nare negligible with more than 2 tokens/object.\\n#Tokens\\nReVOS MeViS(val u)\\nJ F J&F J F J&F\\n1 59.6 63.5 61.6 55.8 62.5 59.2\\n2 59.7 64.4 62.1 56.1 63.2 59.7\\n4 59.8 63.9 61.9 56.863.1 59.9\\n8 59.5 64.0 61.8 56.4 62.8 59.6\\nTable 16: Ablation study on M→L projector. Init\\nand PT denote weight initialization from V→L\\nprojector and extra pre-training, respectively.\\nInit PT\\nVideoRefer-BenchQ PixelQA\\nSingle-Frame Multi-Frame Mixed Acc\\n71.4 71.9 67.7\\n✓ 71.5 71.7 67.4\\n✓ 72.4 72.6 68.2\\n✓ ✓ 72.2 72.8 68.5\\nTable 17: Ablation study on training data used in stage 3. The best and second-best results are marked\\nboldand underlined, respectively. Gradually adding more pixel-level data brings performance gains.\\nRegional Segmentation Memory General\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ – – – – – – 72.1 72.0\\n✓ 58.9 63.8 61.4 56.0 63.2 59.6 – –\\n✓ ✓ 59.2 63.7 61.5 55.8 63.1 59.5 72.3 72.6\\n✓ ✓ ✓ 59.6 64.5 62.1 56.3 63.5 59.9 72.472.5\\n✓ ✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nCombination of Training DataTab. 17 studies the effect of the combination of multi-task co-\\ntraining data in stage 3. Compared with training only on the regional or segmentation data, leveraging\\nboth of them leads to considerable performance on both tasks. Incorporating memory pre-filling data\\n(requiring both referring and segmentation) can further boost the performance. We also mix some\\ngeneral holistic-level video understanding data to preserve the original capabilities of the pre-trained\\nmodel, while it slightly affects the performance on pixel-level tasks.\\nB.5 Qualitative Results\\nFig. 6∼11 present more visualizations of outputs from UniPixel on different pixel-level understanding\\ntasks. Our method can effectively handle flexible visual prompts [103], implicit queries [32, 96], long\\nqueries [101], and motion-grounded questions [20].\\nC Discussion\\nC.1 Limitations & Future Work\\nDue to the limited computing resources, we did not further scale up the training data to incorporate\\nmore pixel-level tasks such as grounded caption generation (GCG) on images [65] or videos [56],\\nwhich are interesting scenarios and their data may bring more performance gains. Besides, the mask\\ndecoder currently predicts the first mask on the first frame and propagates it to the following frames,\\nwhile it potentially supports predicting on the best frame (defined as the frame with the best view of\\nthe target) and propagates it to both sides of the video. We will focus in our future work to explore\\nmore pixel-level understanding tasks and more flexible mechanisms for the mask decoder.\\nC.2 Potential Societal Impacts\\nThis work introduces a new framework for pixel-level visual-language understanding, which could\\npotentially be used in education, surveillance, and healthcare industries, where flexible interactions\\nwith the users and fine-grained understanding of images & videos are required. In other scenarios\\nrequiring multi-modal assistants, our method can also serve as a more advanced alternative. To the\\nbest of our knowledge, there are no potential negative societal impacts to declare.\\nD Licenses\\nOur model is built based on the pre-trained Qwen2.5-VL [3] and SAM 2.1 [66] models. They are both\\nlicensed under the Apache License 2.0 (https://www.apache.org/licenses/LICENSE-2.0).\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content=\"If [1] continues to move forward, what is a likely future event involving [2]?(A) [2] will run away (B) [2] will sit down and stop moving (C) [2] will start barking (D) [2] will continue walking by the wheelchair\\nWhat action does [1] perform that involves [3]?(A) [1] extends an arm across [3]'s chest (B) [1] hands something to [3] (C) [1] talks to [3] (D) [1] ignores [3]\\nWhat is [1] wearing?(A) Blue sweatshirt and black jeans (B) Red sweatshirt and light blue jeans (C) Green t-shirt and white pants (D) Yellow hoodie and dark blue jeans\\nIf <object1><region> continues riding the bike, what is a likely future event?(A) [1] will stop (B) [1] will start running (C) [1] will change a different outfit (D) [1] will continue to challenge different high difficulty movements\\nFigure 6: Visualization of the predictions from UniPixel on PixelQA.\\nPlease segment the zebra which is younger in this video.\\nPlease segment the cow that is the furthest from the camera in this video.\\nWhich goldfish is on the left side of the screen at the beginning of the video? Please provide the segmentation mask.\\nCan you find the skunk that has black fur all over its body and a tuft of white fur on its head and the tip of its tail?\\nWhere is the instrument that serves to shield from the sun or protect from rain and snow?\\nWhich ferret(s) is/are being licked by a cat consistently? Please provide the segmentation mask.\\nCan you segment the insect(s) belonging to the superfamily Papilionoidea of the Lepidoptera order in this video?\\nFigure 7: Visualization of the predictions from UniPixel on ReVOS [96].\\n15\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Where is the man wearing a cap and shorts in this video? Respond with the segmentation mask.\\nCan you find the blue wooden car in the frames?\\nSegment and track the green motorbike in this video.\\nPlease segment the black swan in this video.\\nWhere is the rope? Give me the segmentation results directly.\\nFigure 8: Visualization of the predictions from UniPixel on Ref-DA VIS17 [62].\\nQ: Who might not open the cooler if not for feeding the walrus a fish?                                                       A: The woman.\\nQ: Who opens the ziploc bag to transfer the crushed Oreo cookies into the bowl?                                                  A: The girl.\\nQ: Who dribbles the ball before he shoots it?                                                                      A: The man in the black shorts.\\nQ: Who asked if the little girl could carry the box before she picked it up?                                                 A: The man.\\nQ: What might not be given to the woman by the man if he did not eat by himself? A: The bag.\\nQ: Who kicks the ball into the goal? A: The boy.\\nFigure 9: Visualization of the predictions from UniPixel on GroundMoRe [20].\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content=\"Findtheobjectaccordingtothedescription:Theobjectisadark-coloredbackpackwithlight-coloredaccents,featuringmultiplecompartmentsandpockets,securelyfastenedtoanindividual'sback.Thepersonisdressedindarkclothingandascendinganescalatorinapublicsetting,likelyamallortransportationhub.Thebackpackhasadjustablestrapsandatophandle,appearingfunctionalforcarryingvariousitems.Theindividualmovessteadilyuptheescalator,indicatingapurposefuljourney.\\nAnalyzethefollowingsentencesandprovidethecorrespondingsegmentationmask:Theobjectisadark-coloredsedan,likelyblueorblack,parkedonanunpavedsurface,possiblyadirtroadoranareawithloosesoil.Ithasfourdoors,avisiblerearspoileronthetrunk,silverwheels,andtintedwindows.Thecarisslightlytilted,suggestingitmightbeparkedonunevengroundorexperiencingsomeformofimbalance.Throughoutthevideo,thesedanremainsstationary,withnoindicationofmovementoractionsbeingperformedbythevehicle.\\nPleasesegmenttheobjectaccordingtothedescription:Theobjectisapersonwithlongdarkhair,wearingadarktopandapatternedskirtwithgeometricdesigns.Thisindividualisstationaryormovingveryslowlyinthebackgroundofaretailstore,possiblyafurnitureorhomegoodsstore.Thepersonremainsincloseproximitytoanothershopperpushingashoppingcart,suggestingtheymightbetogetherorinteracting.Thescenecapturesatypicalshoppingexperience.\\nFigure 10: Visualization of the predictions from UniPixel on Ref-SA V [101].\\nFind the lens that is more suitable for photographing nearby objects.\\n Where is the goat nearest to the bottom stone? Give me the segmentation mask.\\nIn some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture?\\nWhat item in the picture can provide information to help guide travelers through this rugged terrain that can be challenging to navigate?\\nPlease localize the place where piano players should sit in this image.\\nWhere is the place where the garbage should be put? Please respond with the segmentation mask.\\nWhich part of the vehicle must be used to display identifying information as required by law? Segment the target directly.\\nSegment the place where the patient lies down to receive examination in this image.\\nFigure 11: Visualization of the predictions from UniPixel on ReasonSeg [32].\\n17\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Ali Athar, Xueqing Deng, and Liang-Chieh Chen. Vicas: A dataset for combining holistic and pixel-level\\nvideo understanding using captions with grounded segmentation.arXiv:2412.09754, 2024.\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\\nand Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text\\nreading, and beyond. 2023.\\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\\nWang, Jun Tang, et al. Qwen2.5-vl technical report.arXiv:2502.13923, 2025.\\n[4] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng\\nShou. One token to seg them all: Language instructed reasoning segmentation in videos. InNeurIPS,\\npages 6833–6859, 2024.\\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation\\nwith multimodal transformers. InCVPR, pages 4985–4995, 2022.\\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\\nlarge-scale video benchmark for human activity understanding. InCVPR, pages 961–970, 2015.\\n[7] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\\nCVPR, pages 1209–1218, 2018.\\n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krish-\\nnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model\\nas a unified interface for vision-language multi-task learning.arXiv:2310.09478, 2023.\\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic.arXiv:2306.15195, 2023.\\n[10] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect\\nwhat you can: Detecting and representing objects using holistic models and body parts. InCVPR, pages\\n1971–1978, 2014.\\n[11] Yuan Chen, Zi-han Ding, Ziqin Wang, Yan Wang, Lijun Zhang, and Si Liu. Asynchronous large language\\nmodel enhanced planner for autonomous driving. InECCV, pages 22–38, 2024.\\n[12] Zewen Chen, Juan Wang, Wen Wang, Sunhan Xu, Hang Xiong, Yun Zeng, Jian Guo, Shuxun Wang,\\nChunfeng Yuan, Bing Li, othersShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei\\nZhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-\\ninterest.arXiv:2307.03601, 2023.\\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\\nHao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models\\nwith model, data, and test-time scaling.arXiv:2412.05271, 2024.\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,\\nXizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic\\nvisual-linguistic tasks.arXiv:2312.14238, 2023.\\n[15] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an\\natkinson-shiffrin memory model. InECCV, pages 640–658, 2022.\\n[16] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu,\\nZichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous\\ndriving. InWACV, pages 958–979, 2024.\\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\\nLi, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\\ninstruction tuning. InNeurIPS, 2024.\\n[18] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era, 2024.\\n[19] Google DeepMind. Gemini 2.5: Our most intelligent ai model, 2025.\\n[20] Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed\\nMian, Mohit Bansal, and Chen Chen. Motion-grounded video reasoning: Understanding and perceiving\\nmotion at pixel level.arXiv:2411.09921, 2024.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[21] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: A large-scale\\nbenchmark for video segmentation with motion expressions. InICCV, pages 2694–2703, 2023.\\n[22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan\\nTompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language\\nmodel.arXiv:2303.03378, 2023.\\n[23] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified\\npixel-level vision llm for understanding, generating, segmenting, editing.arXiv:2412.19806, 2024.\\n[24] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\\nYunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark\\nof multi-modal llms in video analysis.arXiv:2405.21075, 2024.\\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning.arXiv:2501.12948, 2025.\\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models.ICLR, 2022.\\n[27] Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, and Dae-Shik Kim. Mmr: A large-scale\\nbenchmark dataset for multi-target and multi-granularity reasoning segmentation.arXiv:2503.13881,\\n2025.\\n[28] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-\\ntemporal reasoning in visual question answering. InCVPR, pages 2758–2766, 2017.\\n[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\\nin photographs of natural scenes. InEMNLP, pages 787–798, 2014.\\n[30] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael\\nRafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-\\naction model.arXiv:2406.09246, 2024.\\n[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. InICCV, pages 4015–4026,\\n2023.\\n[32] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning\\nsegmentation via large language model. InCVPR, pages 9579–9589, 2024.\\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer.arXiv:2408.03326, 2024.\\n[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. InICML, pages 19730–19742, 2023.\\n[35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\\nYu Qiao. Videochat: Chat-centric video understanding.arXiv:2305.06355, 2023.\\n[36] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\\nLuo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. InCVPR, pages\\n22195–22206, 2024.\\n[37] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli\\nZhao, and Wenbing Huang. Star-r1: Spacial transformation reasoning by reinforcing multimodal llms.\\narXiv:2505.15804, 2025.\\n[38] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\\nrepresentation by alignment before projection.arXiv:2311.10122, 2023.\\n[39] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into a\\nsingle large language model for video segmentation.arXiv:2504.07962, 2025.\\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. InECCV, pages 740–755, 2014.\\n[41] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In\\nCVPR, pages 23592–23601, 2023.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning. InCVPR, pages 26296–26306, 2024.\\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. InNeurIPS,\\npages 34892–34916, 2023.\\n[44] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models\\nare effective temporal learners. InECCV, pages 1–18, 2024.\\n[45] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen.\\nr2-tuning: Efficient image-to-video transfer learning for video temporal grounding. InECCV, 2024.\\n[46] Ye Liu, Huifang Li, Chao Hu, Shuang Luo, Yan Luo, and Chang Wen Chen. Learning to aggregate\\nmulti-scale context for instance segmentation in remote sensing images.IEEE Transactions on Neural\\nNetworks and Learning Systems, 36(1):595–609, 2024.\\n[47] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal\\ntransformers for joint video moment retrieval and highlight detection. InCVPR, pages 3042–3051, 2022.\\n[48] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: A chain-of-lora\\nagent for long video reasoning.arXiv:2503.13444, 2025.\\n[49] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang W Chen. E.t. bench: Towards\\nopen-ended event-level video-language understanding. InNeurIPS, pages 32076–32110, 2024.\\n[50] Ye Liu, Junsong Yuan, and Chang Wen Chen. Consnet: Learning consistency graph for zero-shot\\nhuman-object interaction detection. InACM MM, pages 4235–4243, 2020.\\n[51] Zongyang Ma, Yuxin Chen, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Shaojie Zhu, Chengxiang\\nZhuo, Bing Li, Ye Liu, Zang Li, Ying Shan, and Weiming Hu. Visionmath: Vision-form mathematical\\nproblem-solving. InICCV, 2025.\\n[52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and\\nvideo encoders for enhanced video understanding.arXiv:2406.09418, 2024.\\n[53] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\\ndetailed video understanding via large vision and language models.arXiv:2306.05424, 2023.\\n[54] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark\\nfor very long-form video language understanding. InNeurIPS, 2024.\\n[55] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\\nGeneration and comprehension of unambiguous object descriptions. InCVPR, pages 11–20, 2016.\\n[56] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and\\nSalman Khan. Videoglamm: A large multimodal model for pixel-level visual grounding in videos.\\narXiv:2411.04923, 2024.\\n[57] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas\\ndataset for semantic understanding of street scenes. InICCV, pages 4990–4999, 2017.\\n[58] OpenAI. Gpt-4v(ision) system card, 2023.\\n[59] OpenAI. Gpt-4o system card, 2024.\\n[60] OpenAI. Openai o1 system card, 2024.\\n[61] Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu,\\nZuxuan Wu, and Yu-Gang Jiang. Inst-it: Boosting multimodal instance understanding via explicit visual\\nprompt instruction tuning.arXiv:2412.03565, 2024.\\n[62] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc\\nVan Gool. The 2017 davis challenge on video object segmentation.arXiv:1704.00675, 2017.\\n[63] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye,\\nand Yunjie Tian. Artemis: Towards referential understanding in complex videos. InNeurIPS, 2024.\\n[64] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects.\\nInCVPR, pages 7141–7151, 2023.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[65] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham\\nCholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding\\nlarge multimodal model. InCVPR, pages 13009–13018, 2024.\\n[66] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham\\nKhedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and\\nvideos.arXiv:2408.00714, 2024.\\n[67] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of context.arXiv:2403.05530, 2024.\\n[68] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large\\nlanguage model for long video understanding. InCVPR, pages 14313–14323, 2024.\\n[69] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin.\\nPixellm: Pixel reasoning with large multimodal model. InCVPR, pages 26374–26383, 2024.\\n[70] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal,\\nArkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical vision transformer\\nwithout the bells-and-whistles. InICML, pages 29441–29454, 2023.\\n[71] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation\\nnetwork with a large-scale benchmark. InECCV, pages 208–223, 2020.\\n[72] Nicholas Stroh. Trackgpt–a generative pre-trained transformer for cross-domain entity trajectory forecast-\\ning.arXiv:2402.00066, 2024.\\n[73] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high\\nfrequency functions in low dimensional domains. InNeurIPS, pages 7537–7547, 2020.\\n[74] OpenGVLab Team. Internvl2: Better than the best—expanding performance boundaries of open-source\\nmultimodal models with the progressive scaling strategy, 2024.\\n[75] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level\\nperception in videos via mllm. InECCV, pages 166–185, 2024.\\n[76] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios\\nGavves. Towards open-vocabulary video instance segmentation. InICCV, pages 4057–4066, 2023.\\n[77] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\\nresolution.arXiv:2409.12191, 2024.\\n[78] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-\\nto-sequence learning framework. InICML, pages 23318–23340, 2022.\\n[79] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and\\nJose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception,\\nreasoning and planning.arXiv:2405.01533, 2024.\\n[80] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. Lasagna: Language-based segmentation\\nassistant for complex queries.arXiv:2404.08506, 2024.\\n[81] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. Hyperseg:\\nTowards universal visual segmentation with large language model.arXiv:2411.17606, 2024.\\n[82] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, and Yujiu Yang. Instructseg:\\nUnifying instructed visual segmentation with multi-modal large language models.arXiv:2412.14006,\\n2024.\\n[83] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: A simple\\nonline baseline for referring video object segmentation. InICCV, pages 2761–2770, 2023.\\n[84] Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, and Chang Wen Chen. A survey\\non video temporal grounding with multimodal large language model.arXiv:2508.10922, 2025.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[85] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video\\nobject segmentation. InCVPR, pages 4974–4984, 2022.\\n[86] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Segment every reference\\nobject in spatial and temporal spaces. InICCV, pages 2538–2550, 2023.\\n[87] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized\\nsegmentation via multimodal large language models. InCVPR, pages 3858–3869, 2024.\\n[88] Linshan Xie, Xuehong Lin, Xunda Wang, Junjian Wen, Teng Ma, Jiahao Hu, Peng Cao, Alex TL Leong,\\nand Ed X Wu. Simultaneous eeg-fmri reveals spontaneous neural oscillatory activity in cingulate cortex\\nunderlying transient rsfmri network dynamics. InISMRM, 2024.\\n[89] Linshan Xie, Xunda Wang, Xuehong Lin, Teng Ma, Junjian Wen, Peng Cao, Alex TL Leong, and Ed X\\nWu. Single-pulse optogenetic perturbation of thalamo-cortical networks reveals functional architecture of\\nrsfmri networks. InISMRM, 2024.\\n[90] Linshan Xie, Xunda Wang, Xuehong Lin, Junjian Wen, Teng Ma, Alex TL Leong, and Ed X Wu.\\nBrain-wide resting-state fmri network dynamics elicited by activation of single thalamic input.Nature\\nCommunications, 2025.\\n[91] Linshan Xie, Xunda Wang, Teng Ma, Pit Shan Chong, Lee Wei Lim, Peng Cao, Pek-Lan Khong, Ed X Wu,\\nand Alex TL Leong. Are topographically segregated excitatory neurons in visual thalamus functionally\\ndiverse? an optogenetic fmri study. InISMRM, 2022.\\n[92] Linshan Xie, Xunda Wang, Teng Ma, Hang Zeng, Junjian Wen, Peng Cao, Ed X Wu, and Alex TL Leong.\\nShort single pulse optogenetic fmri mapping of downstream targets in thalamo-cortical pathways. In\\nISMRM, 2023.\\n[93] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\\nquestion answering via gradually refined attention over appearance and motion. InACM MM, pages\\n1645–1653, 2017.\\n[94] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free\\nllava extension from images to videos for video dense captioning.arXiv:2404.16994, 2024.\\n[95] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\\nperception as object discovery and retrieval. InCVPR, pages 15325–15336, 2023.\\n[96] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios\\nGavves. Visa: Reasoning video object segmentation via large language models. InECCV, pages 98–115,\\n2024.\\n[97] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\\nunleashes extraordinary visual grounding in gpt-4v.arXiv:2310.11441, 2023.\\n[98] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao,\\nShih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity.\\narXiv:2310.07704, 2023.\\n[99] Samson Yu, Kelvin Lin, Anxing Xiao, Jiafei Duan, and Harold Soh. Octopi: Object property reasoning\\nwith large tactile-language models.arXiv:2405.02794, 2024.\\n[100] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A\\ndataset for understanding complex web videos via question answering. InAAAI, pages 9127–9134, 2019.\\n[101] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi\\nFeng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of\\nimages and videos.arXiv:2501.04001, 2025.\\n[102] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.\\nOsprey: Pixel understanding with visual instruction tuning. InCVPR, pages 28202–28211, 2024.\\n[103] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao,\\nWenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding\\nwith video llm.arXiv:2501.00599, 2024.\\n[104] Ao Zhang, Yuan Yao, Wei Ji, Zhiyuan Liu, and Tat-Seng Chua. Next-chat: An lmm for chat, detection\\nand segmentation.arXiv:2311.04498, 2023.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[105] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\\nfor video understanding.arXiv:2306.02858, 2023.\\n[106] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation\\nfor autonomous vehicles. InCVPR, pages 15459–15469, 2024.\\n[107] Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi\\nZhang, and Lianwen Jin. Aesthetics is cheap, show me the text: An empirical evaluation of state-of-the-art\\ngenerative models for ocr.arXiv:2507.15085, 2025.\\n[108] Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, and Lianwen Jin. Smaller but better: Unifying\\nlayout generation with smaller large language models.International Journal of Computer Vision,\\n133:3891–3917, 2025.\\n[109] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and\\nShuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding.\\nInNeurIPS, pages 71737–71767, 2024.\\n[110] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. InCVPR, pages 14227–14238, 2024.\\n[111] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat\\nwith the environment: Interactive multimodal perception using large language models. InIROS, pages\\n3590–3596, 2023.\\n[112] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, and Hengshuang Zhao. Villa: Video\\nreasoning segmentation with large language model.arXiv:2407.14500, 2024.\\n[113] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. InCVPR, pages 633–641, 2017.\\n[114] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web\\ninstructional videos. InAAAI, 2018.\\n[115] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models.arXiv:2304.10592, 2023.\\n[116] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl,\\nJianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. InCVPR, pages\\n15116–15127, 2023.\\n[117] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao,\\nand Yong Jae Lee. Segment everything everywhere all at once. InNeurIPS, pages 19769–19782, 2023.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL\\nENHANCEMENTS\\nRahima Khanam* and Muhammad Hussain\\nDepartment of Computer Science, Huddersfield University, Queensgate, Huddersfield HD1 3DH, UK;\\n*Correspondence: rahima.khanam@hud.ac.uk;\\nOctober 24, 2024\\nABSTRACT\\nThis study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only\\nLook Once) series of object detection models. We examine the models architectural innovations,\\nincluding the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object\\ndetection (OBB). We review the model’s performance improvements in terms of mean Average\\nPrecision (mAP) and computational efficiency compared to its predecessors, with a focus on the\\ntrade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11’s\\nversatility across different model sizes, from nano to extra-large, catering to diverse application needs\\nfrom edge devices to high-performance computing environments. Our research provides insights into\\nYOLOv11’s position within the broader landscape of object detection and its potential impact on\\nreal-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction\\nComputer vision, a rapidly advancing field, enables machines to interpret and understand visual data [ 1]. A crucial\\naspect of this domain is object detection[2], which involves the precise identification and localization of objects within\\nimages or video streams[3]. Recent years have witnessed remarkable progress in algorithmic approaches to address this\\nchallenge [4].\\nA pivotal breakthrough in object detection came with the introduction of the You Only Look Once (YOLO) algorithm\\nby Redmon et al. in 2015 [5]. This innovative approach, as its name suggests, processes the entire image in a single pass\\nto detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by\\nframing object detection as a regression problem [5]. It employs a single convolutional neural network to simultaneously\\npredict bounding boxes and class probabilities across the entire image [6], streamlining the detection pipeline compared\\nto more complex traditional methods.\\nYOLOv11 is the latest iteration in the YOLO series, building upon the foundation established by YOLOv1. Unveiled at\\nthe YOLO Vision 2024 (YV24) conference, YOLOv11 represents a significant leap forward in real-time object detection\\ntechnology. This new version introduces substantial enhancements in both architecture and training methodologies,\\npushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail\\ncapture while maintaining a lean parameter count. This results in improved accuracy across a diverse range of computer\\narXiv:2410.17725v1  [cs.CV]  23 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nvision (CV) tasks, from object detection to classification. Furthermore, YOLOv11 achieves remarkable gains in\\nprocessing speed, substantially enhancing real-time performance capabilities.\\nIn the following sections, this paper will provide a comprehensive analysis of YOLOv11’s architecture, exploring its\\nkey components and innovations. We will examine the evolution of YOLO models, leading up to the development\\nof YOLOv11. The study will delve into the model’s expanded capabilities across various CV tasks, including object\\ndetection, instance segmentation, pose estimation, and oriented object detection. We will also review YOLOv11’s\\nperformance improvements in terms of accuracy and computational efficiency compared to its predecessors, with a\\nparticular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11\\non real-time CV applications and its position within the broader landscape of object detection technologies.\\n2 Evolution of YOLO models\\nTable 1 illustrates the progression of YOLO models from their inception to the most recent versions. Each iteration has\\nbrought significant improvements in object detection capabilities, computational efficiency, and versatility in handling\\nvarious CV tasks.\\nTable 1: YOLO: Evolution of models\\nRelease Year Tasks Contributions Framework\\nYOLO [5] 2015 Object Detection, Basic Classifica-\\ntion\\nSingle-stage object detector Darknet\\nYOLOv2 [7] 2016 Object Detection, Improved Classi-\\nfication\\nMulti-scale training, dimension clus-\\ntering\\nDarknet\\nYOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone\\nDarknet\\nYOLOv5 [10] 2020 Object Detection, Basic Instance\\nSegmentation (via custom modifica-\\ntions)\\nAnchor-free detection, SWISH acti-\\nvation, PANet\\nPyTorch\\nYOLOv6 [11] 2022 Object Detection, Instance Segmen-\\ntation\\nSelf-attention, anchor-free OD PyTorch\\nYOLOv7 [12] 2022 Object Detection, Object Tracking,\\nInstance Segmentation\\nTransformers, E-ELAN reparame-\\nterisation\\nPyTorch\\nYOLOv8 [13] 2023 Object Detection, Instance Segmen-\\ntation, Panoptic Segmentation, Key-\\npoint Estimation\\nGANs, anchor-free detection PyTorch\\nYOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch\\nThis evolution showcases the rapid advancement in object detection technologies, with each version introducing novel\\nfeatures and expanding the range of supported tasks. From the original YOLO’s groundbreaking single-stage detection\\nto YOLOv10’s NMS-free training, the series has consistently pushed the boundaries of real-time object detection.\\nThe latest iteration, YOLO11, builds upon this legacy with further enhancements in feature extraction, efficiency,\\nand multi-task capabilities. Our subsequent analysis will delve into YOLO11’s architectural innovations, including\\nits improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nspatial attention in the feature maps. This spatial attention mechanism allows the model to focus more effectively on\\nimportant regions within the image. By pooling features spatially, the C2PSA block enables YOLO11 to concentrate on\\nspecific areas of interest, potentially improving detection accuracy for objects of varying sizes and positions.\\n4.2 Neck\\nThe neck combines features at different scales and transmits them to the head for prediction. This process typically\\ninvolves upsampling and concatenation of feature maps from different levels, enabling the model to capture multi-scale\\ninformation effectively.\\n4.2.1 C3k2 Block\\nYOLO11 introduces a significant change by replacing the C2f block in the neck with the C3k2 block. The C3k2 block\\nis designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After\\nupsampling and concatenation, the neck in YOLO11 incorporates this improved block, resulting in enhanced speed and\\nperformance [18].\\n4.2.2 Attention Mechanism\\nA notable addition to YOLO11 is its increased focus on spatial attention through the C2PSA module. This attention\\nmechanism enables the model to concentrate on key regions within the image, potentially leading to more accurate\\ndetection, especially for smaller or partially occluded objects. The inclusion of C2PSA sets YOLO11 apart from its\\npredecessor, YOLOv8, which lacks this specific attention mechanism [18].\\n4.3 Head\\nThe head of YOLOv11 is responsible for generating the final predictions in terms of object detection and classification.\\nIt processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block\\nIn the head section, YOLOv11 utilizes multiple C3k2 blocks to efficiently process and refine the feature maps. The\\nC3k2 blocks are placed in several pathways within the head, functioning to process multi-scale features at different\\ndepths. The C3k2 block exhibits flexibility depending on the value of the c3k parameter:\\n• When c3k = False, the C3k2 module behaves similarly to the C2f block, utilizing a standard bottleneck\\nstructure.\\n• When c3k = True, the bottleneck structure is replaced by the C3 module, which allows for deeper and more\\ncomplex feature extraction.\\nKey characteristics of the C3k2 block:\\n• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.\\n• Parameter efficiency: C3k2 is a more compact version of the CSP bottleneck, making the architecture more\\nefficient in terms of the number of trainable parameters.\\nAnother notable addition is the C3k block, which offers enhanced flexibility by allowing customizable kernel sizes. The\\nadaptability of C3k is particularly useful for extracting more detailed features from images, contributing to improved\\ndetection accuracy.\\n4.3.2 CBS Blocks\\nThe head of YOLOv11 includes several CBS (Convolution-BatchNorm-Silu) [19] layers after the C3k2 blocks. These\\nlayers further refine the feature maps by:\\n• Extracting relevant features for accurate object detection.\\n• Stabilizing and normalizing the data flow through batch normalization.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n• Utilizing the Sigmoid Linear Unit (SiLU) activation function for non-linearity, which improves model perfor-\\nmance.\\nCBS blocks serve as foundational components in both feature extraction and the detection process, ensuring that the\\nrefined feature maps are passed to the subsequent layers for bounding box and classification predictions.\\n4.3.3 Final Convolutional Layers and Detect Layer\\nEach detection branch ends with a set of Conv2D layers, which reduce the features to the required number of outputs for\\nbounding box coordinates and class predictions. The final Detect layer consolidates these predictions, which include:\\n• Bounding box coordinates for localizing objects in the image.\\n• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11\\nYOLO11 supports a diverse range of CV tasks, showcasing its versatility and power in various applications. Here’s an\\noverview of the key tasks:\\n1. Object Detection: YOLO11 excels in identifying and localizing objects within images or video frames,\\nproviding bounding boxes for each detected item [ 20]. This capability finds applications in surveillance\\nsystems, autonomous vehicles, and retail analytics, where precise object identification is crucial [21].\\n2. Instance Segmentation: Going beyond simple detection, YOLO11 can identify and separate individual\\nobjects within an image down to the pixel level [20]. This fine-grained segmentation is particularly valuable in\\nmedical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].\\n3. Image Classification: YOLOv11 is capable of classifying entire images into predetermined categories,\\nmaking it ideal for applications like product categorization in e-commerce platforms or wildlife monitoring in\\necological studies [21].\\n4. Pose Estimation: The model can detect specific key points within images or video frames to track movements\\nor poses. This capability is beneficial for fitness tracking applications, sports performance analysis, and various\\nhealthcare applications requiring motion assessment [21].\\n5. Oriented Object Detection (OBB): YOLO11 introduces the ability to detect objects with an orientation angle,\\nallowing for more precise localization of rotated objects. This feature is especially valuable in aerial imagery\\nanalysis, robotics, and warehouse automation tasks where object orientation is crucial [21].\\n6. Object Tracking: It identifies and traces the path of objects in a sequence of images or video frames[ 21].\\nThis real-time tracking capability is essential for applications such as traffic monitoring, sports analysis, and\\nsecurity systems.\\nTable 2 outlines the YOLOv11 model variants and their corresponding tasks. Each variant is designed for specific\\nuse cases, from object detection to pose estimation. Moreover, all variants support core functionalities like inference,\\nvalidation, training, and export, making YOLOv11 a versatile tool for various CV applications.\\n6 Advancements and Key Features of YOLOv11\\nYOLOv11 represents a significant advancement in object detection technology, building upon the foundations laid by\\nits predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics\\nshowcases enhanced architectural designs, more sophisticated feature extraction techniques, and refined training\\nmethodologies. The synergy of YOLOv11’s rapid processing, high accuracy, and computational efficiency positions it\\nas one of the most formidable models in Ultralytics’ portfolio to date [22]. A key strength of YOLOv11 lies in its refined\\narchitecture, which facilitates the detection of subtle details even in challenging scenarios. The model’s improved\\nfeature extraction capabilities allow it to identify and process a broader range of patterns and intricate elements within\\nimages. Compared to earlier versions, YOLOv11 introduces several notable enhancements:\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nTable 2: YOLOv11 Model Variants and Tasks\\nModel Variants Task Inference Validation Training Export\\nYOLOv11 yolo11-nano yolo11-small\\nyolo11-medium yolo11-\\nlarge yolo11-xlarge\\nDetection ✓ ✓ ✓ ✓\\nYOLOv11-seg yolo11-nano-seg yolo11-\\nsmall-seg yolo11-medium-\\nseg yolo11-large-seg\\nyolo11-xlarge-seg\\nInstance Segmen-\\ntation\\n✓ ✓ ✓ ✓\\nYOLOv11-pose yolo11-nano-pose yolo11-\\nsmall-pose yolo11-medium-\\npose yolo11-large-pose\\nyolo11-xlarge-pose\\nPose/Keypoints ✓ ✓ ✓ ✓\\nYOLOv11-obb yolo11-nano-obb yolo11-\\nsmall-obb yolo1-medium-\\nobb yolo11-large-obb\\nyolo11-xlarge-obb\\nOriented Detec-\\ntion\\n✓ ✓ ✓ ✓\\nYOLOv11-cls yolo11-nano-cls yolo11-\\nsmall-cls yolo11-medium-\\ncls yolo11-large-cls yolo11-\\nxlarge-cls\\nClassification ✓ ✓ ✓ ✓\\n1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m\\ncounterpart, demonstrating improved computational efficiency without compromising accuracy [23].\\n2. Versatility in CV tasks: YOLOv11 exhibits proficiency across a diverse array of CV applications, including\\npose estimation, object recognition, image classification, instance segmentation, and oriented bounding box\\n(OBB) detection [23].\\n3. Optimized speed and performance: Through refined architectural designs and streamlined training pipelines,\\nYOLOv11 achieves faster processing speeds while maintaining a balance between accuracy and computational\\nefficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].\\n5. Advanced feature extraction: YOLOv11 incorporates improvements in both its backbone and neck architec-\\ntures, resulting in enhanced feature extraction capabilities and, consequently, more precise object detection\\n[23].\\n6. Contextual adaptability: YOLOv11 demonstrates versatility across various deployment scenarios, including\\ncloud platforms, edge devices, and systems optimized for NVIDIA GPUs [23].\\nYOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].\\nThe performance comparison graph depicted in Figure 2 overs several key insights. The YOLOv11 variants (11n, 11s,\\n11m, and 11x) form a distinct performance frontier, with each model achieving higher COCO mAP 50−95 scores at\\ntheir respective latency points. Notably, the YOLOv11x achieves approximately 54.5% mAP50−95 at 13ms latency,\\nsurpassing all previous YOLO iterations. The intermediate variants, particularly YOLOv11m, demonstrate exceptional\\nefficiency by achieving comparable accuracy to larger models from previous generations while requiring significantly\\nless processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s\\nmaintains high accuracy (approximately 47% mAP50−95) while operating at speeds previously associated with much\\nless accurate models. This represents a crucial advancement for real-time applications where both speed and accuracy\\nare critical. The improvement curve of YOLOv11 also shows better scaling characteristics across its model variants,\\nsuggesting more efficient utilization of additional computational resources compared to previous generations.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nFigure 2: Benchmarking YOLOv11 Against Previous Versions [23]\\n7 Discussion\\nYOLO11 marks a significant leap forward in object detection technology, building upon its predecessors while\\nintroducing innovative enhancements. This latest iteration demonstrates remarkable versatility and efficiency across\\nvarious CV tasks.\\n1. Efficiency and Scalability: YOLO11 introduces a range of model sizes, from nano to extra-large, catering\\nto diverse application needs. This scalability allows for deployment in scenarios ranging from resource-\\nconstrained edge devices to high-performance computing environments. The nano variant, in particular,\\nshowcases impressive speed and efficiency improvements over its predecessor, making it ideal for real-time\\napplications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature\\nextraction and processing capabilities. The incorporation of novel elements such as the C3k2 block, SPPF, and\\nC2PSA contributes to more effective feature extraction and processing. These enhancements allow the model\\nto better analyze and interpret complex visual information, potentially leading to improved detection accuracy\\nacross various scenarios.\\n3. Multi-Task Proficiency: YOLO11’s versatility extends beyond object detection, encompassing tasks such as\\ninstance segmentation, image classification, pose estimation, and oriented object detection. This multi-faceted\\napproach positions YOLO11 as a comprehensive solution for diverse CV challenges.\\n4. Enhanced Attention Mechanisms: A key advancement in YOLO11 is the integration of sophisticated spatial\\nattention mechanisms, particularly the C2PSA component. This feature enables the model to focus more\\neffectively on critical regions within an image, enhancing its ability to detect and analyze objects. The\\nimproved attention capability is especially beneficial for identifying complex or partially occluded objects,\\naddressing a common challenge in object detection tasks. This refinement in spatial awareness contributes to\\nYOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.\\n6. Implications for Real-World Applications: The advancements in YOLO11 have significant implications\\nfor various industries. Its improved efficiency and multi-task capabilities make it particularly suitable for\\napplications in autonomous vehicles, surveillance systems, and industrial automation. The model’s ability to\\nperform well across different scales also opens up new possibilities for deployment in resource-constrained\\nenvironments without compromising on performance.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n8 Conclusion\\nYOLOv11 represents a significant advancement in the field of CV , offering a compelling combination of enhanced\\nperformance and versatility. This latest iteration of the YOLO architecture demonstrates marked improvements in\\naccuracy and processing speed, while simultaneously reducing the number of parameters required. Such optimizations\\nmake YOLOv11 particularly well-suited for a wide range of applications, from edge computing to cloud-based analysis.\\nThe model’s adaptability across various tasks, including object detection, instance segmentation, and pose estimation,\\npositions it as a valuable tool for diverse industries such as emotion detection [26], healthcare [27] and various other\\nindustries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses\\nseeking to implement or upgrade their CV systems. In summary, YOLOv11’s blend of enhanced feature extraction,\\noptimized performance, and broad task support establishes it as a formidable solution for addressing complex visual\\nrecognition challenges in both research and practical applications.\\nReferences\\n[1] Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image processing, analysis and machine vision. Springer, 2013.\\n[2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\\nProceedings of the IEEE, 111(3):257–276, 2023.\\n[3] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review.\\nIEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-\\nvoltaic defect detection. In Solar, volume 4, pages 351–386. MDPI, 2024.\\n[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,\\n2016.\\n[6] Juan Du. Understanding of object detection based on cnn family and yolo. In Journal of Physics: Conference\\nSeries, volume 1004, page 012029. IOP Publishing, 2018.\\n[7] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\\n[10] Roboflow Blog Jacob Solawetz. What is yolov5? a guide for beginners., 2020. Accessed: 21 October 2024.\\n[11] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint\\narXiv:2209.02976, 2022.\\n[12] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new\\nstate-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.\\n[13] Francesco Jacob Solawetz. What is yolov8? the ultimate guide, 2023. Accessed: 21 October 2024.\\n[14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using\\nprogrammable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time\\nend-to-end object detection. arXiv preprint arXiv:2405.14458, 2024.\\n[16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024.\\n[17] Rahima Khanam, Muhammad Hussain, Richard Hill, and Paul Allen. A comprehensive review of convolutional\\nneural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.\\n[19] Jingwen Feng, Qiaofeng An, Jiahao Zhang, Shuxun Zhou, Guangwei Du, and Kai Yang. Application of yolov7-tiny\\nin the detection of steel surface defects. In 2024 5th International Seminar on Artificial Intelligence, Networking\\nand Information Technology (AINIT), pages 2241–2245. IEEE, 2024.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n[20] Ultralytics. Instance segmentation and tracking, 2024. Accessed: 2024-10-21.\\n[21] Ultralytics Abirami Vina. Ultralytics yolo11 has arrived: Redefine what’s possible in ai, 2024. Accessed:\\n2024-10-21.\\n[22] Viso.AI Gaudenz Boesch. Yolov11: A new iteration of “you only look once. https://viso.ai/\\ncomputer-vision/yolov11/, 2024. Accessed: 2024-10-21.\\n[23] Ultralytics. Ultralytics yolov11. https://docs.ultralytics.com/models/yolo11/s, 2024. Accessed:\\n21-Oct-2024.\\n[24] Rahima Khanam and Muhammad Hussain. What is yolov5: A deep look into the internal features of the popular\\nobject detector. arXiv preprint arXiv:2407.20892, 2024.\\n[25] DigitalOcean. What’s new in yolov11 transforming object detection once again part 1, 2024. Accessed: 2024-10-\\n21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.\\nIn Kids Cybersecurity Using Computational Intelligence Techniques, pages 165–174. Springer, 2023.\\n[27] Burcu Ataer Aydin, Muhammad Hussain, Richard Hill, and Hussain Al-Aqrabi. Domain modelling for a\\nlightweight convolutional network focused on automated exudate detection in retinal fundus images. In 2023 9th\\nInternational Conference on Information Technology Trends (ITT), pages 145–150. IEEE, 2023.\\n9')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a738df",
   "metadata": {},
   "source": [
    "embedding vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9d8a4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Total documents after splitting: {len(split_docs)}\")\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ca44998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents after splitting: 147\n",
      "\n",
      "Example chunk:\n",
      "Content: UniPixel: Unified Object Referring and\n",
      "Segmentation for Pixel-Level Visual Reasoning\n",
      "Ye Liu1,2, Zongyang Ma2,3, Junfu Pu2, Zhongang Qi4, Yang Wu5,\n",
      "Ying Shan2, Chang Wen Chen1∗\n",
      "1 The Hong Kong Polytech...\n",
      "Metadata: {'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='UniPixel: Unified Object Referring and\\nSegmentation for Pixel-Level Visual Reasoning\\nYe Liu1,2, Zongyang Ma2,3, Junfu Pu2, Zhongang Qi4, Yang Wu5,\\nYing Shan2, Chang Wen Chen1∗\\n1 The Hong Kong Polytechnic University 2 ARC Lab, Tencent PCG\\n3 Institute of Automation, Chinese Academy of Sciences 4 vivo Mobile Communication Co.\\n5 MindWingman Technology (Shenzhen) Co., Ltd.\\ncoco.ye.liu@connect.polyu.hk\\nhttps://polyu-chenlab.github.io/unipixel/\\nReasoning Segmentation (ReasonSeg)\\nReferring Expression Segmentation (RES)\\nInteractive Segmentation (IS)\\nReasoning Video Object Segmentation (ReVOS)\\nReferring Video Object Segmentation (RVOS)\\nMotion-Grounded Video Reasoning\\nReferred Video Description\\nReferred Video Question-Answering'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Reasoning Video Object Segmentation (ReVOS)\\nReferring Video Object Segmentation (RVOS)\\nMotion-Grounded Video Reasoning\\nReferred Video Description\\nReferred Video Question-Answering\\nFind the empty chair that is to the left of the main sitting down.Segment the zebra standing in the middle of the frame.Locate the area that a cyclist uses to navigate in the city.Find the place where patients lie down to receive examination.Please segment and track the marked <region>.\\nPlease segment and track the hopping rabbit that leaped from the other one in the video.Where are the utensils used for drinking? Answer with masks.Q: Who shook off and scored?A: The man in red pants.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='A: This police officer is a middle-aged man with a beard, wearing a blue uniform shirt, a black hat, and glasses. He stopped a blue vintage car and talked to the driver sitting inside.Q: Please describe the <region>. Q: If <region> continues his breakdance routine, what is a likely future event?A: He will perform more complex and varied breakdance moves.\\nPixel-Level Video Question Answering (PixelQA) — Joint Referring + Segmentation + QA in Videos\\nQ: How does the behavior of [1] differ from that of [2]? Why?\\n1\\n2\\n 3\\n4\\nA: [1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering food to [2] and [3]. This might because [1] doesn’t like the food from [4].\\n1\\n2\\nFigure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding\\ntasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='2\\nFigure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding\\ntasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and\\nreferred video description & question answering. It can also handle a novelPixelQAtask that jointly\\nrequires object-centric referring, segmentation, and question answering in videos.\\nAbstract\\nRecent advances in Large Multi-modal Models (LMMs) have demonstrated their\\nremarkable success as general-purpose multi-modal assistants, with particular\\nfocuses on holistic image- and video-language understanding. Conversely, less\\nattention has been given to scaling fine-grained pixel-level understanding capabili-\\nties, where the models are expected to realize pixel-level alignment between visual\\n∗Corresponding author.\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2509.18094v3  [cs.CV]  27 Oct 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='signals and language semantics. Some previous studies have applied LMMs to\\nrelated tasks such as region-level captioning and referring expression segmentation.\\nHowever, these models are limited to performing either referring or segmentation\\ntasks independently and fail to integrate these fine-grained perception capabilities\\ninto visual reasoning. To bridge this gap, we proposeUniPixel, a large multi-modal\\nmodel capable of flexibly comprehending visual prompt inputs and generating\\nmask-grounded responses. Our model distinguishes itself by seamlessly integrating\\npixel-level perception with general visual understanding capabilities. Specifically,\\nUniPixel processes visual prompts and generates relevant masks on demand, and\\nperforms subsequent reasoning conditioning on these intermediate pointers during\\ninference, thereby enabling fine-grainedpixel-level reasoning. The effectiveness\\nof our approach has been verified on 10 benchmarks across a diverse set of tasks,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='inference, thereby enabling fine-grainedpixel-level reasoning. The effectiveness\\nof our approach has been verified on 10 benchmarks across a diverse set of tasks,\\nincluding pixel-level referring/segmentation and object-centric understanding in\\nimages/videos. A novelPixelQAtask that jointly requires referring, segmentation,\\nand question answering is also designed to verify the flexibility of our method.\\n1 Introduction\\nLarge Multi-modal Models (LMMs) have been the de facto standard for developing general-purpose\\nassistants. By effectively aligning multi-modalities with language, their significance has been\\ndemonstrated across various applications, including multi-modal analysis [ 59, 19, 107, 108, 48],\\nautonomous driving (AD) [16, 79, 106, 11], and Embodied AI [111, 22, 30, 99].\\nIn the field of visual-language understanding, efforts have been dedicated to developingholistic\\nunderstanding models, where simple projection layers between visual encoders and LLMs are utilized'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='In the field of visual-language understanding, efforts have been dedicated to developingholistic\\nunderstanding models, where simple projection layers between visual encoders and LLMs are utilized\\nto bridge vision and language modalities. Supported by large-scale alignment pre-training and\\nvisual instruction tuning, such a straightforward paradigm achieves strong performance in holistic\\nunderstanding tasks such as captioning [ 40, 6, 114] and general question answering [ 36, 24, 54,\\n49]. However, these models exhibit two fundamental limitations in fine-grained scenarios.First,\\ntheir interactions with users are limited to text format, lacking support for more intuitive forms of\\ncommunication such as drawing points/boxes as references or grounding model responses with key\\nregions represented by masks.Second, the internal reasoning process of these models predominantly\\noperates at a coarse level, directly perceiving the entire content rather than reasoning over specific'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='operates at a coarse level, directly perceiving the entire content rather than reasoning over specific\\nobjects/regions, making them hard to understand fine-grained details. Some previous studies have\\nexplored the application of LMMs to related tasks such as region-level captioning [ 12, 102, 103],\\nreferring expression segmentation [ 29, 55, 41, 21, 71, 62], and reasoning segmentation [ 32, 27,\\n96, 4, 112]. Nevertheless, their models are limited to performing either referring or segmentation\\ntasks independently via rigidly defined input/output templates (e.g., “It’s <SEG>.” in LISA [ 32]),\\nlacking the flexibility to comprehend user-referred concepts and generate mask-grounded responses\\nsimultaneously. More importantly, these methods cannot integrate such fine-grained perception\\ncapabilities with their original human-like [ 90, 89, 88, 92, 91] multi-modal reasoning abilities,\\nresulting in degraded performance on general visual understanding benchmarks [100, 93, 28].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='capabilities with their original human-like [ 90, 89, 88, 92, 91] multi-modal reasoning abilities,\\nresulting in degraded performance on general visual understanding benchmarks [100, 93, 28].\\nIn this work, we seek to bridge this gap by introducingUniPixel, a large multi-modal model that\\ncan flexibly comprehend visual prompt inputs (i.e., points, boxes, and masks) and generate mask-\\ngrounded responses. Our model significantly differentiates itself from existing ones by unifying\\nthe internal representations of referred and segmented objects via a novelobject memory bank,\\nwhich is a hashmap storing the spatial-temporal information of object-of-interests. During inference,\\nUniPixel initializes the object memory bank and updates it on demand by adding object-centric\\ninformation according to the context. The model responses are then generated conditioning on the\\nfine-grained object memory. Benefits from such unification, UniPixel is able to perform not only basic'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='information according to the context. The model responses are then generated conditioning on the\\nfine-grained object memory. Benefits from such unification, UniPixel is able to perform not only basic\\nreferring/segmentation tasks, but also flexiblepixel-level reasoningtasks that require simultaneous\\nvisual prompt comprehension and mask prediction. As illustrated in Fig. 1 (the last row), given a\\nvideo2, a question, and optionally a visual prompt (e.g., a point specified by a click on an object in\\nany frame), UniPixel can (1) infer the mask for the referred object in the corresponding frame, (2)\\npropagate it to all video frames containing the same instance, (3) extract the mask-grounded object\\nfeatures, and finally (4) answer the question conditioning on both the video-level and object-centric\\n2Images are treated as single-frame videos, thus we do not explicitly differentiate them in this work.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='LLM\\nBox/Mask Encoder\\nPoint/Box/Mask Prompt\\nMask Decoder\\nIt shows a...\\nLLM\\nImage/Video\\nSure. It’s <SEG>.\\nPrompt Encoder\\n[3] is less interested in [2] compared with [1].\\nLLM\\nMasks\\nMask Decoder\\nHow does the behavior of [3] differ from [1]?\\n12\\n3\\nDescribe this region.\\nSegment the bunny.\\n(a) Referring-only Models(b) Segmentation-only Models(c) Unified Pixel-level Reasoning Model\\nUpdate\\n(e.g., Osprey, GPT4RoI, VideoRefer) (e.g., LISA, PixelLM, VISA)\\nObject Memory Bank\\nFigure 2:Schematic comparison between UniPixel and its counterparts.To the best of our knowl-\\nedge, UniPixel is the first unified method supporting simultaneous object referring and segmentation.\\ninformation. All these operations are seamlessly conductedwithin a single model, eliminating the\\nneed for external frame samplers [96], mask generators [102, 103], or object trackers [4].\\nWe evaluate the effectiveness of UniPixel from two aspects,i.e., basic referring/segmentation ca-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='need for external frame samplers [96], mask generators [102, 103], or object trackers [4].\\nWe evaluate the effectiveness of UniPixel from two aspects,i.e., basic referring/segmentation ca-\\npabilities and flexible pixel-level reasoning capabilities. For the first aspect, we conduct extensive\\nexperiments on 10 public benchmarks across 9 image/video referring/segmentation tasks. Our method\\nachieves state-of-the-art performance in diverse scenarios. Notably, on the challenging video reason-\\ning segmentation and referred video QA tasks, our 3B model obtains62.1 J&F on ReVOS [96] and\\n72.8%Acc on VideoRefer-Bench Q [103], surpassing strong counterparts with 7B ∼13B parameters.\\nFurther ablation studies also demonstrate the mutual reinforcement effect of referring and segmenta-\\ntion. For the second aspect, we introduce a novelPixelQAtask that jointly requires object-centric\\nreferring, segmentation, and QA in videos, which cannot be handled by existing methods. UniPixel'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='tion. For the second aspect, we introduce a novelPixelQAtask that jointly requires object-centric\\nreferring, segmentation, and QA in videos, which cannot be handled by existing methods. UniPixel\\nestablishes a strong baseline for this novel setting. Our contributions are summarized below:\\n1. We proposeUniPixel, a unified large multi-modal model that supports flexible object referring\\nand segmentation in images and videos, via a novelobject memory bankdesign.\\n2. Our model achieves state-of-the-art performance on 10 public benchmarks across 9 refer-\\nring/segmentation tasks, verifying themutual reinforcement effectof such unification.\\n3. We also introduce a novelPixelQAtask that jointly requires object-centric referring, segmen-\\ntation, and QA in videos, where UniPixel establishes a strong baseline for this setting.\\n2 Related Work\\nLarge Multi-modal ModelsThe remarkable success of large multi-modal models (LMMs) has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='tation, and QA in videos, where UniPixel establishes a strong baseline for this setting.\\n2 Related Work\\nLarge Multi-modal ModelsThe remarkable success of large multi-modal models (LMMs) has\\nshifted the paradigm of visual-language understanding from close-ended experts to open-ended task\\nsolvers. Early attempts [43, 42, 17, 115] involve an MLP projector or Q-Former [34] to align visual\\nencoders to LLMs, enabling open-ended tasks such as visual question answering. With advanced\\ndesigns such as dynamic resolution and data augmentation, open-source models,e.g., Qwen-VL\\n[2, 77, 3] and InternVL [14, 74, 13] series, have narrowed the gap with advanced proprietary models\\nlike the GPT [58, 59] and Gemini families [67, 18]. Recent studies [60, 25, 51, 37, 48] also explore\\ntest-time scaling on visual-language understanding. However, these methods are spatially coarse-\\ngrained. UniPixel can also be regarded as an object-centric test-time scaling approach, where key'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='test-time scaling on visual-language understanding. However, these methods are spatially coarse-\\ngrained. UniPixel can also be regarded as an object-centric test-time scaling approach, where key\\nobjects are first segmented then encoded to facilitate the subsequent reasoning process.\\nVisual Referring and SegmentationTo meet the growing demand for fine-grained visual under-\\nstanding [50, 46, 47, 45, 84], recent efforts have focused on enhancing LMMs with object referring\\nand segmentation capabilities, as compared in Fig. 2. LISA [32] is a representative model that enables\\nLMM-based segmentation by integrating SAM [ 31] as its decoder. They also introduced a novel\\nreasoning segmentation task, requiring models to perform segmentation based on implicit queries.\\nOther works in this direction [104, 69, 110, 65, 27] have explored advanced mask decoders, more\\nflexible tasks, and larger-scale datasets. Recent studies have also extended these capabilities to videos'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='flexible tasks, and larger-scale datasets. Recent studies have also extended these capabilities to videos\\n[4, 96, 101]. Additionally, some research has examined regional understanding through boxes [12]\\nand masks [102, 103]. While recent approaches attempt to unify these two capabilities, they either\\nsupport only images [65] or rely on sub-optimal, tool-based pipelines [23]. To the best of knowledge,\\nUniPixel is the first end-to-end method unifying object referring and mask prediction.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='LLM\\nMask Decoder\\nVisual Encoder\\nPoint Encoder\\nBox Encoder\\nMask Encoder\\n Object Memory Bank\\nPrompt Encoder\\nMemory Injection\\nIn this video, how does the behavior of [1] differ from [2]and[3]? [1] appears disinterested and focuses on nibbling on the ground, while [2]is engaging with [4], who is offering some food to [2]and [3].\\nMasked Crop\\nVisual Tokens\\n Update Memory\\n<REF>Tokens\\n<SEG>Tokens\\n<MEM>Tokens\\nVisual TokenText Token\\nTokenizer\\nFigure 3:The architecture of UniPixel.Given a video, a question, and visual prompts, the model\\nencodes them into tokens via the visual encoder, prompt encoder, and tokenizer, respectively, then\\npredicts a spatial-temporal mask for each visual prompt via the mask decoder. The masks are updated\\ninto the object memory bank, and subsequently injected into the prompt for pixel-level reasoning.\\n3 Method\\nProblem FormulationWe provide a unified definition for pixel-level reasoning tasks. Formally,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='3 Method\\nProblem FormulationWe provide a unified definition for pixel-level reasoning tasks. Formally,\\nthe inputs are an image or a video X, a text prompt T , and N optional visual prompts {Pi}N\\ni=1 where\\neach Pi could be a point, box, or mask on a specific frame. The outputs are textual responses to\\nthe prompt with K grounded spatial-temporal masks {Mi}K\\ni=1. Here, both N and K could be zero\\n(degenerating to a normal visual understanding task) and K is not necessarily equal to N, as the\\nmodel may segment extra objects/regions that are not specified by the visual prompts.\\nOverviewFig. 3 presents an overview of UniPixel. It is built upon the Qwen2.5-VL [ 3] framework,\\nconsisting of an LLM backbone and a ViT-based visual encoder that supports dynamic resolution\\ninputs. Given a video and a text prompt, the model first tokenizes them via the visual encoder and\\ntext tokenizer, then sends them into the LLM for response generation. To boost this framework from'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='inputs. Given a video and a text prompt, the model first tokenizes them via the visual encoder and\\ntext tokenizer, then sends them into the LLM for response generation. To boost this framework from\\nholistic-level to pixel-level, we introduce (1) aprompt encoder(Sec.3.1) supporting three types of\\nvisual prompts, (2) anobject memory bank(Sec.3.2) for storing object information and injecting it\\ninto the response generation process, and (3) amask decoder(Sec.3.3) for generating spatial-temporal\\nmasks. We also extend the LLM’s vocabulary by adding <REF>, <MEM>, and <SEG> tokens. The\\nformer two serve as placeholders in the input prompt that would be replaced by visual prompt and\\nmemory tokens, respectively, while the<SEG> token is utilized to trigger and guide the mask decoding\\nprocess. Detailed designs and interactions among these components are illustrated as follows.\\n3.1 Prompt Encoder\\nT (0.3)\\nX1Y1(0.3, 0.2)\\nLinear\\nLinear\\nGELU + Linear\\nX2Y2(0.8, 0.5)\\n Linear\\n1D\\n2D\\n2D'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='process. Detailed designs and interactions among these components are illustrated as follows.\\n3.1 Prompt Encoder\\nT (0.3)\\nX1Y1(0.3, 0.2)\\nLinear\\nLinear\\nGELU + Linear\\nX2Y2(0.8, 0.5)\\n Linear\\n1D\\n2D\\n2D\\nTemporal EmbPositional EmbType EmbFourier EmbElement-wise AddChannel-wise Cat\\nFigure 4:Joint positional & temporal encodingfor\\npoint (X1Y1T) and box (X1Y1X2Y2T) prompts.\\nThis module aims to effectively encode each\\nvisual prompt into a single token that can be\\nprocessed by the LLM. We denote a point\\nprompt as a tuple (x, y, t)containing its spa-\\ntial coordinates (x, y)and the corresponding\\nframe indext. For box prompts, it is extended\\nto (x1, y1, x2, y2, t)containing the positions\\nof top-left and bottom-right corners. A mask\\nprompt is densely represented by a 2D binary\\nmask mij ∈ {0,1}with the same shape as\\nthe encoded target frame.\\nFor sparse prompts (points and boxes), as shown in Fig. 4, we encode each position(xi, yi) as the sum'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='mask mij ∈ {0,1}with the same shape as\\nthe encoded target frame.\\nFor sparse prompts (points and boxes), as shown in Fig. 4, we encode each position(xi, yi) as the sum\\nof a 2D Fourier embedding [73] and a learnable type embedding (indicating whether it is a single point,\\ntop-left corner, or bottom-right corner). For box prompts, we merge the two positional embeddings\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='by concatenating them along the channel dimension and linearly projecting them back to the original\\nsize. Frame indices are also encoded similarly with 1D Fourier embeddings. The resulting positional\\nand temporal embeddings are concatenated again, and then projected to the LLM’s embedding space\\nvia a GELU→Linear block, such that the sparse coordinates in a point/box are encoded into a\\ncompact high-dimensional token. This design is inspired by [31, 66] with two key differences: (1) the\\nspatial-only embeddings are extended to include temporal information, and (2) the negative points\\nare discarded. For dense prompts (masks), we directly resize the binary masks and apply masked\\npooling on the outputs of the visual encoder. An M →L projector (Linear→GELU→Linear ) is\\nleveraged to project the pooled visual features to the LLM’s embedding space.\\n3.2 Object Memory Bank\\nAlthough sparse prompts contain rich positional and temporal information indicating the objects that'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='3.2 Object Memory Bank\\nAlthough sparse prompts contain rich positional and temporal information indicating the objects that\\nusers are referring to, it is still hard for the model to focus on these important regions. Previous studies\\n[12, 102, 103] also confirm that direct region cropping can generally provide better object awareness\\ncompared to positional pointers. To seamlessly integrate such a mechanism while preserving the\\nflexibility of visual prompts (e.g., allow pointing on a single frame instead of drawing complete\\nmasks on all frames), we propose an object memory bank to bridge sparse visual prompts and dense\\nobject masks. This is a hashmap where the keys are object IDs and the values are the corresponding\\nspatial-temporal masks. It is initialized as an empty storage for every chat session, and is dynamically\\nupdated on demand. We define two operations for the object memory bank, namelymemory pre-filling'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='updated on demand. We define two operations for the object memory bank, namelymemory pre-filling\\nandmemory injection. Below is an example of memory-enhanced multi-round conversation.\\nPrompt 1:How does the behavior of [1]<REF>differ from [2]<REF>and [3]<REF>?\\n<REF>detected, enhancing the prompt with object memory.\\nMemory Pre-filling Response:\\nThe relevant regions for this question are [1] <SEG> [2] <SEG> [3] <SEG> [4] <SEG>. ← 4 objects saved into the memory\\nMemory Injected Prompt:\\nHere is a video with 4 frames denoted as <1> to <4>. The highlighted regions are as follows:\\n[1]: <1><MEM><2><MEM><3><MEM>←This object cannot be seen in the last frame\\n[2]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[3]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[4]: <1><MEM><2><MEM><3><MEM><4><MEM>\\nHow does the behavior of [1] differ from [2] and [3]?\\nResponse 1:[1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering\\nsome food to [2] and [3].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Response 1:[1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering\\nsome food to [2] and [3].\\nPrompt 2:What food is [4] offering?←Users can directly refer to objects in the memory\\nResponse 2:[4] is offering carrots.\\nMemory Pre-fillingThis operation is triggered upon the detection of <REF> tokens in the input\\nprompt, aiming to thoroughly analyze the referred objects and predict their corresponding masks. In\\nthis stage, the model responds with object IDs and <SEG> tokens for the relevant objects according to\\nthe context, and predicts their spatial-temporal masks accordingly. These object-mask pairs are then\\nsaved into the object memory bank.\\nMemory InjectionWe inject the features of the saved objects into the prompt to enhance object-\\nawareness. Similar to the mask prompt encoder described in Sec. 3.1, each frame-level object mask\\nis downsampled to match the resolution of visual tokens. We then apply masked pooling to aggregate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='awareness. Similar to the mask prompt encoder described in Sec. 3.1, each frame-level object mask\\nis downsampled to match the resolution of visual tokens. We then apply masked pooling to aggregate\\nobject-centric features. Each frame-level mask is condensed into a single feature token, projected\\nthrough the mask projector, and subsequently utilized to replace the corresponding <MEM> token\\nin the memory-injected prompt. Through thispre-filling and injectionmechanism, object-centric\\ninformation is effectively integrated into the model inference process.\\nWhy using object memory bank?An alternative is directly appending a <SEG> token to each\\n<REF> token, followed by masked pooled features obtained during inference. However, we do not\\nadopt this approach for two reasons: (1) During mask prediction, the <SEG> tokens, due to the\\nunidirectional nature of causal self-attention, are unable to aggregate the full context of the prompt,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='adopt this approach for two reasons: (1) During mask prediction, the <SEG> tokens, due to the\\nunidirectional nature of causal self-attention, are unable to aggregate the full context of the prompt,\\nthereby compromising the quality of predicted masks. (2) By utilizing the object memory bank, we\\ncan effectively decouple regional understanding and mask prediction, allowing each to benefit from\\nreferring and segmentation data during training, thus enhancing both capabilities.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison with state-of-the-art methods on ReVOS [96] val split. The best and second-\\nbest results are markedboldand underlined , respectively.\\nMethod Size\\nReferring Reasoning Overall\\nR\\nJ F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nMTTR [5] – 29.8 30.2 30.0 20.4 21.5 21.0 25.1 25.9 25.5 5.6\\nLMPM [21] – 29.0 39.1 34.1 13.3 24.3 18.8 21.2 31.7 26.4 3.2\\nReferFormer [85] – 31.2 34.3 32.7 21.3 25.6 23.4 26.2 29.9 28.1 8.8\\nLLM-based Generalists\\nLISA [32] 13B 45.2 47.9 46.6 34.3 39.1 36.7 39.8 43.5 41.6 8.6\\nTrackGPT [72] 13B 48.3 50.6 49.5 38.1 42.9 40.5 43.2 46.8 45.0 12.8\\nVISA [96] 13B 55.6 59.1 57.4 42.0 46.7 44.3 48.8 52.9 50.9 14.5\\nHyperSeg [81] 3B 56.0 60.9 58.5 50.2 55.8 53.0 53.1 58.4 55.7 –\\nInstructSeg [82] 3B 54.8 59.2 57.0 49.2 54.7 51.9 52.0 56.9 54.5 –\\nGLUS [39] 7B 56.0 60.7 58.3 48.8 53.9 51.4 52.4 57.3 54.9 17.9\\nViLLa [112] 6B – – – – – – 54.9 59.1 57.0 –\\nSa2V A [101] 4B – – – – – – – – 53.2 –\\nUniPixel(Ours) 3B 62.3 66.7 64.5 57.1 62.1 59.6 59.7 64.4 62.1 19.0'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='ViLLa [112] 6B – – – – – – 54.9 59.1 57.0 –\\nSa2V A [101] 4B – – – – – – – – 53.2 –\\nUniPixel(Ours) 3B 62.3 66.7 64.5 57.1 62.1 59.6 59.7 64.4 62.1 19.0\\nUniPixel(Ours) 7B 63.9 67.8 65.8 59.4 63.7 61.5 61.7 65.7 63.7 19.4\\n3.3 Mask Decoder\\nWe adopt SAM 2.1 [ 66] as the mask decoder to disentangle the discrete language modeling and\\ncontinuous mask prediction capabilities. For each <SEG> token, we extract its last-layer hidden\\nstates, downsample them via an L→M projector (architecturally identical to the M→L projector),\\nand reshape them into two tokens. Using two tokens ensures better preservation of object information\\nwhen downsampling from high- to low-dimensional channel space. These tokens prompt the mask\\ndecoder to predict the mask on the first frame, which is then propagated to the other frames.\\n3.4 Model Training\\nThe training loss for UniPixel is a linear combination of language modeling loss and mask decoding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='3.4 Model Training\\nThe training loss for UniPixel is a linear combination of language modeling loss and mask decoding\\nlosses [66], including a focal loss and dice loss for mask prediction, a mean-absolute-error (MAE)\\nloss for IoU prediction, and a cross-entropy loss for objectness prediction. The loss weights are set\\nto 1, 100, 5, 5, and 5, respectively. We train the model through a three-stage progressive alignment\\nrecipe. The datasets are listed in Tab. 12. In the first stage, we pre-train the sparse prompt encoder\\nusing 851K regional captioning data. Then, we align the LLM and mask decoder by training the\\nL→M projector on 87K referring segmentation data. In the last stage, we further unfreeze the M→L\\nprojector and mask decoder, and apply LoRA [26] on the visual encoder and LLM. The model is\\njointly trained on a large-scale corpus with around 1M samples for diverse tasks.\\n4 Experiments'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='projector and mask decoder, and apply LoRA [26] on the visual encoder and LLM. The model is\\njointly trained on a large-scale corpus with around 1M samples for diverse tasks.\\n4 Experiments\\nWe evaluate the effectiveness of UniPixel by conducting extensive experiments across a diverse set of\\nbenchmarks. Specifically, we study the following research questions.\\nQ1. Whether UniPixel is flexible and effective on basic image/video referring and segmentation\\ntasks compared to the corresponding representative methods?\\nQ2. How does it perform on the more challenging PixelQA task, which requires joint referring,\\nsegmentation, and question answering in videos?\\nQ3. What effects does each architectural design contribute? More importantly, does the unified\\nmodeling of referring and segmentation lead to a mutual reinforcement effect?\\nDetailed information about the benchmarks, evaluation metrics, implementation details, and more\\nexperimental results can be found in the appendix.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Detailed information about the benchmarks, evaluation metrics, implementation details, and more\\nexperimental results can be found in the appendix.\\n4.1 Q1: Comparison with State-of-the-Arts on Referring and Segmentation Tasks\\nReasoning Video Object SegmentationWe begin with the most challenging ReVOS [ 96] dataset,\\nwhich requires models to predict masks based on implicit text queries demanding complex reasoning\\nabilities based on world knowledge. The results are shown in Tab. 1. Our 3B variant outperforms all\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 2: Comparison with state-of-the-art methods on referring video object segmentation (RVOS)\\nand motion-grounded video reasoning datasets, including MeViS [21] (val), Ref-YouTube-VOS [71]\\n(val), Ref-DA VIS17 [62] (val), and GroundMoRe [20] (test). The best and second-best results are\\nmarkedboldand underlined , respectively.\\nMethod Size\\nMeViS Ref-YouTube-VOS Ref-DA VIS17 GroundMoRe\\nJ F J&F J F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nReferFormer [85] – 29.8 32.2 31.0 61.3 64.6 62.9 58.1 64.1 61.1 11.2 14.3 12.7\\nLMPM [21] – 34.2 40.2 37.2 – – – – – – 12.7 14.0 13.3\\nOnlineRefer [83] – – – – 61.6 65.5 63.5 61.6 67.7 64.8 – – –\\nLLM-based Generalists\\nPixelLM [69] 7B 36.3 41.1 38.7 54.3 55.7 55.0 63.4 70.0 66.7 9.9 10.0 10.0\\nLISA [32] 13B 35.8 40.0 37.9 54.0 54.8 54.4 63.2 68.8 66.0 6.3 6.7 6.5\\nVISA [96] 13B 41.8 47.1 44.5 61.4 64.7 63.0 67.0 73.8 70.4 5.3 4.7 5.9\\nVideoLISA [4] 3.8B 41.3 47.6 44.4 61.7 65.7 63.7 64.9 72.7 68.8 – – –'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='VISA [96] 13B 41.8 47.1 44.5 61.4 64.7 63.0 67.0 73.8 70.4 5.3 4.7 5.9\\nVideoLISA [4] 3.8B 41.3 47.6 44.4 61.7 65.7 63.7 64.9 72.7 68.8 – – –\\nVideoGLaMM [56] 3.8B 42.1 48.2 45.2 65.4 68.2 66.8 73.3 65.6 69.5 – – –\\nViLLa [112] 6B 46.5 52.3 49.4 64.6 70.4 67.5 70.6 78.0 74.3 – – –\\nGLUS [39] 7B 48.5 54.2 51.3 65.5 69.0 67.3 – – – – – –\\nSa2V A [101] 4B – – 46.2 – – 70.0 – – 73.8 – – –\\nMoRA [20] 7B – – – – – – – – – 27.4 26.9 27.2\\nUniPixel(Ours) 3B 50.4 55.7 53.1 68.6 72.3 70.5 70.7 77.8 74.2 36.0 38.7 37.4\\nUniPixel(Ours) 7B 53.2 58.3 55.8 69.5 72.4 71.0 72.7 80.1 76.4 36.5 39.1 37.8\\nTable 3: Comparison with state-of-the-art methods on image referring expression segmentation (RES)\\nand reasoning segmentation datasets, including RefCOCO/+/g [29, 55] and ReasonSeg [32] (val).\\nThe best and second-best results are markedboldand underlined , respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg ReasonSeg\\nval testA testB val testA testB val(U) test(U) gIoU cIoU\\nNon-LLM-based Specialists'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Method Size\\nRefCOCO RefCOCO+ RefCOCOg ReasonSeg\\nval testA testB val testA testB val(U) test(U) gIoU cIoU\\nNon-LLM-based Specialists\\nReLA [41] – 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0 – –\\nX-Decoder [116] – – – – – – – 64.6 – 22.6 17.9\\nSEEM [117] – – – – – – – 65.7 – 25.5 21.2\\nLLM-based Image Generalists\\nNExT-Chat [104] 7B 74.7 78.9 69.5 65.1 71.9 56.7 67.0 67.0 – –\\nPixelLM [69] 7B 73.0 76.5 68.2 66.3 71.7 58.3 69.3 70.5 – –\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6 61.3 62.9\\nGroundhog [110] 7B 78.5 79.9 75.7 70.5 75.0 64.9 74.1 74.6 56.2 –\\nLaSagnA [80] 7B 76.8 78.7 73.8 66.4 70.6 60.1 70.6 71.9 48.8 47.2\\nM2SA [27] 13B 74.6 77.6 71.0 64.0 68.1 57.6 69.0 69.3 – –\\nLLM-based Video Generalists\\nVideoLISA [4] 3.8B 73.8 76.6 68.8 63.4 68.8 56.2 68.3 68.8 61.4 67.1\\nVISA [96] 7B 72.4 75.5 68.1 59.8 64.8 53.1 65.5 66.4 52.7 57.8\\nVitron [23] 7B 75.5 79.5 72.2 66.7 72.5 58.0 67.9 68.9 – –\\nSa2V A [101] 4B 78.9 – – 71.7 – – 74.1 – – –'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='VISA [96] 7B 72.4 75.5 68.1 59.8 64.8 53.1 65.5 66.4 52.7 57.8\\nVitron [23] 7B 75.5 79.5 72.2 66.7 72.5 58.0 67.9 68.9 – –\\nSa2V A [101] 4B 78.9 – – 71.7 – – 74.1 – – –\\nUniPixel(Ours) 3B 80.5 82.6 76.9 74.3 78.9 68.4 76.3 77.0 64.0 56.2\\nUniPixel(Ours) 7B 80.8 83.0 77.4 75.3 80.1 70.0 76.4 77.1 60.5 58.7\\nexisting methods with larger LLMs (including Sa2V A-4B [101] also with SAM 2 decoder), achieving\\n62.1 overall J&F . The 7B model further boosts the performance to 64.0 J&F – an improvement\\nof 12% over the previous state-of-the-art – demonstrating that UniPixel can effectively understand\\nimplicit queries based on its world knowledge, and accurately generate masks as responses.\\nReferring Video Object SegmentationThe performance comparisons on MeViS [ 21], Ref-\\nYouTube-VOS [71], and Ref-DA VIS17 [62] datasets are presented in Tab. 2. UniPixel consistently\\nachieves the best performance among its counterparts. Its advantage is particularly evident on the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='YouTube-VOS [71], and Ref-DA VIS17 [62] datasets are presented in Tab. 2. UniPixel consistently\\nachieves the best performance among its counterparts. Its advantage is particularly evident on the\\nmore challenging MeViS dataset, where our 3B model outperforms GLUS-7B [39] by around 3.5%,\\nas well as the similarly sized VideoGLaMM-3.8B [56] by 17%. More experimental results on MeViS\\n[21] valu set and Ref-SA V [101] val set are provided in Tab. 4 and Tab. 5, respectively. Ref-SA V\\nfeatures long referring descriptions, large object motion, large camera motion, and heavy occlusion\\ncompared with existing datasets. Given these complex descriptions and video content, our method\\nconsistently performs better than counterparts, including those fine-tuned on the target dataset.\\nMotion-Grounded Video ReasoningWe also evaluate our method on GroundMoRe [20] dataset\\n(results shown in Tab. 2), which highlights visual answer generation that requires joint spatial and\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 4: Experimental results on MeViS [21] valu\\nset. Post means applying post optimization.\\nMethod Size FT J F J&F\\nLMPM [21] –✗ 36.5 43.9 40.2\\nLISA [32] 7B✗ 39.9 46.5 43.2\\nLISA [32] + XMem [15] 7B✗ 41.9 49.3 45.6\\nVideoLISA [4] 7B✗ 48.4 54.9 51.7\\nVideoLISA [4] + Post 7B✗ 50.9 58.1 54.5\\nSa2V A [101] 4B✗ – – 52.1\\nSa2V A [101] 8B✗ – – 57.0\\nUniPixel(Ours) 3B ✗ 56.1 63.2 59.7\\nUniPixel(Ours) 7B ✗ 58.4 65.0 61.7\\nTable 5: Comparison on Ref-SA V [101] val set.\\nFT means fine-tuning after pre-/co-training.\\nMethod Size FT J F J&F\\nUniRef++ [86] –✗ 11.6 9.5 10.5\\nUNINEXT [95] –✗ 8.8 6.4 7.6\\nLMPM [21] –✗ 12.2 9.8 10.3\\nVISA [96] 7B✗ 13.2 11.3 11.8\\nSa2V A [101] 8B✗ 39.6 43.0 41.3\\nUniRef++ [86] –✓ 15.8 13.4 14.6\\nSa2V A [101] 8B✓ 48.3 51.7 50.0\\nUniPixel(Ours) 3B ✗ 66.9 67.6 67.2\\nUniPixel(Ours) 7B ✗ 68.5 69.6 69.0\\nTable 6: Fine-tuned performance on referring expression segmentation (RES) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 6: Fine-tuned performance on referring expression segmentation (RES) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6\\nGSV A [87] 7B 77.2 78.9 73.5 65.9 69.6 59.8 72.7 73.3\\nOMG-LLaV A [109] 7B 78.0 80.3 74.1 69.1 73.1 63.0 72.9 72.9\\nGLaMM [65] 7B 79.5 83.2 76.9 72.6 78.7 64.6 74.2 74.9\\nSa2V A [101] 4B 80.4 – – 74.3 – – 75.7 –\\nUniPixel(Ours) 3B 81.9 83.5 78.6 75.3 80.3 70.6 77.2 78.5\\nUniPixel(Ours) 7B 83.0 84.9 80.4 77.8 82.3 72.7 78.7 79.7\\nTable 7: Experimental results on referring expression comprehension (REC) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nOFA [78] – 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Method Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nOFA [78] – 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6\\nShikra [9] 7B 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2\\nMiniGPT-v2 [8] 7B 88.7 91.6 85.3 79.9 85.1 74.4 84.4 84.6\\nVitron [23] 7B 90.9 93.289.3 83.7 89.1 76.9 86.4 87.0\\nUniPixel(Ours) 3B 91.8 93.8 87.5 86.3 90.8 80.3 88.0 88.2\\nUniPixel(Ours) 7B 92.0 94.4 88.1 87.2 91.9 82.1 88.6 88.7\\ntemporal grounding. Note that we mainly compare the results with MoRA [20], which is fine-tuned\\non GroundMoRe while other methods are evaluated under the zero-shot setting. Benefit from the\\nstrong pixel-level reasoning capability, UniPixel significantly performs better than the baseline.\\nReferring Expression Segmentation and Reasoning SegmentationTab. 3 compares the image\\nsegmentation capabilities using explicit and implicit queries. We evaluate our co-trained model on\\nRefCOCO/+/g [29, 55] and ReasonSeg [32]. While state-of-the-art performance has been achieved'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='segmentation capabilities using explicit and implicit queries. We evaluate our co-trained model on\\nRefCOCO/+/g [29, 55] and ReasonSeg [32]. While state-of-the-art performance has been achieved\\non RES datasets, we observe that the reasoning segmentation data (239 samples) can be easily\\noverwhelmed by the other samples during training due to its limited size. Tab. 6 presents the RES\\nperformance after fine-tuning. We follow the common practice that jointly fine-tunes the model on\\nRefCOCO/+/g datasets [29, 55], and then evaluate on them separately. These results demonstrate the\\ngeneralizability of UniPixel when facing both explicit and implicit queries.\\nReferring Expression ComprehensionOur method also supports referring expression compre-\\nhension by inferring the bounding boxes from predicted masks. Its performance (accuracy with IoU\\n⩾ 0.5) is compared with representative methods in Tab. 7. Benefiting from the high-quality mask'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='hension by inferring the bounding boxes from predicted masks. Its performance (accuracy with IoU\\n⩾ 0.5) is compared with representative methods in Tab. 7. Benefiting from the high-quality mask\\nprediction, UniPixel can also achieve very competitive performance on this simpler task.\\nReferred Video Description and Question AnsweringWe study UniPixel’s regional understand-\\ning capabilities on VideoRefer-Bench [103], which contains two subsets for description and question\\nanswering tasks. The comparisons are in Tab. 8 and Tab. 9. BQ, SQ, RQ, CQ, and FP denote basic\\nquestions, sequential questions, relational questions, reasoning questions, and future predictions,\\nrespectively. Both tasks leverage mask prompts as inputs, where single-frame and multi-frame modes\\ndenote applying the masks only on a specific frame and on all frames, respectively. UniPixel can\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 8: Comparison with state-of-the-art methods on VideoRefer-Bench D [103]. The best and\\nsecond-best results are markedboldand underlined , respectively.\\nMethod Size\\nSingle-Frame Multi-Frame\\nSC AD TD HD Avg. SC AD TD HD Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B 2.62 1.58 2.19 2.07 2.12 3.09 1.94 2.50 2.41 2.48\\nQwen2-VL [77] 7B 2.97 2.24 2.03 2.31 2.39 3.30 2.54 2.22 2.12 2.55\\nInternVL2 [74] 26B 3.55 2.99 2.57 2.25 2.84 4.08 3.353.08 2.28 3.20\\nGPT-4o-mini [59] – 3.56 2.85 2.87 2.38 2.92 3.89 3.18 2.62 2.50 3.05\\nGPT-4o [59] – 3.34 2.96 3.01 2.50 2.95 4.15 3.31 3.11 2.43 3.25\\nImage Referring LMMs\\nFerret [98] 7B 3.08 2.01 1.54 2.14 2.19 3.20 2.38 1.97 1.38 2.23\\nOsprey [102] 7B 3.19 2.16 1.54 2.45 2.34 3.30 2.66 2.10 1.58 2.41\\nVideo Referring LMMs\\nElysium [75] 7B 2.35 0.30 0.02 3.59 1.57 – – – – –\\nArtemis [63] 7B – – – – – 3.42 1.34 1.39 2.90 2.26\\nVideoRefer [103] 7B 4.41 3.273.03 2.973.42 4.443.27 3.10 3.043.46\\nUniPixel(Ours) 3B 4.04 3.15 3.10 3.37 3.42 4.08 3.13 3.13 3.42 3.44'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Artemis [63] 7B – – – – – 3.42 1.34 1.39 2.90 2.26\\nVideoRefer [103] 7B 4.41 3.273.03 2.973.42 4.443.27 3.10 3.043.46\\nUniPixel(Ours) 3B 4.04 3.15 3.10 3.37 3.42 4.08 3.13 3.13 3.42 3.44\\nUniPixel(Ours) 7B 3.83 3.07 2.96 3.62 3.37 3.82 3.05 3.01 3.57 3.36\\nTable 9: Comparison with state-of-the-art methods on\\nVideoRefer-BenchQ [103] (mask prompts). MF denotes\\nmulti-frame mode. Full question types are in Sec. 4.1.\\nMethod Size MF BQ SQ RQ CQ FP Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B✗ 58.7 62.9 64.7 87.4 76.3 67.4\\nQwen2-VL [77] 7B✗ 62.0 69.6 54.9 87.3 74.6 66.0\\nInternVL2 [74] 26B✗ 58.5 63.5 53.4 88.0 78.9 65.0\\nGPT-4o-mini [59] –✗ 57.6 67.1 56.5 85.9 75.4 65.8\\nGPT-4o [59] –✗ 62.374.5 66.088.0 73.7 71.3\\nImage Referring LMMs\\nFerret [98] 7B✗ 35.2 44.7 41.9 70.4 74.6 48.8\\nOsprey [102] 7B✗ 45.9 47.1 30.0 48.6 23.7 39.9\\nVideo Referring LMMs\\nVideoRefer [103] 7B✗ 75.468.6 59.389.478.1 71.9\\nUniPixel(Ours) 3B ✗ 73.6 70.3 60.7 88.8 78.0 72.2\\nUniPixel(Ours) 7B ✗ 68.9 73.1 64.7 88.8 83.3 73.4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Video Referring LMMs\\nVideoRefer [103] 7B✗ 75.468.6 59.389.478.1 71.9\\nUniPixel(Ours) 3B ✗ 73.6 70.3 60.7 88.8 78.0 72.2\\nUniPixel(Ours) 7B ✗ 68.9 73.1 64.7 88.8 83.3 73.4\\nVideoRefer [103] 7B✓ – 70.6 60.5 – – 72.1\\nUniPixel(Ours) 3B ✓ 75.3 70.7 62.3 87.4 77.2 72.8\\nUniPixel(Ours) 7B ✓ 70.6 74.6 64.7 88.8 82.5 74.1\\nTable 10: Evaluation results on our newly\\nintroduced PixelQA task. All the visual\\nprompts are applied in a single frame. See\\nSec. 4.2 for detailed settings.\\nMethod Size J F J&F Acc\\nPoint Prompts\\nInternVL2 [74] 26B – – – 60.8\\nQwen2-VL [77] 72B – – – 69.3\\nUniPixel(Ours) 3B 57.3 64.4 60.9 71.1\\nUniPixel(Ours) 7B 42.1 47.1 44.6 71.4\\nBox Prompts\\nInternVL2 [74] 26B – – – 61.3\\nQwen2-VL [77] 72B – – – 69.0\\nUniPixel(Ours) 3B 57.8 64.7 61.3 70.3\\nUniPixel(Ours) 7B 41.1 46.4 43.8 71.4\\nMixed (50% Points + 50% Boxes)\\nInternVL2 [74] 26B – – – 60.9\\nQwen2-VL [77] 72B – – – 69.1\\nUniPixel(Ours) 3B 57.2 64.1 60.6 70.8\\nUniPixel(Ours) 7B 42.3 47.5 44.9 71.4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Mixed (50% Points + 50% Boxes)\\nInternVL2 [74] 26B – – – 60.9\\nQwen2-VL [77] 72B – – – 69.1\\nUniPixel(Ours) 3B 57.2 64.1 60.6 70.8\\nUniPixel(Ours) 7B 42.3 47.5 44.9 71.4\\neffectively comprehend both types of prompts, and accurately respond with object-centric descriptions\\nor answers, surpassing strong models including GPT-4o [59] and VideoRefer [103].\\n4.2 Q2: Pixel-Level Video Question Answering (PixelQA)\\nWe design the new PixelQA task based on VideoRefer-Bench Q [103], where the original mask\\nprompts are replaced with more challenging point or box prompts. Given these ambiguous visual\\ncues, models are expected to correctly identify the target object according to the question and the\\nvisual prompt, then respond withboth the textual answer and the corresponding object masks.\\nWe report the mask prediction J&F and MCQ accuracy in Tab. 10. Note that none of the existing\\nmethods supports this scenario. Thus, we apply set-of-mark prompts [97] directly on video frames,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='We report the mask prediction J&F and MCQ accuracy in Tab. 10. Note that none of the existing\\nmethods supports this scenario. Thus, we apply set-of-mark prompts [97] directly on video frames,\\nand evaluate the QA accuracies of two strong LMMs [77, 74] as our baselines. Aside from point- or\\nbox-only prompts, we also explore a more flexible setting that randomly chooses different prompts\\nfor different objects. The results verify that ourmemory pre-filling & injectionparadigm effectively\\nenhances the model’s reasoning capabilities. Visualizations of this task are shown in Fig. 5.\\n4.3 Q3: Key Ablation Studies\\nEffect of Task UnificationWe study the effect of task unification in Tab. 11 (a). Unifying referring\\nand segmentation capabilities into a single model and training them jointly leads to better results\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='If [1] continues moving forward, what is a likely future event?(A) The bear will encounter other animals (B) The bear will find a place to rest (C) The bear will start running (D) The bear will climb the stone wall\\nWhat is a likely future event with [1]?(A) He will stop and rest (B) He will start walking slowly (C) He will continue to navigate through more obstacles (D) He will sit down and take a break\\nHow is [4] related to [2]?(A) [4] is holding [2] (B) [4] is controlling [2] with a leash (C) [4] is walking away from [2] (D) [4] is ignoring [2]\\nFigure 5:Visualization of the outputs from UniPixel on PixelQA task. Star marks and boxes\\nrefer to point and box prompts, respectively. The boxed frames denote where the visual prompts are\\napplied. Given different types of visual prompts on a single frame, our method can flexibly infer the\\nrelevant object, track it across the entire video, and involve its features in reasoning.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='applied. Given different types of visual prompts on a single frame, our method can flexibly infer the\\nrelevant object, track it across the entire video, and involve its features in reasoning.\\nTable 11: Key ablation studies with UniPixel-3B on PixelQA (mixed). See Sec. 4.3 for explanations.\\n(a) Task Unification\\nRefer Segment Memory J&F Acc\\n✓ – 64.6\\n✓ 47.5 –\\n✓ ✓ 48.2 67.4\\n✓ ✓ ✓ 49.0 68.5\\n(b) Object Memory Bank\\nReferring Method J&F Acc\\n①<REF> 46.8 64.5\\n②<REF><SEG> 47.8 64.9\\n③<REF><SEG>+ Pooling 47.5 66.3\\n④Object Memory Bank 49.0 68.5\\n(c) Prompt Encoder & Mask Decoder\\nEncoder Decoder J&F Acc\\nw/o Time – 44.3 63.7\\nw/ Time – 49.0 68.5\\n– Independent 46.1 66.2\\n– Propagation 49.0 68.5\\non both tasks (first three rows), demonstrating themutual reinforcement effectof such unification.\\nIncorporating memory pre-filling as an auxiliary task (last row) brings extra improvements.\\nEffect of Object Memory BankTab. 11 (b) verifies the effectiveness of object memory bank. ①'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Incorporating memory pre-filling as an auxiliary task (last row) brings extra improvements.\\nEffect of Object Memory BankTab. 11 (b) verifies the effectiveness of object memory bank. ①\\nmeans using a single token for each referred object. ② means adding an extra segmentation token to\\nsegment it as an auxiliary task. ③ further appends masked-pooled visual tokens after it. The results\\nshow that (1) both adding auxiliary segmentation task and masked-pooled features help regional\\nunderstanding, and (2) decoupling them via object memory bank can further boost the performance.\\nDesign Space of Prompt Encoder & Mask DecoderWe compare different prompt encoder\\nand mask decoder designs in Tab. 11 (c). The performance significantly drops when the temporal\\nencoding in the prompt encoder is removed (first two rows). For the mask decoder (last two rows),\\nwe explore an alternative strategy that treats video frames independently (as batched images), which'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='encoding in the prompt encoder is removed (first two rows). For the mask decoder (last two rows),\\nwe explore an alternative strategy that treats video frames independently (as batched images), which\\ncould largely accelerate inference but lead to sub-optimal accuracies. We hypothesize that this is\\nbecause the LLM-generated <SEG> token cannot well-capture the object information in all frames,\\nthus disentangling the segmentation and tracking capabilities to an external module is reasonable.\\n5 Conclusion\\nIn this work, we proposedUniPixel, a large multi-modal model that supports flexible pixel-level\\nvisual reasoning. It unifies the internal representations of referred and segmented objects through\\na novelobject memory bank. We observe that by such unification, the performance of object\\nreferring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Appendix\\nIn this appendix, we provide more details about the training data, model implementation, and experi-\\nmental settings to complement the main paper. Additional analysis, ablation studies, visualizations,\\nand discussions are also incorporated. Below is the table of contents.\\nA.Model\\n1.Implementation Details\\n2.Training Recipe\\nB.Experiments\\n1.Tasks and Benchmarks\\n2.Evaluation Metrics\\n3.More Experimental Results\\n4.Ablation Studies\\n5.Qualitative Results\\nC.Discussions\\n1.Limitations & Future Work\\n2.Potential Societal Impacts\\nD.Licenses\\nA Model\\nA.1 Implementation Details\\nWe instantiate our base models with 3B and 7B versions of Qwen2.5-VL [3]. Both variants employ\\npre-trained SAM 2.1 [66] with Hiera Base+ [70] backbone as the mask decoder. The M→L projector\\nis initialized with the weights from the V→L projector of Qwen2.5-VL. The hidden size inside the\\nprompt encoder is 256. To reduce GPU memory and accelerate training, we randomly sample 8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='is initialized with the weights from the V→L projector of Qwen2.5-VL. The hidden size inside the\\nprompt encoder is 256. To reduce GPU memory and accelerate training, we randomly sample 8\\nframes per video, with each frame resized to 3162 ∼448 2 pixels (128∼256 tokens per frame). The\\nframe sampling strategies follow the specifications of each benchmark during inference. The mask\\ndecoder has a fixed resolution of 768 ×768. For each segmentation sample, up to 5 objects are\\nrandomly selected to compute the mask prediction losses. During training, LoRA adapters [26] with\\nrank=128 and alpha=256 are applied to all QKVO layers in the visual encoder and LLM. The input\\nsequences are restricted to 4K tokens. We train the model with 8 RTX A6000 Ada (48G) GPUs, with\\na global batch size of 256 for stages 1 and 2, and 32 for stage 3. In the first two stages, the learning\\nrates are set to 1e-3. In the last stage, it is set to 5e-6 for the mask decoder and 2e-5 for all the other'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='rates are set to 1e-3. In the last stage, it is set to 5e-6 for the mask decoder and 2e-5 for all the other\\nparameters, respectively. A linear warmup in the first 3% steps followed by cosine decay is adopted\\nin all stages. The configurations of datasets are introduced in the following section.\\nA.2 Training Recipe\\nThe detailed distribution of training datasets for UniPixel is shown in Tab. 12. Within the three-stage\\ntraining recipe, we first pre-train the sparse prompt encoder using short caption samples from Inst-IT\\n[61] and VideoRefer [103]. For each sample, we randomly select a point inside the ground truth\\nmask (50%) or generate an augmented box from it (50%). This stage aims to enable the model with\\nsimple visual prompt comprehension and regional captioning capabilities on images and videos. In\\nthe second stage, we align the LLM and mask decoder using referring object segmentation datasets'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='simple visual prompt comprehension and regional captioning capabilities on images and videos. In\\nthe second stage, we align the LLM and mask decoder using referring object segmentation datasets\\n[29, 55, 71]. We use short caption/query samples for the first two stages to focus on alignment rather\\nthan knowledge learning. For the last stage, we collect a large-scale, high-quality corpus called\\nUniPixel-SFT-1M3 to jointly train the model on diverse pixel-level tasks. The original annotations\\nhave been rewritten using task-specific templates to incorporate instructions. All the repurposed\\ndatasets and pre-processing pipelines will be publicly available to facilitate future research.\\n3 https://huggingface.co/datasets/PolyU-ChenLab/UniPixel-SFT-1M\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 12: The distribution of training datasets for UniPixel. We use different background colors to\\ndenote object referring , object segmentation , regional understanding , memory pre-filling , and\\ngeneral video understanding data, respectively.\\nStage Dataset\\nInputs Outputs\\n#Samples #Repeat Ratio\\nText Image Video Point Box Mask Text Mask\\n1 Inst-IT-Image-Short-Caption [61] ✓ ✓ ✓ ✓ ✓ 351K 1 41.2%\\nVideoRefer-Short-Caption [103] ✓ ✓ ✓ ✓ ✓ 500K 1 58.8%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\n2 RefCOCOg [55] ✓ ✓ ✓ ✓ 22K 5 26.8%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 5 22.0%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 3 9.5%\\n3\\nOsprey-Conversation [102] ✓ ✓ ✓ ✓ 1.4K 5 0.1%\\nOsprey-Detail-Description [102] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nOsprey-Pos-Neg [102] ✓ ✓ ✓ ✓ 20K 5 1.7%\\nVideoRefer-Detailed-Caption [103] ✓ ✓ ✓ ✓ 120K 5 10.1%\\nVideoRefer-QA [103] ✓ ✓ ✓ ✓ 69K 5 5.8%\\nInst-IT-Video-QA [61] ✓ ✓ ✓ ✓ 159K 5 13.4%\\nVideoRefer-QA-Memory [103] ✓ ✓ ✓ ✓ ✓ ✓ 69K 3 3.5%\\nInst-IT-QA-Memory [61] ✓ ✓ ✓ ✓ ✓ ✓ 158K 3 8.0%'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='VideoRefer-QA [103] ✓ ✓ ✓ ✓ 69K 5 5.8%\\nInst-IT-Video-QA [61] ✓ ✓ ✓ ✓ 159K 5 13.4%\\nVideoRefer-QA-Memory [103] ✓ ✓ ✓ ✓ ✓ ✓ 69K 3 3.5%\\nInst-IT-QA-Memory [61] ✓ ✓ ✓ ✓ ✓ ✓ 158K 3 8.0%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCOg [55] ✓ ✓ ✓ ✓ 22K 10 3.7%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 10 3.0%\\nReasonSeg [32] ✓ ✓ ✓ ✓ 1.6K 10 0.3%\\nADE20K [113] ✓ ✓ ✓ ✓ 20K 3 1.0%\\nCOCOStuff [7] ✓ ✓ ✓ ✓ 118K 3 6.0%\\nMapillary Vistas [57] ✓ ✓ ✓ ✓ 18K 3 0.9%\\nPACO-LVIS [64] ✓ ✓ ✓ ✓ 46K 3 2.3%\\nPASCAL-Part [10] ✓ ✓ ✓ ✓ 4.4K 3 0.2%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 5 1.1%\\nRef-DA VIS17 [62] ✓ ✓ ✓ ✓ 0.6K 10 0.1%\\nRef-SA V [101] ✓ ✓ ✓ ✓ 56K 3 2.8%\\nMeViS [21] ✓ ✓ ✓ ✓ 23K 5 1.9%\\nLV-VIS [76] ✓ ✓ ✓ ✓ 11K 3 0.6%\\nViCaS [1] ✓ ✓ ✓ ✓ 41K 3 2.1%\\nReVOS [96] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nGroundMoRe [20] ✓ ✓ ✓ ✓ 5.6K 3 0.3%\\nLLaV A-1.5-Mix-665K [42] ✓ ✓ ✓ 647K 1 10.9%\\nVideoGPT+ Instruct [52] ✓ ✓ ✓ 573K 1 9.7%\\nB Experiments\\nB.1 Tasks and Benchmarks'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='ReVOS [96] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nGroundMoRe [20] ✓ ✓ ✓ ✓ 5.6K 3 0.3%\\nLLaV A-1.5-Mix-665K [42] ✓ ✓ ✓ 647K 1 10.9%\\nVideoGPT+ Instruct [52] ✓ ✓ ✓ 573K 1 9.7%\\nB Experiments\\nB.1 Tasks and Benchmarks\\nOur method is extensively evaluated across 9 fine-grained image/video understanding tasks. The\\nbenchmark(s) used for each task are listed as follows:\\n1.Reasoning Video Object Segmentation:ReVOS [96]\\n2. Referring Video Object Segmentation:MeViS [ 21], Ref-YouTube-VOS [71], Ref-DA VIS17 [62], Ref-SA V [101]\\n3.Motion-Grounded Video Reasoning:GroundMoRe [20]\\n4.Referring Expression Segmentation:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n5.Reasoning Segmentation:ReasonSeg [32]\\n6.Referring Expression Comprehension:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n7.Referred Video Description:VideoRefer-Bench D [103]\\n8.Referred Video Question Answering:VideoRefer-Bench Q [103]\\n9.Flexible Pixel-Level Understanding:PixelQA (Ours)\\nB.2 Evaluation Metrics'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='7.Referred Video Description:VideoRefer-Bench D [103]\\n8.Referred Video Question Answering:VideoRefer-Bench Q [103]\\n9.Flexible Pixel-Level Understanding:PixelQA (Ours)\\nB.2 Evaluation Metrics\\nFor video segmentation tasks, we adopt J&F as the main metric to jointly consider region similarity\\nJ and contour accuracy F. Image segmentation is evaluated using cIoU (the cumulative intersection\\nover the cumulative union) and gIoU (the average of all per-image IoUs) following existing work. For\\nreferred video description and question answering tasks, we follow the official evaluation protocols to\\nreport GPT-4o [59] scores and MCQ accuracy, respectively. For referring expression comprehension,\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 13: Performance comparison on general video question answering (VideoQA) on MVBench\\n[36]. Note that UniPixel is the only model supporting pixel-level referring & segmentation.\\nModel Size AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg.\\nGPT-4V [58] – 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.059.011.0 43.5\\nVideo-ChatGPT [53] 7B 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 39.5 48.5 29.0 33.0 29.5 26.0 35.5 32.7\\nVideo-LLaMA [105] 7B 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1\\nVideoChat [35] 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5\\nVideo-LLaV A [38] 7B 46.0 42.5 56.5 39.0 53.5 53.0 48.0 41.0 29.0 31.5 82.5 45.0 26.0 53.0 41.5 33.5 41.5 27.5 38.5 31.5 43.0\\nTimeChat [68] 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='TimeChat [68] 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5\\nPLLaV A [94] 7B 58.0 49.0 55.5 41.0 61.0 56.0 61.0 36.0 23.5 26.0 82.0 39.5 42.0 52.0 45.0 42.0 53.5 30.5 48.0 31.0 46.6\\nST-LLM [44] 7B 66.0 53.584.044.0 58.5 80.5 73.5 38.5 42.5 31.0 86.5 36.5 56.5 78.5 43.0 44.5 46.5 34.5 41.5 58.5 54.9\\nVideoGPT+ [52] 4B 69.0 60.0 83.0 48.5 66.5 85.575.536.0 44.0 34.0 89.5 39.5 71.0 90.5 45.0 53.0 50.0 29.5 44.0 60.0 58.7\\nVideoChat2 [36] 7B 75.558.0 83.5 50.560.5 87.5 74.5 45.047.544.082.5 37.0 64.5 87.5 51.0 66.547.035.037.0 72.5 60.4\\nUniPixel(Ours) 3B 69.5 62.5 83.0 48.5 76.5 86.5 66.5 38.0 49.0 40.5 87.0 49.0 74.0 95.0 49.0 45.0 63.5 34.5 58.0 73.5 62.5\\nUniPixel(Ours) 7B 71.0 68.0 84.0 45.0 78.0 91.5 66.5 35.5 57.5 43.0 91.5 47.0 73.5 92.5 58.0 53.0 74.0 37.5 49.0 69.0 64.3\\nTable 14: Effectiveness justification of multi-stage training. The best and second-best results are'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 14: Effectiveness justification of multi-stage training. The best and second-best results are\\nmarkedboldand underlined , respectively. The three-stage recipe leads to optimal performance.\\nStage 1 Stage 2 Stage 3\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ 58.3 63.6 61.0 54.8 61.9 58.4 71.1 71.5\\n✓ ✓ 59.0 63.4 61.2 55.2 62.1 58.7 71.8 72.3\\n✓ ✓ 59.6 63.5 61.6 55.7 62.5 59.1 71.2 71.6\\n✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nwe leverage mean accuracies, where a predicted bounding box is considered correct when it has the\\nintersection over union (IoU) with the ground truth no less than 0.5.\\nB.3 More Experimental Results\\nGeneral Video Question AnsweringWe also evaluate UniPixel on MVBench [ 36] to compare its\\ngeneral video understanding capabilities with existing methods. The results are illustrated in Tab. 13.\\nNote that our method is the only one in the table that supports referring and segmentation. By jointly'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Note that our method is the only one in the table that supports referring and segmentation. By jointly\\ntraining on holistic-level and pixel-level data, UniPixel can effectively balance the capabilities under\\nboth scenarios, demonstrated by the strong performance compared with holistic-level models.\\nB.4 Ablation Studies\\nEffect of Multi-stage TrainingWe investigate the effectiveness of multi-stage training in Tab. 14.\\nAs shown in the first line, directly training the model using large-scale data only leads to sub-optimal\\nperformance, due to the unaligned representations among prompt encoder, LLM, and mask decoder.\\nWe observe that pre-training either the sparse prompt encoder or the L→M projector (the second and\\nthird lines) brings performance gains on both tasks (referring and segmentation). We hypothesize that\\nthis is because pre-aligning either of them can alleviate the burden of joint-task learning in stage 3.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='this is because pre-aligning either of them can alleviate the burden of joint-task learning in stage 3.\\nThe last row verifies that the performance can be further boosted by pre-aligning both of them.\\nNumber of Hidden Tokens for Mask DecoderAs mentioned in the main paper, there is a huge\\ngap between the feature dimensions of the LLM and the mask decoder, thus splitting the<SEG> token\\ninto more hidden tokens can better preserve the object information from the LLM. We ablate this\\nmechanism in Tab. 15. According to the results, using only 1 hidden token cannot fully preserve the\\nobject information, as the mask prediction performance is sub-optimal. However, we also observe\\nthat using more than 2 hidden tokens (e.g., 4 or 8) only brings negligible performance gain. Therefore,\\nwe choose 2 hidden tokens per object in our final model.\\nTraining Strategy for the M→L projectorThe M →L projector aims to project the masked-pooled'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='we choose 2 hidden tokens per object in our final model.\\nTraining Strategy for the M→L projectorThe M →L projector aims to project the masked-pooled\\nobject-centric features to the LLM’s embedding space. Since the object features originate from the\\nvisual encoder, it is possible to re-use the pre-trained weights of the original V →L projector in\\nQwen2.5-VL. Its effects are studied in Tab. 16. We investigated two strategies: 1) re-using the\\nweights and 2) adding an extra pre-training stage for better alignment. The comparison shows that\\ndirectly re-using weights without extra pre-training can achieve the best results.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Table 15: Ablation study on the number of hid-\\nden tokens for each <SEG>. Performance gains\\nare negligible with more than 2 tokens/object.\\n#Tokens\\nReVOS MeViS(val u)\\nJ F J&F J F J&F\\n1 59.6 63.5 61.6 55.8 62.5 59.2\\n2 59.7 64.4 62.1 56.1 63.2 59.7\\n4 59.8 63.9 61.9 56.863.1 59.9\\n8 59.5 64.0 61.8 56.4 62.8 59.6\\nTable 16: Ablation study on M→L projector. Init\\nand PT denote weight initialization from V→L\\nprojector and extra pre-training, respectively.\\nInit PT\\nVideoRefer-BenchQ PixelQA\\nSingle-Frame Multi-Frame Mixed Acc\\n71.4 71.9 67.7\\n✓ 71.5 71.7 67.4\\n✓ 72.4 72.6 68.2\\n✓ ✓ 72.2 72.8 68.5\\nTable 17: Ablation study on training data used in stage 3. The best and second-best results are marked\\nboldand underlined, respectively. Gradually adding more pixel-level data brings performance gains.\\nRegional Segmentation Memory General\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ – – – – – – 72.1 72.0\\n✓ 58.9 63.8 61.4 56.0 63.2 59.6 – –'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Regional Segmentation Memory General\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ – – – – – – 72.1 72.0\\n✓ 58.9 63.8 61.4 56.0 63.2 59.6 – –\\n✓ ✓ 59.2 63.7 61.5 55.8 63.1 59.5 72.3 72.6\\n✓ ✓ ✓ 59.6 64.5 62.1 56.3 63.5 59.9 72.472.5\\n✓ ✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nCombination of Training DataTab. 17 studies the effect of the combination of multi-task co-\\ntraining data in stage 3. Compared with training only on the regional or segmentation data, leveraging\\nboth of them leads to considerable performance on both tasks. Incorporating memory pre-filling data\\n(requiring both referring and segmentation) can further boost the performance. We also mix some\\ngeneral holistic-level video understanding data to preserve the original capabilities of the pre-trained\\nmodel, while it slightly affects the performance on pixel-level tasks.\\nB.5 Qualitative Results'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='general holistic-level video understanding data to preserve the original capabilities of the pre-trained\\nmodel, while it slightly affects the performance on pixel-level tasks.\\nB.5 Qualitative Results\\nFig. 6∼11 present more visualizations of outputs from UniPixel on different pixel-level understanding\\ntasks. Our method can effectively handle flexible visual prompts [103], implicit queries [32, 96], long\\nqueries [101], and motion-grounded questions [20].\\nC Discussion\\nC.1 Limitations & Future Work\\nDue to the limited computing resources, we did not further scale up the training data to incorporate\\nmore pixel-level tasks such as grounded caption generation (GCG) on images [65] or videos [56],\\nwhich are interesting scenarios and their data may bring more performance gains. Besides, the mask\\ndecoder currently predicts the first mask on the first frame and propagates it to the following frames,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='decoder currently predicts the first mask on the first frame and propagates it to the following frames,\\nwhile it potentially supports predicting on the best frame (defined as the frame with the best view of\\nthe target) and propagates it to both sides of the video. We will focus in our future work to explore\\nmore pixel-level understanding tasks and more flexible mechanisms for the mask decoder.\\nC.2 Potential Societal Impacts\\nThis work introduces a new framework for pixel-level visual-language understanding, which could\\npotentially be used in education, surveillance, and healthcare industries, where flexible interactions\\nwith the users and fine-grained understanding of images & videos are required. In other scenarios\\nrequiring multi-modal assistants, our method can also serve as a more advanced alternative. To the\\nbest of our knowledge, there are no potential negative societal impacts to declare.\\nD Licenses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='requiring multi-modal assistants, our method can also serve as a more advanced alternative. To the\\nbest of our knowledge, there are no potential negative societal impacts to declare.\\nD Licenses\\nOur model is built based on the pre-trained Qwen2.5-VL [3] and SAM 2.1 [66] models. They are both\\nlicensed under the Apache License 2.0 (https://www.apache.org/licenses/LICENSE-2.0).\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content=\"If [1] continues to move forward, what is a likely future event involving [2]?(A) [2] will run away (B) [2] will sit down and stop moving (C) [2] will start barking (D) [2] will continue walking by the wheelchair\\nWhat action does [1] perform that involves [3]?(A) [1] extends an arm across [3]'s chest (B) [1] hands something to [3] (C) [1] talks to [3] (D) [1] ignores [3]\\nWhat is [1] wearing?(A) Blue sweatshirt and black jeans (B) Red sweatshirt and light blue jeans (C) Green t-shirt and white pants (D) Yellow hoodie and dark blue jeans\\nIf <object1><region> continues riding the bike, what is a likely future event?(A) [1] will stop (B) [1] will start running (C) [1] will change a different outfit (D) [1] will continue to challenge different high difficulty movements\\nFigure 6: Visualization of the predictions from UniPixel on PixelQA.\\nPlease segment the zebra which is younger in this video.\\nPlease segment the cow that is the furthest from the camera in this video.\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Please segment the zebra which is younger in this video.\\nPlease segment the cow that is the furthest from the camera in this video.\\nWhich goldfish is on the left side of the screen at the beginning of the video? Please provide the segmentation mask.\\nCan you find the skunk that has black fur all over its body and a tuft of white fur on its head and the tip of its tail?\\nWhere is the instrument that serves to shield from the sun or protect from rain and snow?\\nWhich ferret(s) is/are being licked by a cat consistently? Please provide the segmentation mask.\\nCan you segment the insect(s) belonging to the superfamily Papilionoidea of the Lepidoptera order in this video?\\nFigure 7: Visualization of the predictions from UniPixel on ReVOS [96].\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Where is the man wearing a cap and shorts in this video? Respond with the segmentation mask.\\nCan you find the blue wooden car in the frames?\\nSegment and track the green motorbike in this video.\\nPlease segment the black swan in this video.\\nWhere is the rope? Give me the segmentation results directly.\\nFigure 8: Visualization of the predictions from UniPixel on Ref-DA VIS17 [62].\\nQ: Who might not open the cooler if not for feeding the walrus a fish?                                                       A: The woman.\\nQ: Who opens the ziploc bag to transfer the crushed Oreo cookies into the bowl?                                                  A: The girl.\\nQ: Who dribbles the ball before he shoots it?                                                                      A: The man in the black shorts.\\nQ: Who asked if the little girl could carry the box before she picked it up?                                                 A: The man.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Q: Who asked if the little girl could carry the box before she picked it up?                                                 A: The man.\\nQ: What might not be given to the woman by the man if he did not eat by himself? A: The bag.\\nQ: Who kicks the ball into the goal? A: The boy.\\nFigure 9: Visualization of the predictions from UniPixel on GroundMoRe [20].\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content=\"Findtheobjectaccordingtothedescription:Theobjectisadark-coloredbackpackwithlight-coloredaccents,featuringmultiplecompartmentsandpockets,securelyfastenedtoanindividual'sback.Thepersonisdressedindarkclothingandascendinganescalatorinapublicsetting,likelyamallortransportationhub.Thebackpackhasadjustablestrapsandatophandle,appearingfunctionalforcarryingvariousitems.Theindividualmovessteadilyuptheescalator,indicatingapurposefuljourney.\\nAnalyzethefollowingsentencesandprovidethecorrespondingsegmentationmask:Theobjectisadark-coloredsedan,likelyblueorblack,parkedonanunpavedsurface,possiblyadirtroadoranareawithloosesoil.Ithasfourdoors,avisiblerearspoileronthetrunk,silverwheels,andtintedwindows.Thecarisslightlytilted,suggestingitmightbeparkedonunevengroundorexperiencingsomeformofimbalance.Throughoutthevideo,thesedanremainsstationary,withnoindicationofmovementoractionsbeingperformedbythevehicle.\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Pleasesegmenttheobjectaccordingtothedescription:Theobjectisapersonwithlongdarkhair,wearingadarktopandapatternedskirtwithgeometricdesigns.Thisindividualisstationaryormovingveryslowlyinthebackgroundofaretailstore,possiblyafurnitureorhomegoodsstore.Thepersonremainsincloseproximitytoanothershopperpushingashoppingcart,suggestingtheymightbetogetherorinteracting.Thescenecapturesatypicalshoppingexperience.\\nFigure 10: Visualization of the predictions from UniPixel on Ref-SA V [101].\\nFind the lens that is more suitable for photographing nearby objects.\\n Where is the goat nearest to the bottom stone? Give me the segmentation mask.\\nIn some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture?\\nWhat item in the picture can provide information to help guide travelers through this rugged terrain that can be challenging to navigate?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='What item in the picture can provide information to help guide travelers through this rugged terrain that can be challenging to navigate?\\nPlease localize the place where piano players should sit in this image.\\nWhere is the place where the garbage should be put? Please respond with the segmentation mask.\\nWhich part of the vehicle must be used to display identifying information as required by law? Segment the target directly.\\nSegment the place where the patient lies down to receive examination in this image.\\nFigure 11: Visualization of the predictions from UniPixel on ReasonSeg [32].\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Ali Athar, Xueqing Deng, and Liang-Chieh Chen. Vicas: A dataset for combining holistic and pixel-level\\nvideo understanding using captions with grounded segmentation.arXiv:2412.09754, 2024.\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\\nand Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text\\nreading, and beyond. 2023.\\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\\nWang, Jun Tang, et al. Qwen2.5-vl technical report.arXiv:2502.13923, 2025.\\n[4] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng\\nShou. One token to seg them all: Language instructed reasoning segmentation in videos. InNeurIPS,\\npages 6833–6859, 2024.\\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation\\nwith multimodal transformers. InCVPR, pages 4985–4995, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='pages 6833–6859, 2024.\\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation\\nwith multimodal transformers. InCVPR, pages 4985–4995, 2022.\\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\\nlarge-scale video benchmark for human activity understanding. InCVPR, pages 961–970, 2015.\\n[7] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\\nCVPR, pages 1209–1218, 2018.\\n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krish-\\nnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model\\nas a unified interface for vision-language multi-task learning.arXiv:2310.09478, 2023.\\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic.arXiv:2306.15195, 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic.arXiv:2306.15195, 2023.\\n[10] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect\\nwhat you can: Detecting and representing objects using holistic models and body parts. InCVPR, pages\\n1971–1978, 2014.\\n[11] Yuan Chen, Zi-han Ding, Ziqin Wang, Yan Wang, Lijun Zhang, and Si Liu. Asynchronous large language\\nmodel enhanced planner for autonomous driving. InECCV, pages 22–38, 2024.\\n[12] Zewen Chen, Juan Wang, Wen Wang, Sunhan Xu, Hang Xiong, Yun Zeng, Jian Guo, Shuxun Wang,\\nChunfeng Yuan, Bing Li, othersShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei\\nZhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-\\ninterest.arXiv:2307.03601, 2023.\\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='interest.arXiv:2307.03601, 2023.\\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\\nHao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models\\nwith model, data, and test-time scaling.arXiv:2412.05271, 2024.\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,\\nXizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic\\nvisual-linguistic tasks.arXiv:2312.14238, 2023.\\n[15] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an\\natkinson-shiffrin memory model. InECCV, pages 640–658, 2022.\\n[16] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu,\\nZichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous\\ndriving. InWACV, pages 958–979, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Zichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous\\ndriving. InWACV, pages 958–979, 2024.\\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\\nLi, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\\ninstruction tuning. InNeurIPS, 2024.\\n[18] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era, 2024.\\n[19] Google DeepMind. Gemini 2.5: Our most intelligent ai model, 2025.\\n[20] Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed\\nMian, Mohit Bansal, and Chen Chen. Motion-grounded video reasoning: Understanding and perceiving\\nmotion at pixel level.arXiv:2411.09921, 2024.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[21] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: A large-scale\\nbenchmark for video segmentation with motion expressions. InICCV, pages 2694–2703, 2023.\\n[22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan\\nTompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language\\nmodel.arXiv:2303.03378, 2023.\\n[23] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified\\npixel-level vision llm for understanding, generating, segmenting, editing.arXiv:2412.19806, 2024.\\n[24] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\\nYunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark\\nof multi-modal llms in video analysis.arXiv:2405.21075, 2024.\\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='of multi-modal llms in video analysis.arXiv:2405.21075, 2024.\\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning.arXiv:2501.12948, 2025.\\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models.ICLR, 2022.\\n[27] Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, and Dae-Shik Kim. Mmr: A large-scale\\nbenchmark dataset for multi-target and multi-granularity reasoning segmentation.arXiv:2503.13881,\\n2025.\\n[28] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-\\ntemporal reasoning in visual question answering. InCVPR, pages 2758–2766, 2017.\\n[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='temporal reasoning in visual question answering. InCVPR, pages 2758–2766, 2017.\\n[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\\nin photographs of natural scenes. InEMNLP, pages 787–798, 2014.\\n[30] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael\\nRafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-\\naction model.arXiv:2406.09246, 2024.\\n[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. InICCV, pages 4015–4026,\\n2023.\\n[32] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning\\nsegmentation via large language model. InCVPR, pages 9579–9589, 2024.\\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='segmentation via large language model. InCVPR, pages 9579–9589, 2024.\\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer.arXiv:2408.03326, 2024.\\n[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. InICML, pages 19730–19742, 2023.\\n[35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\\nYu Qiao. Videochat: Chat-centric video understanding.arXiv:2305.06355, 2023.\\n[36] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\\nLuo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. InCVPR, pages\\n22195–22206, 2024.\\n[37] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='22195–22206, 2024.\\n[37] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli\\nZhao, and Wenbing Huang. Star-r1: Spacial transformation reasoning by reinforcing multimodal llms.\\narXiv:2505.15804, 2025.\\n[38] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\\nrepresentation by alignment before projection.arXiv:2311.10122, 2023.\\n[39] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into a\\nsingle large language model for video segmentation.arXiv:2504.07962, 2025.\\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. InECCV, pages 740–755, 2014.\\n[41] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In\\nCVPR, pages 23592–23601, 2023.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning. InCVPR, pages 26296–26306, 2024.\\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. InNeurIPS,\\npages 34892–34916, 2023.\\n[44] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models\\nare effective temporal learners. InECCV, pages 1–18, 2024.\\n[45] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen.\\nr2-tuning: Efficient image-to-video transfer learning for video temporal grounding. InECCV, 2024.\\n[46] Ye Liu, Huifang Li, Chao Hu, Shuang Luo, Yan Luo, and Chang Wen Chen. Learning to aggregate\\nmulti-scale context for instance segmentation in remote sensing images.IEEE Transactions on Neural\\nNetworks and Learning Systems, 36(1):595–609, 2024.\\n[47] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Networks and Learning Systems, 36(1):595–609, 2024.\\n[47] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal\\ntransformers for joint video moment retrieval and highlight detection. InCVPR, pages 3042–3051, 2022.\\n[48] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: A chain-of-lora\\nagent for long video reasoning.arXiv:2503.13444, 2025.\\n[49] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang W Chen. E.t. bench: Towards\\nopen-ended event-level video-language understanding. InNeurIPS, pages 32076–32110, 2024.\\n[50] Ye Liu, Junsong Yuan, and Chang Wen Chen. Consnet: Learning consistency graph for zero-shot\\nhuman-object interaction detection. InACM MM, pages 4235–4243, 2020.\\n[51] Zongyang Ma, Yuxin Chen, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Shaojie Zhu, Chengxiang\\nZhuo, Bing Li, Ye Liu, Zang Li, Ying Shan, and Weiming Hu. Visionmath: Vision-form mathematical\\nproblem-solving. InICCV, 2025.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Zhuo, Bing Li, Ye Liu, Zang Li, Ying Shan, and Weiming Hu. Visionmath: Vision-form mathematical\\nproblem-solving. InICCV, 2025.\\n[52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and\\nvideo encoders for enhanced video understanding.arXiv:2406.09418, 2024.\\n[53] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\\ndetailed video understanding via large vision and language models.arXiv:2306.05424, 2023.\\n[54] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark\\nfor very long-form video language understanding. InNeurIPS, 2024.\\n[55] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\\nGeneration and comprehension of unambiguous object descriptions. InCVPR, pages 11–20, 2016.\\n[56] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Generation and comprehension of unambiguous object descriptions. InCVPR, pages 11–20, 2016.\\n[56] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and\\nSalman Khan. Videoglamm: A large multimodal model for pixel-level visual grounding in videos.\\narXiv:2411.04923, 2024.\\n[57] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas\\ndataset for semantic understanding of street scenes. InICCV, pages 4990–4999, 2017.\\n[58] OpenAI. Gpt-4v(ision) system card, 2023.\\n[59] OpenAI. Gpt-4o system card, 2024.\\n[60] OpenAI. Openai o1 system card, 2024.\\n[61] Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu,\\nZuxuan Wu, and Yu-Gang Jiang. Inst-it: Boosting multimodal instance understanding via explicit visual\\nprompt instruction tuning.arXiv:2412.03565, 2024.\\n[62] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='prompt instruction tuning.arXiv:2412.03565, 2024.\\n[62] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc\\nVan Gool. The 2017 davis challenge on video object segmentation.arXiv:1704.00675, 2017.\\n[63] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye,\\nand Yunjie Tian. Artemis: Towards referential understanding in complex videos. InNeurIPS, 2024.\\n[64] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects.\\nInCVPR, pages 7141–7151, 2023.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[65] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham\\nCholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding\\nlarge multimodal model. InCVPR, pages 13009–13018, 2024.\\n[66] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham\\nKhedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and\\nvideos.arXiv:2408.00714, 2024.\\n[67] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of context.arXiv:2403.05530, 2024.\\n[68] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large\\nlanguage model for long video understanding. InCVPR, pages 14313–14323, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[68] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large\\nlanguage model for long video understanding. InCVPR, pages 14313–14323, 2024.\\n[69] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin.\\nPixellm: Pixel reasoning with large multimodal model. InCVPR, pages 26374–26383, 2024.\\n[70] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal,\\nArkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical vision transformer\\nwithout the bells-and-whistles. InICML, pages 29441–29454, 2023.\\n[71] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation\\nnetwork with a large-scale benchmark. InECCV, pages 208–223, 2020.\\n[72] Nicholas Stroh. Trackgpt–a generative pre-trained transformer for cross-domain entity trajectory forecast-\\ning.arXiv:2402.00066, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[72] Nicholas Stroh. Trackgpt–a generative pre-trained transformer for cross-domain entity trajectory forecast-\\ning.arXiv:2402.00066, 2024.\\n[73] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high\\nfrequency functions in low dimensional domains. InNeurIPS, pages 7537–7547, 2020.\\n[74] OpenGVLab Team. Internvl2: Better than the best—expanding performance boundaries of open-source\\nmultimodal models with the progressive scaling strategy, 2024.\\n[75] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level\\nperception in videos via mllm. InECCV, pages 166–185, 2024.\\n[76] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios\\nGavves. Towards open-vocabulary video instance segmentation. InICCV, pages 4057–4066, 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[76] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios\\nGavves. Towards open-vocabulary video instance segmentation. InICCV, pages 4057–4066, 2023.\\n[77] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\\nresolution.arXiv:2409.12191, 2024.\\n[78] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-\\nto-sequence learning framework. InICML, pages 23318–23340, 2022.\\n[79] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and\\nJose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception,\\nreasoning and planning.arXiv:2405.01533, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception,\\nreasoning and planning.arXiv:2405.01533, 2024.\\n[80] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. Lasagna: Language-based segmentation\\nassistant for complex queries.arXiv:2404.08506, 2024.\\n[81] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. Hyperseg:\\nTowards universal visual segmentation with large language model.arXiv:2411.17606, 2024.\\n[82] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, and Yujiu Yang. Instructseg:\\nUnifying instructed visual segmentation with multi-modal large language models.arXiv:2412.14006,\\n2024.\\n[83] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: A simple\\nonline baseline for referring video object segmentation. InICCV, pages 2761–2770, 2023.\\n[84] Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, and Chang Wen Chen. A survey'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='online baseline for referring video object segmentation. InICCV, pages 2761–2770, 2023.\\n[84] Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, and Chang Wen Chen. A survey\\non video temporal grounding with multimodal large language model.arXiv:2508.10922, 2025.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[85] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video\\nobject segmentation. InCVPR, pages 4974–4984, 2022.\\n[86] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Segment every reference\\nobject in spatial and temporal spaces. InICCV, pages 2538–2550, 2023.\\n[87] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized\\nsegmentation via multimodal large language models. InCVPR, pages 3858–3869, 2024.\\n[88] Linshan Xie, Xuehong Lin, Xunda Wang, Junjian Wen, Teng Ma, Jiahao Hu, Peng Cao, Alex TL Leong,\\nand Ed X Wu. Simultaneous eeg-fmri reveals spontaneous neural oscillatory activity in cingulate cortex\\nunderlying transient rsfmri network dynamics. InISMRM, 2024.\\n[89] Linshan Xie, Xunda Wang, Xuehong Lin, Teng Ma, Junjian Wen, Peng Cao, Alex TL Leong, and Ed X\\nWu. Single-pulse optogenetic perturbation of thalamo-cortical networks reveals functional architecture of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Wu. Single-pulse optogenetic perturbation of thalamo-cortical networks reveals functional architecture of\\nrsfmri networks. InISMRM, 2024.\\n[90] Linshan Xie, Xunda Wang, Xuehong Lin, Junjian Wen, Teng Ma, Alex TL Leong, and Ed X Wu.\\nBrain-wide resting-state fmri network dynamics elicited by activation of single thalamic input.Nature\\nCommunications, 2025.\\n[91] Linshan Xie, Xunda Wang, Teng Ma, Pit Shan Chong, Lee Wei Lim, Peng Cao, Pek-Lan Khong, Ed X Wu,\\nand Alex TL Leong. Are topographically segregated excitatory neurons in visual thalamus functionally\\ndiverse? an optogenetic fmri study. InISMRM, 2022.\\n[92] Linshan Xie, Xunda Wang, Teng Ma, Hang Zeng, Junjian Wen, Peng Cao, Ed X Wu, and Alex TL Leong.\\nShort single pulse optogenetic fmri mapping of downstream targets in thalamo-cortical pathways. In\\nISMRM, 2023.\\n[93] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='ISMRM, 2023.\\n[93] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\\nquestion answering via gradually refined attention over appearance and motion. InACM MM, pages\\n1645–1653, 2017.\\n[94] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free\\nllava extension from images to videos for video dense captioning.arXiv:2404.16994, 2024.\\n[95] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\\nperception as object discovery and retrieval. InCVPR, pages 15325–15336, 2023.\\n[96] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios\\nGavves. Visa: Reasoning video object segmentation via large language models. InECCV, pages 98–115,\\n2024.\\n[97] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\\nunleashes extraordinary visual grounding in gpt-4v.arXiv:2310.11441, 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='2024.\\n[97] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\\nunleashes extraordinary visual grounding in gpt-4v.arXiv:2310.11441, 2023.\\n[98] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao,\\nShih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity.\\narXiv:2310.07704, 2023.\\n[99] Samson Yu, Kelvin Lin, Anxing Xiao, Jiafei Duan, and Harold Soh. Octopi: Object property reasoning\\nwith large tactile-language models.arXiv:2405.02794, 2024.\\n[100] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A\\ndataset for understanding complex web videos via question answering. InAAAI, pages 9127–9134, 2019.\\n[101] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi\\nFeng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of\\nimages and videos.arXiv:2501.04001, 2025.\\n[102] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.\\nOsprey: Pixel understanding with visual instruction tuning. InCVPR, pages 28202–28211, 2024.\\n[103] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao,\\nWenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding\\nwith video llm.arXiv:2501.00599, 2024.\\n[104] Ao Zhang, Yuan Yao, Wei Ji, Zhiyuan Liu, and Tat-Seng Chua. Next-chat: An lmm for chat, detection\\nand segmentation.arXiv:2311.04498, 2023.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[105] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\\nfor video understanding.arXiv:2306.02858, 2023.\\n[106] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation\\nfor autonomous vehicles. InCVPR, pages 15459–15469, 2024.\\n[107] Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi\\nZhang, and Lianwen Jin. Aesthetics is cheap, show me the text: An empirical evaluation of state-of-the-art\\ngenerative models for ocr.arXiv:2507.15085, 2025.\\n[108] Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, and Lianwen Jin. Smaller but better: Unifying\\nlayout generation with smaller large language models.International Journal of Computer Vision,\\n133:3891–3917, 2025.\\n[109] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='133:3891–3917, 2025.\\n[109] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and\\nShuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding.\\nInNeurIPS, pages 71737–71767, 2024.\\n[110] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. InCVPR, pages 14227–14238, 2024.\\n[111] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat\\nwith the environment: Interactive multimodal perception using large language models. InIROS, pages\\n3590–3596, 2023.\\n[112] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, and Hengshuang Zhao. Villa: Video\\nreasoning segmentation with large language model.arXiv:2407.14500, 2024.\\n[113] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. InCVPR, pages 633–641, 2017.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen', 'doi': 'https://doi.org/10.48550/arXiv.2509.18094', 'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.18094v3', 'source': '../data/pdf/2509.18094v3.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': '2509.18094v3.pdf', 'file_type': 'pdf'}, page_content='[113] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. InCVPR, pages 633–641, 2017.\\n[114] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web\\ninstructional videos. InAAAI, 2018.\\n[115] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models.arXiv:2304.10592, 2023.\\n[116] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl,\\nJianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. InCVPR, pages\\n15116–15127, 2023.\\n[117] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao,\\nand Yong Jae Lee. Segment everything everywhere all at once. InNeurIPS, pages 19769–19782, 2023.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL\\nENHANCEMENTS\\nRahima Khanam* and Muhammad Hussain\\nDepartment of Computer Science, Huddersfield University, Queensgate, Huddersfield HD1 3DH, UK;\\n*Correspondence: rahima.khanam@hud.ac.uk;\\nOctober 24, 2024\\nABSTRACT\\nThis study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only\\nLook Once) series of object detection models. We examine the models architectural innovations,\\nincluding the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='feature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object\\ndetection (OBB). We review the model’s performance improvements in terms of mean Average\\nPrecision (mAP) and computational efficiency compared to its predecessors, with a focus on the\\ntrade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11’s\\nversatility across different model sizes, from nano to extra-large, catering to diverse application needs\\nfrom edge devices to high-performance computing environments. Our research provides insights into\\nYOLOv11’s position within the broader landscape of object detection and its potential impact on\\nreal-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='real-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction\\nComputer vision, a rapidly advancing field, enables machines to interpret and understand visual data [ 1]. A crucial\\naspect of this domain is object detection[2], which involves the precise identification and localization of objects within\\nimages or video streams[3]. Recent years have witnessed remarkable progress in algorithmic approaches to address this\\nchallenge [4].\\nA pivotal breakthrough in object detection came with the introduction of the You Only Look Once (YOLO) algorithm\\nby Redmon et al. in 2015 [5]. This innovative approach, as its name suggests, processes the entire image in a single pass\\nto detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='to detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by\\nframing object detection as a regression problem [5]. It employs a single convolutional neural network to simultaneously\\npredict bounding boxes and class probabilities across the entire image [6], streamlining the detection pipeline compared\\nto more complex traditional methods.\\nYOLOv11 is the latest iteration in the YOLO series, building upon the foundation established by YOLOv1. Unveiled at\\nthe YOLO Vision 2024 (YV24) conference, YOLOv11 represents a significant leap forward in real-time object detection\\ntechnology. This new version introduces substantial enhancements in both architecture and training methodologies,\\npushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='pushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail\\ncapture while maintaining a lean parameter count. This results in improved accuracy across a diverse range of computer\\narXiv:2410.17725v1  [cs.CV]  23 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nvision (CV) tasks, from object detection to classification. Furthermore, YOLOv11 achieves remarkable gains in\\nprocessing speed, substantially enhancing real-time performance capabilities.\\nIn the following sections, this paper will provide a comprehensive analysis of YOLOv11’s architecture, exploring its\\nkey components and innovations. We will examine the evolution of YOLO models, leading up to the development\\nof YOLOv11. The study will delve into the model’s expanded capabilities across various CV tasks, including object\\ndetection, instance segmentation, pose estimation, and oriented object detection. We will also review YOLOv11’s\\nperformance improvements in terms of accuracy and computational efficiency compared to its predecessors, with a\\nparticular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='particular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11\\non real-time CV applications and its position within the broader landscape of object detection technologies.\\n2 Evolution of YOLO models\\nTable 1 illustrates the progression of YOLO models from their inception to the most recent versions. Each iteration has\\nbrought significant improvements in object detection capabilities, computational efficiency, and versatility in handling\\nvarious CV tasks.\\nTable 1: YOLO: Evolution of models\\nRelease Year Tasks Contributions Framework\\nYOLO [5] 2015 Object Detection, Basic Classifica-\\ntion\\nSingle-stage object detector Darknet\\nYOLOv2 [7] 2016 Object Detection, Improved Classi-\\nfication\\nMulti-scale training, dimension clus-\\ntering\\nDarknet\\nYOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='YOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone\\nDarknet\\nYOLOv5 [10] 2020 Object Detection, Basic Instance\\nSegmentation (via custom modifica-\\ntions)\\nAnchor-free detection, SWISH acti-\\nvation, PANet\\nPyTorch\\nYOLOv6 [11] 2022 Object Detection, Instance Segmen-\\ntation\\nSelf-attention, anchor-free OD PyTorch\\nYOLOv7 [12] 2022 Object Detection, Object Tracking,\\nInstance Segmentation\\nTransformers, E-ELAN reparame-\\nterisation\\nPyTorch\\nYOLOv8 [13] 2023 Object Detection, Instance Segmen-\\ntation, Panoptic Segmentation, Key-\\npoint Estimation\\nGANs, anchor-free detection PyTorch\\nYOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='YOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch\\nThis evolution showcases the rapid advancement in object detection technologies, with each version introducing novel\\nfeatures and expanding the range of supported tasks. From the original YOLO’s groundbreaking single-stage detection\\nto YOLOv10’s NMS-free training, the series has consistently pushed the boundaries of real-time object detection.\\nThe latest iteration, YOLO11, builds upon this legacy with further enhancements in feature extraction, efficiency,\\nand multi-task capabilities. Our subsequent analysis will delve into YOLO11’s architectural innovations, including\\nits improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='simultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='Building on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='image. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nspatial attention in the feature maps. This spatial attention mechanism allows the model to focus more effectively on\\nimportant regions within the image. By pooling features spatially, the C2PSA block enables YOLO11 to concentrate on\\nspecific areas of interest, potentially improving detection accuracy for objects of varying sizes and positions.\\n4.2 Neck\\nThe neck combines features at different scales and transmits them to the head for prediction. This process typically\\ninvolves upsampling and concatenation of feature maps from different levels, enabling the model to capture multi-scale\\ninformation effectively.\\n4.2.1 C3k2 Block\\nYOLO11 introduces a significant change by replacing the C2f block in the neck with the C3k2 block. The C3k2 block\\nis designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='is designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After\\nupsampling and concatenation, the neck in YOLO11 incorporates this improved block, resulting in enhanced speed and\\nperformance [18].\\n4.2.2 Attention Mechanism\\nA notable addition to YOLO11 is its increased focus on spatial attention through the C2PSA module. This attention\\nmechanism enables the model to concentrate on key regions within the image, potentially leading to more accurate\\ndetection, especially for smaller or partially occluded objects. The inclusion of C2PSA sets YOLO11 apart from its\\npredecessor, YOLOv8, which lacks this specific attention mechanism [18].\\n4.3 Head\\nThe head of YOLOv11 is responsible for generating the final predictions in terms of object detection and classification.\\nIt processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='It processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block\\nIn the head section, YOLOv11 utilizes multiple C3k2 blocks to efficiently process and refine the feature maps. The\\nC3k2 blocks are placed in several pathways within the head, functioning to process multi-scale features at different\\ndepths. The C3k2 block exhibits flexibility depending on the value of the c3k parameter:\\n• When c3k = False, the C3k2 module behaves similarly to the C2f block, utilizing a standard bottleneck\\nstructure.\\n• When c3k = True, the bottleneck structure is replaced by the C3 module, which allows for deeper and more\\ncomplex feature extraction.\\nKey characteristics of the C3k2 block:\\n• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.\\n• Parameter efficiency: C3k2 is a more compact version of the CSP bottleneck, making the architecture more\\nefficient in terms of the number of trainable parameters.\\nAnother notable addition is the C3k block, which offers enhanced flexibility by allowing customizable kernel sizes. The\\nadaptability of C3k is particularly useful for extracting more detailed features from images, contributing to improved\\ndetection accuracy.\\n4.3.2 CBS Blocks\\nThe head of YOLOv11 includes several CBS (Convolution-BatchNorm-Silu) [19] layers after the C3k2 blocks. These\\nlayers further refine the feature maps by:\\n• Extracting relevant features for accurate object detection.\\n• Stabilizing and normalizing the data flow through batch normalization.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n• Utilizing the Sigmoid Linear Unit (SiLU) activation function for non-linearity, which improves model perfor-\\nmance.\\nCBS blocks serve as foundational components in both feature extraction and the detection process, ensuring that the\\nrefined feature maps are passed to the subsequent layers for bounding box and classification predictions.\\n4.3.3 Final Convolutional Layers and Detect Layer\\nEach detection branch ends with a set of Conv2D layers, which reduce the features to the required number of outputs for\\nbounding box coordinates and class predictions. The final Detect layer consolidates these predictions, which include:\\n• Bounding box coordinates for localizing objects in the image.\\n• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11\\nYOLO11 supports a diverse range of CV tasks, showcasing its versatility and power in various applications. Here’s an\\noverview of the key tasks:\\n1. Object Detection: YOLO11 excels in identifying and localizing objects within images or video frames,\\nproviding bounding boxes for each detected item [ 20]. This capability finds applications in surveillance\\nsystems, autonomous vehicles, and retail analytics, where precise object identification is crucial [21].\\n2. Instance Segmentation: Going beyond simple detection, YOLO11 can identify and separate individual\\nobjects within an image down to the pixel level [20]. This fine-grained segmentation is particularly valuable in\\nmedical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='medical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].\\n3. Image Classification: YOLOv11 is capable of classifying entire images into predetermined categories,\\nmaking it ideal for applications like product categorization in e-commerce platforms or wildlife monitoring in\\necological studies [21].\\n4. Pose Estimation: The model can detect specific key points within images or video frames to track movements\\nor poses. This capability is beneficial for fitness tracking applications, sports performance analysis, and various\\nhealthcare applications requiring motion assessment [21].\\n5. Oriented Object Detection (OBB): YOLO11 introduces the ability to detect objects with an orientation angle,\\nallowing for more precise localization of rotated objects. This feature is especially valuable in aerial imagery\\nanalysis, robotics, and warehouse automation tasks where object orientation is crucial [21].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='analysis, robotics, and warehouse automation tasks where object orientation is crucial [21].\\n6. Object Tracking: It identifies and traces the path of objects in a sequence of images or video frames[ 21].\\nThis real-time tracking capability is essential for applications such as traffic monitoring, sports analysis, and\\nsecurity systems.\\nTable 2 outlines the YOLOv11 model variants and their corresponding tasks. Each variant is designed for specific\\nuse cases, from object detection to pose estimation. Moreover, all variants support core functionalities like inference,\\nvalidation, training, and export, making YOLOv11 a versatile tool for various CV applications.\\n6 Advancements and Key Features of YOLOv11\\nYOLOv11 represents a significant advancement in object detection technology, building upon the foundations laid by\\nits predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='its predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics\\nshowcases enhanced architectural designs, more sophisticated feature extraction techniques, and refined training\\nmethodologies. The synergy of YOLOv11’s rapid processing, high accuracy, and computational efficiency positions it\\nas one of the most formidable models in Ultralytics’ portfolio to date [22]. A key strength of YOLOv11 lies in its refined\\narchitecture, which facilitates the detection of subtle details even in challenging scenarios. The model’s improved\\nfeature extraction capabilities allow it to identify and process a broader range of patterns and intricate elements within\\nimages. Compared to earlier versions, YOLOv11 introduces several notable enhancements:\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nTable 2: YOLOv11 Model Variants and Tasks\\nModel Variants Task Inference Validation Training Export\\nYOLOv11 yolo11-nano yolo11-small\\nyolo11-medium yolo11-\\nlarge yolo11-xlarge\\nDetection ✓ ✓ ✓ ✓\\nYOLOv11-seg yolo11-nano-seg yolo11-\\nsmall-seg yolo11-medium-\\nseg yolo11-large-seg\\nyolo11-xlarge-seg\\nInstance Segmen-\\ntation\\n✓ ✓ ✓ ✓\\nYOLOv11-pose yolo11-nano-pose yolo11-\\nsmall-pose yolo11-medium-\\npose yolo11-large-pose\\nyolo11-xlarge-pose\\nPose/Keypoints ✓ ✓ ✓ ✓\\nYOLOv11-obb yolo11-nano-obb yolo11-\\nsmall-obb yolo1-medium-\\nobb yolo11-large-obb\\nyolo11-xlarge-obb\\nOriented Detec-\\ntion\\n✓ ✓ ✓ ✓\\nYOLOv11-cls yolo11-nano-cls yolo11-\\nsmall-cls yolo11-medium-\\ncls yolo11-large-cls yolo11-\\nxlarge-cls\\nClassification ✓ ✓ ✓ ✓\\n1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m\\ncounterpart, demonstrating improved computational efficiency without compromising accuracy [23].\\n2. Versatility in CV tasks: YOLOv11 exhibits proficiency across a diverse array of CV applications, including\\npose estimation, object recognition, image classification, instance segmentation, and oriented bounding box\\n(OBB) detection [23].\\n3. Optimized speed and performance: Through refined architectural designs and streamlined training pipelines,\\nYOLOv11 achieves faster processing speeds while maintaining a balance between accuracy and computational\\nefficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='efficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].\\n5. Advanced feature extraction: YOLOv11 incorporates improvements in both its backbone and neck architec-\\ntures, resulting in enhanced feature extraction capabilities and, consequently, more precise object detection\\n[23].\\n6. Contextual adaptability: YOLOv11 demonstrates versatility across various deployment scenarios, including\\ncloud platforms, edge devices, and systems optimized for NVIDIA GPUs [23].\\nYOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='such as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].\\nThe performance comparison graph depicted in Figure 2 overs several key insights. The YOLOv11 variants (11n, 11s,\\n11m, and 11x) form a distinct performance frontier, with each model achieving higher COCO mAP 50−95 scores at\\ntheir respective latency points. Notably, the YOLOv11x achieves approximately 54.5% mAP50−95 at 13ms latency,\\nsurpassing all previous YOLO iterations. The intermediate variants, particularly YOLOv11m, demonstrate exceptional\\nefficiency by achieving comparable accuracy to larger models from previous generations while requiring significantly\\nless processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='less processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s\\nmaintains high accuracy (approximately 47% mAP50−95) while operating at speeds previously associated with much\\nless accurate models. This represents a crucial advancement for real-time applications where both speed and accuracy\\nare critical. The improvement curve of YOLOv11 also shows better scaling characteristics across its model variants,\\nsuggesting more efficient utilization of additional computational resources compared to previous generations.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nFigure 2: Benchmarking YOLOv11 Against Previous Versions [23]\\n7 Discussion\\nYOLO11 marks a significant leap forward in object detection technology, building upon its predecessors while\\nintroducing innovative enhancements. This latest iteration demonstrates remarkable versatility and efficiency across\\nvarious CV tasks.\\n1. Efficiency and Scalability: YOLO11 introduces a range of model sizes, from nano to extra-large, catering\\nto diverse application needs. This scalability allows for deployment in scenarios ranging from resource-\\nconstrained edge devices to high-performance computing environments. The nano variant, in particular,\\nshowcases impressive speed and efficiency improvements over its predecessor, making it ideal for real-time\\napplications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='applications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature\\nextraction and processing capabilities. The incorporation of novel elements such as the C3k2 block, SPPF, and\\nC2PSA contributes to more effective feature extraction and processing. These enhancements allow the model\\nto better analyze and interpret complex visual information, potentially leading to improved detection accuracy\\nacross various scenarios.\\n3. Multi-Task Proficiency: YOLO11’s versatility extends beyond object detection, encompassing tasks such as\\ninstance segmentation, image classification, pose estimation, and oriented object detection. This multi-faceted\\napproach positions YOLO11 as a comprehensive solution for diverse CV challenges.\\n4. Enhanced Attention Mechanisms: A key advancement in YOLO11 is the integration of sophisticated spatial\\nattention mechanisms, particularly the C2PSA component. This feature enables the model to focus more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='attention mechanisms, particularly the C2PSA component. This feature enables the model to focus more\\neffectively on critical regions within an image, enhancing its ability to detect and analyze objects. The\\nimproved attention capability is especially beneficial for identifying complex or partially occluded objects,\\naddressing a common challenge in object detection tasks. This refinement in spatial awareness contributes to\\nYOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='speed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.\\n6. Implications for Real-World Applications: The advancements in YOLO11 have significant implications\\nfor various industries. Its improved efficiency and multi-task capabilities make it particularly suitable for\\napplications in autonomous vehicles, surveillance systems, and industrial automation. The model’s ability to\\nperform well across different scales also opens up new possibilities for deployment in resource-constrained\\nenvironments without compromising on performance.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n8 Conclusion\\nYOLOv11 represents a significant advancement in the field of CV , offering a compelling combination of enhanced\\nperformance and versatility. This latest iteration of the YOLO architecture demonstrates marked improvements in\\naccuracy and processing speed, while simultaneously reducing the number of parameters required. Such optimizations\\nmake YOLOv11 particularly well-suited for a wide range of applications, from edge computing to cloud-based analysis.\\nThe model’s adaptability across various tasks, including object detection, instance segmentation, and pose estimation,\\npositions it as a valuable tool for diverse industries such as emotion detection [26], healthcare [27] and various other\\nindustries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='industries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses\\nseeking to implement or upgrade their CV systems. In summary, YOLOv11’s blend of enhanced feature extraction,\\noptimized performance, and broad task support establishes it as a formidable solution for addressing complex visual\\nrecognition challenges in both research and practical applications.\\nReferences\\n[1] Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image processing, analysis and machine vision. Springer, 2013.\\n[2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\\nProceedings of the IEEE, 111(3):257–276, 2023.\\n[3] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review.\\nIEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='IEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-\\nvoltaic defect detection. In Solar, volume 4, pages 351–386. MDPI, 2024.\\n[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,\\n2016.\\n[6] Juan Du. Understanding of object detection based on cnn family and yolo. In Journal of Physics: Conference\\nSeries, volume 1004, page 012029. IOP Publishing, 2018.\\n[7] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='computer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\\n[10] Roboflow Blog Jacob Solawetz. What is yolov5? a guide for beginners., 2020. Accessed: 21 October 2024.\\n[11] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint\\narXiv:2209.02976, 2022.\\n[12] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new\\nstate-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.\\n[13] Francesco Jacob Solawetz. What is yolov8? the ultimate guide, 2023. Accessed: 21 October 2024.\\n[14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using\\nprogrammable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time\\nend-to-end object detection. arXiv preprint arXiv:2405.14458, 2024.\\n[16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024.\\n[17] Rahima Khanam, Muhammad Hussain, Richard Hill, and Paul Allen. A comprehensive review of convolutional\\nneural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='neural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.\\n[19] Jingwen Feng, Qiaofeng An, Jiahao Zhang, Shuxun Zhou, Guangwei Du, and Kai Yang. Application of yolov7-tiny\\nin the detection of steel surface defects. In 2024 5th International Seminar on Artificial Intelligence, Networking\\nand Information Technology (AINIT), pages 2241–2245. IEEE, 2024.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n[20] Ultralytics. Instance segmentation and tracking, 2024. Accessed: 2024-10-21.\\n[21] Ultralytics Abirami Vina. Ultralytics yolo11 has arrived: Redefine what’s possible in ai, 2024. Accessed:\\n2024-10-21.\\n[22] Viso.AI Gaudenz Boesch. Yolov11: A new iteration of “you only look once. https://viso.ai/\\ncomputer-vision/yolov11/, 2024. Accessed: 2024-10-21.\\n[23] Ultralytics. Ultralytics yolov11. https://docs.ultralytics.com/models/yolo11/s, 2024. Accessed:\\n21-Oct-2024.\\n[24] Rahima Khanam and Muhammad Hussain. What is yolov5: A deep look into the internal features of the popular\\nobject detector. arXiv preprint arXiv:2407.20892, 2024.\\n[25] DigitalOcean. What’s new in yolov11 transforming object detection once again part 1, 2024. Accessed: 2024-10-\\n21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2410.17725v1.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': '2410.17725v1.pdf', 'file_type': 'pdf'}, page_content='21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.\\nIn Kids Cybersecurity Using Computational Intelligence Techniques, pages 165–174. Springer, 2023.\\n[27] Burcu Ataer Aydin, Muhammad Hussain, Richard Hill, and Hussain Al-Aqrabi. Domain modelling for a\\nlightweight convolutional network focused on automated exudate detection in retinal fundus images. In 2023 9th\\nInternational Conference on Information Technology Trends (ITT), pages 145–150. IEEE, 2023.\\n9')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_split_documents(pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f8b0b",
   "metadata": {},
   "source": [
    "Embedding and VectorDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9cef8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from typing import List, Dict, Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06c81834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingVectorDB at 0x738d288f7c50>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingVectorDB:\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "      try:\n",
    "        print(f\"Loading embedding model: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(\"Model loaded successfully.\")\n",
    "      except Exception as e:\n",
    "        print(f\"Error loading model {self.model_name}: {e}\")\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts.\")\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True,show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager = EmbeddingVectorDB()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a0e83",
   "metadata": {},
   "source": [
    "Vector Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f84d4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 2058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x738d2a4a4ec0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_db\"):\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "       \n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            # shutil.rmtree(self.persist_directory,ignore_errors=True)  \n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb88d3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UniPixel: Unified Object Referring and\\nSegmentation for Pixel-Level Visual Reasoning\\nYe Liu1,2, Zongyang Ma2,3, Junfu Pu2, Zhongang Qi4, Yang Wu5,\\nYing Shan2, Chang Wen Chen1∗\\n1 The Hong Kong Polytechnic University 2 ARC Lab, Tencent PCG\\n3 Institute of Automation, Chinese Academy of Sciences 4 vivo Mobile Communication Co.\\n5 MindWingman Technology (Shenzhen) Co., Ltd.\\ncoco.ye.liu@connect.polyu.hk\\nhttps://polyu-chenlab.github.io/unipixel/\\nReasoning Segmentation (ReasonSeg)\\nReferring Expression Segmentation (RES)\\nInteractive Segmentation (IS)\\nReasoning Video Object Segmentation (ReVOS)\\nReferring Video Object Segmentation (RVOS)\\nMotion-Grounded Video Reasoning\\nReferred Video Description\\nReferred Video Question-Answering',\n",
       " 'Reasoning Video Object Segmentation (ReVOS)\\nReferring Video Object Segmentation (RVOS)\\nMotion-Grounded Video Reasoning\\nReferred Video Description\\nReferred Video Question-Answering\\nFind the empty chair that is to the left of the main sitting down.Segment the zebra standing in the middle of the frame.Locate the area that a cyclist uses to navigate in the city.Find the place where patients lie down to receive examination.Please segment and track the marked <region>.\\nPlease segment and track the hopping rabbit that leaped from the other one in the video.Where are the utensils used for drinking? Answer with masks.Q: Who shook off and scored?A: The man in red pants.',\n",
       " 'A: This police officer is a middle-aged man with a beard, wearing a blue uniform shirt, a black hat, and glasses. He stopped a blue vintage car and talked to the driver sitting inside.Q: Please describe the <region>. Q: If <region> continues his breakdance routine, what is a likely future event?A: He will perform more complex and varied breakdance moves.\\nPixel-Level Video Question Answering (PixelQA) — Joint Referring + Segmentation + QA in Videos\\nQ: How does the behavior of [1] differ from that of [2]? Why?\\n1\\n2\\n 3\\n4\\nA: [1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering food to [2] and [3]. This might because [1] doesn’t like the food from [4].\\n1\\n2\\nFigure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding\\ntasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and',\n",
       " '2\\nFigure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding\\ntasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and\\nreferred video description & question answering. It can also handle a novelPixelQAtask that jointly\\nrequires object-centric referring, segmentation, and question answering in videos.\\nAbstract\\nRecent advances in Large Multi-modal Models (LMMs) have demonstrated their\\nremarkable success as general-purpose multi-modal assistants, with particular\\nfocuses on holistic image- and video-language understanding. Conversely, less\\nattention has been given to scaling fine-grained pixel-level understanding capabili-\\nties, where the models are expected to realize pixel-level alignment between visual\\n∗Corresponding author.\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2509.18094v3  [cs.CV]  27 Oct 2025',\n",
       " 'signals and language semantics. Some previous studies have applied LMMs to\\nrelated tasks such as region-level captioning and referring expression segmentation.\\nHowever, these models are limited to performing either referring or segmentation\\ntasks independently and fail to integrate these fine-grained perception capabilities\\ninto visual reasoning. To bridge this gap, we proposeUniPixel, a large multi-modal\\nmodel capable of flexibly comprehending visual prompt inputs and generating\\nmask-grounded responses. Our model distinguishes itself by seamlessly integrating\\npixel-level perception with general visual understanding capabilities. Specifically,\\nUniPixel processes visual prompts and generates relevant masks on demand, and\\nperforms subsequent reasoning conditioning on these intermediate pointers during\\ninference, thereby enabling fine-grainedpixel-level reasoning. The effectiveness\\nof our approach has been verified on 10 benchmarks across a diverse set of tasks,',\n",
       " 'inference, thereby enabling fine-grainedpixel-level reasoning. The effectiveness\\nof our approach has been verified on 10 benchmarks across a diverse set of tasks,\\nincluding pixel-level referring/segmentation and object-centric understanding in\\nimages/videos. A novelPixelQAtask that jointly requires referring, segmentation,\\nand question answering is also designed to verify the flexibility of our method.\\n1 Introduction\\nLarge Multi-modal Models (LMMs) have been the de facto standard for developing general-purpose\\nassistants. By effectively aligning multi-modalities with language, their significance has been\\ndemonstrated across various applications, including multi-modal analysis [ 59, 19, 107, 108, 48],\\nautonomous driving (AD) [16, 79, 106, 11], and Embodied AI [111, 22, 30, 99].\\nIn the field of visual-language understanding, efforts have been dedicated to developingholistic\\nunderstanding models, where simple projection layers between visual encoders and LLMs are utilized',\n",
       " 'In the field of visual-language understanding, efforts have been dedicated to developingholistic\\nunderstanding models, where simple projection layers between visual encoders and LLMs are utilized\\nto bridge vision and language modalities. Supported by large-scale alignment pre-training and\\nvisual instruction tuning, such a straightforward paradigm achieves strong performance in holistic\\nunderstanding tasks such as captioning [ 40, 6, 114] and general question answering [ 36, 24, 54,\\n49]. However, these models exhibit two fundamental limitations in fine-grained scenarios.First,\\ntheir interactions with users are limited to text format, lacking support for more intuitive forms of\\ncommunication such as drawing points/boxes as references or grounding model responses with key\\nregions represented by masks.Second, the internal reasoning process of these models predominantly\\noperates at a coarse level, directly perceiving the entire content rather than reasoning over specific',\n",
       " 'operates at a coarse level, directly perceiving the entire content rather than reasoning over specific\\nobjects/regions, making them hard to understand fine-grained details. Some previous studies have\\nexplored the application of LMMs to related tasks such as region-level captioning [ 12, 102, 103],\\nreferring expression segmentation [ 29, 55, 41, 21, 71, 62], and reasoning segmentation [ 32, 27,\\n96, 4, 112]. Nevertheless, their models are limited to performing either referring or segmentation\\ntasks independently via rigidly defined input/output templates (e.g., “It’s <SEG>.” in LISA [ 32]),\\nlacking the flexibility to comprehend user-referred concepts and generate mask-grounded responses\\nsimultaneously. More importantly, these methods cannot integrate such fine-grained perception\\ncapabilities with their original human-like [ 90, 89, 88, 92, 91] multi-modal reasoning abilities,\\nresulting in degraded performance on general visual understanding benchmarks [100, 93, 28].',\n",
       " 'capabilities with their original human-like [ 90, 89, 88, 92, 91] multi-modal reasoning abilities,\\nresulting in degraded performance on general visual understanding benchmarks [100, 93, 28].\\nIn this work, we seek to bridge this gap by introducingUniPixel, a large multi-modal model that\\ncan flexibly comprehend visual prompt inputs (i.e., points, boxes, and masks) and generate mask-\\ngrounded responses. Our model significantly differentiates itself from existing ones by unifying\\nthe internal representations of referred and segmented objects via a novelobject memory bank,\\nwhich is a hashmap storing the spatial-temporal information of object-of-interests. During inference,\\nUniPixel initializes the object memory bank and updates it on demand by adding object-centric\\ninformation according to the context. The model responses are then generated conditioning on the\\nfine-grained object memory. Benefits from such unification, UniPixel is able to perform not only basic',\n",
       " 'information according to the context. The model responses are then generated conditioning on the\\nfine-grained object memory. Benefits from such unification, UniPixel is able to perform not only basic\\nreferring/segmentation tasks, but also flexiblepixel-level reasoningtasks that require simultaneous\\nvisual prompt comprehension and mask prediction. As illustrated in Fig. 1 (the last row), given a\\nvideo2, a question, and optionally a visual prompt (e.g., a point specified by a click on an object in\\nany frame), UniPixel can (1) infer the mask for the referred object in the corresponding frame, (2)\\npropagate it to all video frames containing the same instance, (3) extract the mask-grounded object\\nfeatures, and finally (4) answer the question conditioning on both the video-level and object-centric\\n2Images are treated as single-frame videos, thus we do not explicitly differentiate them in this work.\\n2',\n",
       " 'LLM\\nBox/Mask Encoder\\nPoint/Box/Mask Prompt\\nMask Decoder\\nIt shows a...\\nLLM\\nImage/Video\\nSure. It’s <SEG>.\\nPrompt Encoder\\n[3] is less interested in [2] compared with [1].\\nLLM\\nMasks\\nMask Decoder\\nHow does the behavior of [3] differ from [1]?\\n12\\n3\\nDescribe this region.\\nSegment the bunny.\\n(a) Referring-only Models(b) Segmentation-only Models(c) Unified Pixel-level Reasoning Model\\nUpdate\\n(e.g., Osprey, GPT4RoI, VideoRefer) (e.g., LISA, PixelLM, VISA)\\nObject Memory Bank\\nFigure 2:Schematic comparison between UniPixel and its counterparts.To the best of our knowl-\\nedge, UniPixel is the first unified method supporting simultaneous object referring and segmentation.\\ninformation. All these operations are seamlessly conductedwithin a single model, eliminating the\\nneed for external frame samplers [96], mask generators [102, 103], or object trackers [4].\\nWe evaluate the effectiveness of UniPixel from two aspects,i.e., basic referring/segmentation ca-',\n",
       " 'need for external frame samplers [96], mask generators [102, 103], or object trackers [4].\\nWe evaluate the effectiveness of UniPixel from two aspects,i.e., basic referring/segmentation ca-\\npabilities and flexible pixel-level reasoning capabilities. For the first aspect, we conduct extensive\\nexperiments on 10 public benchmarks across 9 image/video referring/segmentation tasks. Our method\\nachieves state-of-the-art performance in diverse scenarios. Notably, on the challenging video reason-\\ning segmentation and referred video QA tasks, our 3B model obtains62.1 J&F on ReVOS [96] and\\n72.8%Acc on VideoRefer-Bench Q [103], surpassing strong counterparts with 7B ∼13B parameters.\\nFurther ablation studies also demonstrate the mutual reinforcement effect of referring and segmenta-\\ntion. For the second aspect, we introduce a novelPixelQAtask that jointly requires object-centric\\nreferring, segmentation, and QA in videos, which cannot be handled by existing methods. UniPixel',\n",
       " 'tion. For the second aspect, we introduce a novelPixelQAtask that jointly requires object-centric\\nreferring, segmentation, and QA in videos, which cannot be handled by existing methods. UniPixel\\nestablishes a strong baseline for this novel setting. Our contributions are summarized below:\\n1. We proposeUniPixel, a unified large multi-modal model that supports flexible object referring\\nand segmentation in images and videos, via a novelobject memory bankdesign.\\n2. Our model achieves state-of-the-art performance on 10 public benchmarks across 9 refer-\\nring/segmentation tasks, verifying themutual reinforcement effectof such unification.\\n3. We also introduce a novelPixelQAtask that jointly requires object-centric referring, segmen-\\ntation, and QA in videos, where UniPixel establishes a strong baseline for this setting.\\n2 Related Work\\nLarge Multi-modal ModelsThe remarkable success of large multi-modal models (LMMs) has',\n",
       " 'tation, and QA in videos, where UniPixel establishes a strong baseline for this setting.\\n2 Related Work\\nLarge Multi-modal ModelsThe remarkable success of large multi-modal models (LMMs) has\\nshifted the paradigm of visual-language understanding from close-ended experts to open-ended task\\nsolvers. Early attempts [43, 42, 17, 115] involve an MLP projector or Q-Former [34] to align visual\\nencoders to LLMs, enabling open-ended tasks such as visual question answering. With advanced\\ndesigns such as dynamic resolution and data augmentation, open-source models,e.g., Qwen-VL\\n[2, 77, 3] and InternVL [14, 74, 13] series, have narrowed the gap with advanced proprietary models\\nlike the GPT [58, 59] and Gemini families [67, 18]. Recent studies [60, 25, 51, 37, 48] also explore\\ntest-time scaling on visual-language understanding. However, these methods are spatially coarse-\\ngrained. UniPixel can also be regarded as an object-centric test-time scaling approach, where key',\n",
       " 'test-time scaling on visual-language understanding. However, these methods are spatially coarse-\\ngrained. UniPixel can also be regarded as an object-centric test-time scaling approach, where key\\nobjects are first segmented then encoded to facilitate the subsequent reasoning process.\\nVisual Referring and SegmentationTo meet the growing demand for fine-grained visual under-\\nstanding [50, 46, 47, 45, 84], recent efforts have focused on enhancing LMMs with object referring\\nand segmentation capabilities, as compared in Fig. 2. LISA [32] is a representative model that enables\\nLMM-based segmentation by integrating SAM [ 31] as its decoder. They also introduced a novel\\nreasoning segmentation task, requiring models to perform segmentation based on implicit queries.\\nOther works in this direction [104, 69, 110, 65, 27] have explored advanced mask decoders, more\\nflexible tasks, and larger-scale datasets. Recent studies have also extended these capabilities to videos',\n",
       " 'flexible tasks, and larger-scale datasets. Recent studies have also extended these capabilities to videos\\n[4, 96, 101]. Additionally, some research has examined regional understanding through boxes [12]\\nand masks [102, 103]. While recent approaches attempt to unify these two capabilities, they either\\nsupport only images [65] or rely on sub-optimal, tool-based pipelines [23]. To the best of knowledge,\\nUniPixel is the first end-to-end method unifying object referring and mask prediction.\\n3',\n",
       " 'LLM\\nMask Decoder\\nVisual Encoder\\nPoint Encoder\\nBox Encoder\\nMask Encoder\\n Object Memory Bank\\nPrompt Encoder\\nMemory Injection\\nIn this video, how does the behavior of [1] differ from [2]and[3]? [1] appears disinterested and focuses on nibbling on the ground, while [2]is engaging with [4], who is offering some food to [2]and [3].\\nMasked Crop\\nVisual Tokens\\n Update Memory\\n<REF>Tokens\\n<SEG>Tokens\\n<MEM>Tokens\\nVisual TokenText Token\\nTokenizer\\nFigure 3:The architecture of UniPixel.Given a video, a question, and visual prompts, the model\\nencodes them into tokens via the visual encoder, prompt encoder, and tokenizer, respectively, then\\npredicts a spatial-temporal mask for each visual prompt via the mask decoder. The masks are updated\\ninto the object memory bank, and subsequently injected into the prompt for pixel-level reasoning.\\n3 Method\\nProblem FormulationWe provide a unified definition for pixel-level reasoning tasks. Formally,',\n",
       " '3 Method\\nProblem FormulationWe provide a unified definition for pixel-level reasoning tasks. Formally,\\nthe inputs are an image or a video X, a text prompt T , and N optional visual prompts {Pi}N\\ni=1 where\\neach Pi could be a point, box, or mask on a specific frame. The outputs are textual responses to\\nthe prompt with K grounded spatial-temporal masks {Mi}K\\ni=1. Here, both N and K could be zero\\n(degenerating to a normal visual understanding task) and K is not necessarily equal to N, as the\\nmodel may segment extra objects/regions that are not specified by the visual prompts.\\nOverviewFig. 3 presents an overview of UniPixel. It is built upon the Qwen2.5-VL [ 3] framework,\\nconsisting of an LLM backbone and a ViT-based visual encoder that supports dynamic resolution\\ninputs. Given a video and a text prompt, the model first tokenizes them via the visual encoder and\\ntext tokenizer, then sends them into the LLM for response generation. To boost this framework from',\n",
       " 'inputs. Given a video and a text prompt, the model first tokenizes them via the visual encoder and\\ntext tokenizer, then sends them into the LLM for response generation. To boost this framework from\\nholistic-level to pixel-level, we introduce (1) aprompt encoder(Sec.3.1) supporting three types of\\nvisual prompts, (2) anobject memory bank(Sec.3.2) for storing object information and injecting it\\ninto the response generation process, and (3) amask decoder(Sec.3.3) for generating spatial-temporal\\nmasks. We also extend the LLM’s vocabulary by adding <REF>, <MEM>, and <SEG> tokens. The\\nformer two serve as placeholders in the input prompt that would be replaced by visual prompt and\\nmemory tokens, respectively, while the<SEG> token is utilized to trigger and guide the mask decoding\\nprocess. Detailed designs and interactions among these components are illustrated as follows.\\n3.1 Prompt Encoder\\nT (0.3)\\nX1Y1(0.3, 0.2)\\nLinear\\nLinear\\nGELU + Linear\\nX2Y2(0.8, 0.5)\\n Linear\\n1D\\n2D\\n2D',\n",
       " 'process. Detailed designs and interactions among these components are illustrated as follows.\\n3.1 Prompt Encoder\\nT (0.3)\\nX1Y1(0.3, 0.2)\\nLinear\\nLinear\\nGELU + Linear\\nX2Y2(0.8, 0.5)\\n Linear\\n1D\\n2D\\n2D\\nTemporal EmbPositional EmbType EmbFourier EmbElement-wise AddChannel-wise Cat\\nFigure 4:Joint positional & temporal encodingfor\\npoint (X1Y1T) and box (X1Y1X2Y2T) prompts.\\nThis module aims to effectively encode each\\nvisual prompt into a single token that can be\\nprocessed by the LLM. We denote a point\\nprompt as a tuple (x, y, t)containing its spa-\\ntial coordinates (x, y)and the corresponding\\nframe indext. For box prompts, it is extended\\nto (x1, y1, x2, y2, t)containing the positions\\nof top-left and bottom-right corners. A mask\\nprompt is densely represented by a 2D binary\\nmask mij ∈ {0,1}with the same shape as\\nthe encoded target frame.\\nFor sparse prompts (points and boxes), as shown in Fig. 4, we encode each position(xi, yi) as the sum',\n",
       " 'mask mij ∈ {0,1}with the same shape as\\nthe encoded target frame.\\nFor sparse prompts (points and boxes), as shown in Fig. 4, we encode each position(xi, yi) as the sum\\nof a 2D Fourier embedding [73] and a learnable type embedding (indicating whether it is a single point,\\ntop-left corner, or bottom-right corner). For box prompts, we merge the two positional embeddings\\n4',\n",
       " 'by concatenating them along the channel dimension and linearly projecting them back to the original\\nsize. Frame indices are also encoded similarly with 1D Fourier embeddings. The resulting positional\\nand temporal embeddings are concatenated again, and then projected to the LLM’s embedding space\\nvia a GELU→Linear block, such that the sparse coordinates in a point/box are encoded into a\\ncompact high-dimensional token. This design is inspired by [31, 66] with two key differences: (1) the\\nspatial-only embeddings are extended to include temporal information, and (2) the negative points\\nare discarded. For dense prompts (masks), we directly resize the binary masks and apply masked\\npooling on the outputs of the visual encoder. An M →L projector (Linear→GELU→Linear ) is\\nleveraged to project the pooled visual features to the LLM’s embedding space.\\n3.2 Object Memory Bank\\nAlthough sparse prompts contain rich positional and temporal information indicating the objects that',\n",
       " '3.2 Object Memory Bank\\nAlthough sparse prompts contain rich positional and temporal information indicating the objects that\\nusers are referring to, it is still hard for the model to focus on these important regions. Previous studies\\n[12, 102, 103] also confirm that direct region cropping can generally provide better object awareness\\ncompared to positional pointers. To seamlessly integrate such a mechanism while preserving the\\nflexibility of visual prompts (e.g., allow pointing on a single frame instead of drawing complete\\nmasks on all frames), we propose an object memory bank to bridge sparse visual prompts and dense\\nobject masks. This is a hashmap where the keys are object IDs and the values are the corresponding\\nspatial-temporal masks. It is initialized as an empty storage for every chat session, and is dynamically\\nupdated on demand. We define two operations for the object memory bank, namelymemory pre-filling',\n",
       " 'updated on demand. We define two operations for the object memory bank, namelymemory pre-filling\\nandmemory injection. Below is an example of memory-enhanced multi-round conversation.\\nPrompt 1:How does the behavior of [1]<REF>differ from [2]<REF>and [3]<REF>?\\n<REF>detected, enhancing the prompt with object memory.\\nMemory Pre-filling Response:\\nThe relevant regions for this question are [1] <SEG> [2] <SEG> [3] <SEG> [4] <SEG>. ← 4 objects saved into the memory\\nMemory Injected Prompt:\\nHere is a video with 4 frames denoted as <1> to <4>. The highlighted regions are as follows:\\n[1]: <1><MEM><2><MEM><3><MEM>←This object cannot be seen in the last frame\\n[2]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[3]: <1><MEM><2><MEM><3><MEM><4><MEM>\\n[4]: <1><MEM><2><MEM><3><MEM><4><MEM>\\nHow does the behavior of [1] differ from [2] and [3]?\\nResponse 1:[1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering\\nsome food to [2] and [3].',\n",
       " 'Response 1:[1] appears disinterested and focuses on nibbling on the ground, while [2] is engaging with [4], who is offering\\nsome food to [2] and [3].\\nPrompt 2:What food is [4] offering?←Users can directly refer to objects in the memory\\nResponse 2:[4] is offering carrots.\\nMemory Pre-fillingThis operation is triggered upon the detection of <REF> tokens in the input\\nprompt, aiming to thoroughly analyze the referred objects and predict their corresponding masks. In\\nthis stage, the model responds with object IDs and <SEG> tokens for the relevant objects according to\\nthe context, and predicts their spatial-temporal masks accordingly. These object-mask pairs are then\\nsaved into the object memory bank.\\nMemory InjectionWe inject the features of the saved objects into the prompt to enhance object-\\nawareness. Similar to the mask prompt encoder described in Sec. 3.1, each frame-level object mask\\nis downsampled to match the resolution of visual tokens. We then apply masked pooling to aggregate',\n",
       " 'awareness. Similar to the mask prompt encoder described in Sec. 3.1, each frame-level object mask\\nis downsampled to match the resolution of visual tokens. We then apply masked pooling to aggregate\\nobject-centric features. Each frame-level mask is condensed into a single feature token, projected\\nthrough the mask projector, and subsequently utilized to replace the corresponding <MEM> token\\nin the memory-injected prompt. Through thispre-filling and injectionmechanism, object-centric\\ninformation is effectively integrated into the model inference process.\\nWhy using object memory bank?An alternative is directly appending a <SEG> token to each\\n<REF> token, followed by masked pooled features obtained during inference. However, we do not\\nadopt this approach for two reasons: (1) During mask prediction, the <SEG> tokens, due to the\\nunidirectional nature of causal self-attention, are unable to aggregate the full context of the prompt,',\n",
       " 'adopt this approach for two reasons: (1) During mask prediction, the <SEG> tokens, due to the\\nunidirectional nature of causal self-attention, are unable to aggregate the full context of the prompt,\\nthereby compromising the quality of predicted masks. (2) By utilizing the object memory bank, we\\ncan effectively decouple regional understanding and mask prediction, allowing each to benefit from\\nreferring and segmentation data during training, thus enhancing both capabilities.\\n5',\n",
       " 'Table 1: Comparison with state-of-the-art methods on ReVOS [96] val split. The best and second-\\nbest results are markedboldand underlined , respectively.\\nMethod Size\\nReferring Reasoning Overall\\nR\\nJ F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nMTTR [5] – 29.8 30.2 30.0 20.4 21.5 21.0 25.1 25.9 25.5 5.6\\nLMPM [21] – 29.0 39.1 34.1 13.3 24.3 18.8 21.2 31.7 26.4 3.2\\nReferFormer [85] – 31.2 34.3 32.7 21.3 25.6 23.4 26.2 29.9 28.1 8.8\\nLLM-based Generalists\\nLISA [32] 13B 45.2 47.9 46.6 34.3 39.1 36.7 39.8 43.5 41.6 8.6\\nTrackGPT [72] 13B 48.3 50.6 49.5 38.1 42.9 40.5 43.2 46.8 45.0 12.8\\nVISA [96] 13B 55.6 59.1 57.4 42.0 46.7 44.3 48.8 52.9 50.9 14.5\\nHyperSeg [81] 3B 56.0 60.9 58.5 50.2 55.8 53.0 53.1 58.4 55.7 –\\nInstructSeg [82] 3B 54.8 59.2 57.0 49.2 54.7 51.9 52.0 56.9 54.5 –\\nGLUS [39] 7B 56.0 60.7 58.3 48.8 53.9 51.4 52.4 57.3 54.9 17.9\\nViLLa [112] 6B – – – – – – 54.9 59.1 57.0 –\\nSa2V A [101] 4B – – – – – – – – 53.2 –\\nUniPixel(Ours) 3B 62.3 66.7 64.5 57.1 62.1 59.6 59.7 64.4 62.1 19.0',\n",
       " 'ViLLa [112] 6B – – – – – – 54.9 59.1 57.0 –\\nSa2V A [101] 4B – – – – – – – – 53.2 –\\nUniPixel(Ours) 3B 62.3 66.7 64.5 57.1 62.1 59.6 59.7 64.4 62.1 19.0\\nUniPixel(Ours) 7B 63.9 67.8 65.8 59.4 63.7 61.5 61.7 65.7 63.7 19.4\\n3.3 Mask Decoder\\nWe adopt SAM 2.1 [ 66] as the mask decoder to disentangle the discrete language modeling and\\ncontinuous mask prediction capabilities. For each <SEG> token, we extract its last-layer hidden\\nstates, downsample them via an L→M projector (architecturally identical to the M→L projector),\\nand reshape them into two tokens. Using two tokens ensures better preservation of object information\\nwhen downsampling from high- to low-dimensional channel space. These tokens prompt the mask\\ndecoder to predict the mask on the first frame, which is then propagated to the other frames.\\n3.4 Model Training\\nThe training loss for UniPixel is a linear combination of language modeling loss and mask decoding',\n",
       " '3.4 Model Training\\nThe training loss for UniPixel is a linear combination of language modeling loss and mask decoding\\nlosses [66], including a focal loss and dice loss for mask prediction, a mean-absolute-error (MAE)\\nloss for IoU prediction, and a cross-entropy loss for objectness prediction. The loss weights are set\\nto 1, 100, 5, 5, and 5, respectively. We train the model through a three-stage progressive alignment\\nrecipe. The datasets are listed in Tab. 12. In the first stage, we pre-train the sparse prompt encoder\\nusing 851K regional captioning data. Then, we align the LLM and mask decoder by training the\\nL→M projector on 87K referring segmentation data. In the last stage, we further unfreeze the M→L\\nprojector and mask decoder, and apply LoRA [26] on the visual encoder and LLM. The model is\\njointly trained on a large-scale corpus with around 1M samples for diverse tasks.\\n4 Experiments',\n",
       " 'projector and mask decoder, and apply LoRA [26] on the visual encoder and LLM. The model is\\njointly trained on a large-scale corpus with around 1M samples for diverse tasks.\\n4 Experiments\\nWe evaluate the effectiveness of UniPixel by conducting extensive experiments across a diverse set of\\nbenchmarks. Specifically, we study the following research questions.\\nQ1. Whether UniPixel is flexible and effective on basic image/video referring and segmentation\\ntasks compared to the corresponding representative methods?\\nQ2. How does it perform on the more challenging PixelQA task, which requires joint referring,\\nsegmentation, and question answering in videos?\\nQ3. What effects does each architectural design contribute? More importantly, does the unified\\nmodeling of referring and segmentation lead to a mutual reinforcement effect?\\nDetailed information about the benchmarks, evaluation metrics, implementation details, and more\\nexperimental results can be found in the appendix.',\n",
       " 'Detailed information about the benchmarks, evaluation metrics, implementation details, and more\\nexperimental results can be found in the appendix.\\n4.1 Q1: Comparison with State-of-the-Arts on Referring and Segmentation Tasks\\nReasoning Video Object SegmentationWe begin with the most challenging ReVOS [ 96] dataset,\\nwhich requires models to predict masks based on implicit text queries demanding complex reasoning\\nabilities based on world knowledge. The results are shown in Tab. 1. Our 3B variant outperforms all\\n6',\n",
       " 'Table 2: Comparison with state-of-the-art methods on referring video object segmentation (RVOS)\\nand motion-grounded video reasoning datasets, including MeViS [21] (val), Ref-YouTube-VOS [71]\\n(val), Ref-DA VIS17 [62] (val), and GroundMoRe [20] (test). The best and second-best results are\\nmarkedboldand underlined , respectively.\\nMethod Size\\nMeViS Ref-YouTube-VOS Ref-DA VIS17 GroundMoRe\\nJ F J&F J F J&F J F J&F J F J&F\\nNon-LLM-based Specialists\\nReferFormer [85] – 29.8 32.2 31.0 61.3 64.6 62.9 58.1 64.1 61.1 11.2 14.3 12.7\\nLMPM [21] – 34.2 40.2 37.2 – – – – – – 12.7 14.0 13.3\\nOnlineRefer [83] – – – – 61.6 65.5 63.5 61.6 67.7 64.8 – – –\\nLLM-based Generalists\\nPixelLM [69] 7B 36.3 41.1 38.7 54.3 55.7 55.0 63.4 70.0 66.7 9.9 10.0 10.0\\nLISA [32] 13B 35.8 40.0 37.9 54.0 54.8 54.4 63.2 68.8 66.0 6.3 6.7 6.5\\nVISA [96] 13B 41.8 47.1 44.5 61.4 64.7 63.0 67.0 73.8 70.4 5.3 4.7 5.9\\nVideoLISA [4] 3.8B 41.3 47.6 44.4 61.7 65.7 63.7 64.9 72.7 68.8 – – –',\n",
       " 'VISA [96] 13B 41.8 47.1 44.5 61.4 64.7 63.0 67.0 73.8 70.4 5.3 4.7 5.9\\nVideoLISA [4] 3.8B 41.3 47.6 44.4 61.7 65.7 63.7 64.9 72.7 68.8 – – –\\nVideoGLaMM [56] 3.8B 42.1 48.2 45.2 65.4 68.2 66.8 73.3 65.6 69.5 – – –\\nViLLa [112] 6B 46.5 52.3 49.4 64.6 70.4 67.5 70.6 78.0 74.3 – – –\\nGLUS [39] 7B 48.5 54.2 51.3 65.5 69.0 67.3 – – – – – –\\nSa2V A [101] 4B – – 46.2 – – 70.0 – – 73.8 – – –\\nMoRA [20] 7B – – – – – – – – – 27.4 26.9 27.2\\nUniPixel(Ours) 3B 50.4 55.7 53.1 68.6 72.3 70.5 70.7 77.8 74.2 36.0 38.7 37.4\\nUniPixel(Ours) 7B 53.2 58.3 55.8 69.5 72.4 71.0 72.7 80.1 76.4 36.5 39.1 37.8\\nTable 3: Comparison with state-of-the-art methods on image referring expression segmentation (RES)\\nand reasoning segmentation datasets, including RefCOCO/+/g [29, 55] and ReasonSeg [32] (val).\\nThe best and second-best results are markedboldand underlined , respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg ReasonSeg\\nval testA testB val testA testB val(U) test(U) gIoU cIoU\\nNon-LLM-based Specialists',\n",
       " 'Method Size\\nRefCOCO RefCOCO+ RefCOCOg ReasonSeg\\nval testA testB val testA testB val(U) test(U) gIoU cIoU\\nNon-LLM-based Specialists\\nReLA [41] – 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0 – –\\nX-Decoder [116] – – – – – – – 64.6 – 22.6 17.9\\nSEEM [117] – – – – – – – 65.7 – 25.5 21.2\\nLLM-based Image Generalists\\nNExT-Chat [104] 7B 74.7 78.9 69.5 65.1 71.9 56.7 67.0 67.0 – –\\nPixelLM [69] 7B 73.0 76.5 68.2 66.3 71.7 58.3 69.3 70.5 – –\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6 61.3 62.9\\nGroundhog [110] 7B 78.5 79.9 75.7 70.5 75.0 64.9 74.1 74.6 56.2 –\\nLaSagnA [80] 7B 76.8 78.7 73.8 66.4 70.6 60.1 70.6 71.9 48.8 47.2\\nM2SA [27] 13B 74.6 77.6 71.0 64.0 68.1 57.6 69.0 69.3 – –\\nLLM-based Video Generalists\\nVideoLISA [4] 3.8B 73.8 76.6 68.8 63.4 68.8 56.2 68.3 68.8 61.4 67.1\\nVISA [96] 7B 72.4 75.5 68.1 59.8 64.8 53.1 65.5 66.4 52.7 57.8\\nVitron [23] 7B 75.5 79.5 72.2 66.7 72.5 58.0 67.9 68.9 – –\\nSa2V A [101] 4B 78.9 – – 71.7 – – 74.1 – – –',\n",
       " 'VISA [96] 7B 72.4 75.5 68.1 59.8 64.8 53.1 65.5 66.4 52.7 57.8\\nVitron [23] 7B 75.5 79.5 72.2 66.7 72.5 58.0 67.9 68.9 – –\\nSa2V A [101] 4B 78.9 – – 71.7 – – 74.1 – – –\\nUniPixel(Ours) 3B 80.5 82.6 76.9 74.3 78.9 68.4 76.3 77.0 64.0 56.2\\nUniPixel(Ours) 7B 80.8 83.0 77.4 75.3 80.1 70.0 76.4 77.1 60.5 58.7\\nexisting methods with larger LLMs (including Sa2V A-4B [101] also with SAM 2 decoder), achieving\\n62.1 overall J&F . The 7B model further boosts the performance to 64.0 J&F – an improvement\\nof 12% over the previous state-of-the-art – demonstrating that UniPixel can effectively understand\\nimplicit queries based on its world knowledge, and accurately generate masks as responses.\\nReferring Video Object SegmentationThe performance comparisons on MeViS [ 21], Ref-\\nYouTube-VOS [71], and Ref-DA VIS17 [62] datasets are presented in Tab. 2. UniPixel consistently\\nachieves the best performance among its counterparts. Its advantage is particularly evident on the',\n",
       " 'YouTube-VOS [71], and Ref-DA VIS17 [62] datasets are presented in Tab. 2. UniPixel consistently\\nachieves the best performance among its counterparts. Its advantage is particularly evident on the\\nmore challenging MeViS dataset, where our 3B model outperforms GLUS-7B [39] by around 3.5%,\\nas well as the similarly sized VideoGLaMM-3.8B [56] by 17%. More experimental results on MeViS\\n[21] valu set and Ref-SA V [101] val set are provided in Tab. 4 and Tab. 5, respectively. Ref-SA V\\nfeatures long referring descriptions, large object motion, large camera motion, and heavy occlusion\\ncompared with existing datasets. Given these complex descriptions and video content, our method\\nconsistently performs better than counterparts, including those fine-tuned on the target dataset.\\nMotion-Grounded Video ReasoningWe also evaluate our method on GroundMoRe [20] dataset\\n(results shown in Tab. 2), which highlights visual answer generation that requires joint spatial and\\n7',\n",
       " 'Table 4: Experimental results on MeViS [21] valu\\nset. Post means applying post optimization.\\nMethod Size FT J F J&F\\nLMPM [21] –✗ 36.5 43.9 40.2\\nLISA [32] 7B✗ 39.9 46.5 43.2\\nLISA [32] + XMem [15] 7B✗ 41.9 49.3 45.6\\nVideoLISA [4] 7B✗ 48.4 54.9 51.7\\nVideoLISA [4] + Post 7B✗ 50.9 58.1 54.5\\nSa2V A [101] 4B✗ – – 52.1\\nSa2V A [101] 8B✗ – – 57.0\\nUniPixel(Ours) 3B ✗ 56.1 63.2 59.7\\nUniPixel(Ours) 7B ✗ 58.4 65.0 61.7\\nTable 5: Comparison on Ref-SA V [101] val set.\\nFT means fine-tuning after pre-/co-training.\\nMethod Size FT J F J&F\\nUniRef++ [86] –✗ 11.6 9.5 10.5\\nUNINEXT [95] –✗ 8.8 6.4 7.6\\nLMPM [21] –✗ 12.2 9.8 10.3\\nVISA [96] 7B✗ 13.2 11.3 11.8\\nSa2V A [101] 8B✗ 39.6 43.0 41.3\\nUniRef++ [86] –✓ 15.8 13.4 14.6\\nSa2V A [101] 8B✓ 48.3 51.7 50.0\\nUniPixel(Ours) 3B ✗ 66.9 67.6 67.2\\nUniPixel(Ours) 7B ✗ 68.5 69.6 69.0\\nTable 6: Fine-tuned performance on referring expression segmentation (RES) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.',\n",
       " 'Table 6: Fine-tuned performance on referring expression segmentation (RES) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nLISA [32] 7B 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6\\nGSV A [87] 7B 77.2 78.9 73.5 65.9 69.6 59.8 72.7 73.3\\nOMG-LLaV A [109] 7B 78.0 80.3 74.1 69.1 73.1 63.0 72.9 72.9\\nGLaMM [65] 7B 79.5 83.2 76.9 72.6 78.7 64.6 74.2 74.9\\nSa2V A [101] 4B 80.4 – – 74.3 – – 75.7 –\\nUniPixel(Ours) 3B 81.9 83.5 78.6 75.3 80.3 70.6 77.2 78.5\\nUniPixel(Ours) 7B 83.0 84.9 80.4 77.8 82.3 72.7 78.7 79.7\\nTable 7: Experimental results on referring expression comprehension (REC) datasets, including Ref-\\nCOCO/+/g [29, 55]. The best and second-best results are markedboldand underlined, respectively.\\nMethod Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nOFA [78] – 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6',\n",
       " 'Method Size\\nRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val(U) test(U)\\nOFA [78] – 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6\\nShikra [9] 7B 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2\\nMiniGPT-v2 [8] 7B 88.7 91.6 85.3 79.9 85.1 74.4 84.4 84.6\\nVitron [23] 7B 90.9 93.289.3 83.7 89.1 76.9 86.4 87.0\\nUniPixel(Ours) 3B 91.8 93.8 87.5 86.3 90.8 80.3 88.0 88.2\\nUniPixel(Ours) 7B 92.0 94.4 88.1 87.2 91.9 82.1 88.6 88.7\\ntemporal grounding. Note that we mainly compare the results with MoRA [20], which is fine-tuned\\non GroundMoRe while other methods are evaluated under the zero-shot setting. Benefit from the\\nstrong pixel-level reasoning capability, UniPixel significantly performs better than the baseline.\\nReferring Expression Segmentation and Reasoning SegmentationTab. 3 compares the image\\nsegmentation capabilities using explicit and implicit queries. We evaluate our co-trained model on\\nRefCOCO/+/g [29, 55] and ReasonSeg [32]. While state-of-the-art performance has been achieved',\n",
       " 'segmentation capabilities using explicit and implicit queries. We evaluate our co-trained model on\\nRefCOCO/+/g [29, 55] and ReasonSeg [32]. While state-of-the-art performance has been achieved\\non RES datasets, we observe that the reasoning segmentation data (239 samples) can be easily\\noverwhelmed by the other samples during training due to its limited size. Tab. 6 presents the RES\\nperformance after fine-tuning. We follow the common practice that jointly fine-tunes the model on\\nRefCOCO/+/g datasets [29, 55], and then evaluate on them separately. These results demonstrate the\\ngeneralizability of UniPixel when facing both explicit and implicit queries.\\nReferring Expression ComprehensionOur method also supports referring expression compre-\\nhension by inferring the bounding boxes from predicted masks. Its performance (accuracy with IoU\\n⩾ 0.5) is compared with representative methods in Tab. 7. Benefiting from the high-quality mask',\n",
       " 'hension by inferring the bounding boxes from predicted masks. Its performance (accuracy with IoU\\n⩾ 0.5) is compared with representative methods in Tab. 7. Benefiting from the high-quality mask\\nprediction, UniPixel can also achieve very competitive performance on this simpler task.\\nReferred Video Description and Question AnsweringWe study UniPixel’s regional understand-\\ning capabilities on VideoRefer-Bench [103], which contains two subsets for description and question\\nanswering tasks. The comparisons are in Tab. 8 and Tab. 9. BQ, SQ, RQ, CQ, and FP denote basic\\nquestions, sequential questions, relational questions, reasoning questions, and future predictions,\\nrespectively. Both tasks leverage mask prompts as inputs, where single-frame and multi-frame modes\\ndenote applying the masks only on a specific frame and on all frames, respectively. UniPixel can\\n8',\n",
       " 'Table 8: Comparison with state-of-the-art methods on VideoRefer-Bench D [103]. The best and\\nsecond-best results are markedboldand underlined , respectively.\\nMethod Size\\nSingle-Frame Multi-Frame\\nSC AD TD HD Avg. SC AD TD HD Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B 2.62 1.58 2.19 2.07 2.12 3.09 1.94 2.50 2.41 2.48\\nQwen2-VL [77] 7B 2.97 2.24 2.03 2.31 2.39 3.30 2.54 2.22 2.12 2.55\\nInternVL2 [74] 26B 3.55 2.99 2.57 2.25 2.84 4.08 3.353.08 2.28 3.20\\nGPT-4o-mini [59] – 3.56 2.85 2.87 2.38 2.92 3.89 3.18 2.62 2.50 3.05\\nGPT-4o [59] – 3.34 2.96 3.01 2.50 2.95 4.15 3.31 3.11 2.43 3.25\\nImage Referring LMMs\\nFerret [98] 7B 3.08 2.01 1.54 2.14 2.19 3.20 2.38 1.97 1.38 2.23\\nOsprey [102] 7B 3.19 2.16 1.54 2.45 2.34 3.30 2.66 2.10 1.58 2.41\\nVideo Referring LMMs\\nElysium [75] 7B 2.35 0.30 0.02 3.59 1.57 – – – – –\\nArtemis [63] 7B – – – – – 3.42 1.34 1.39 2.90 2.26\\nVideoRefer [103] 7B 4.41 3.273.03 2.973.42 4.443.27 3.10 3.043.46\\nUniPixel(Ours) 3B 4.04 3.15 3.10 3.37 3.42 4.08 3.13 3.13 3.42 3.44',\n",
       " 'Artemis [63] 7B – – – – – 3.42 1.34 1.39 2.90 2.26\\nVideoRefer [103] 7B 4.41 3.273.03 2.973.42 4.443.27 3.10 3.043.46\\nUniPixel(Ours) 3B 4.04 3.15 3.10 3.37 3.42 4.08 3.13 3.13 3.42 3.44\\nUniPixel(Ours) 7B 3.83 3.07 2.96 3.62 3.37 3.82 3.05 3.01 3.57 3.36\\nTable 9: Comparison with state-of-the-art methods on\\nVideoRefer-BenchQ [103] (mask prompts). MF denotes\\nmulti-frame mode. Full question types are in Sec. 4.1.\\nMethod Size MF BQ SQ RQ CQ FP Avg.\\nGeneral LMMs\\nLLaV A-OV [33] 7B✗ 58.7 62.9 64.7 87.4 76.3 67.4\\nQwen2-VL [77] 7B✗ 62.0 69.6 54.9 87.3 74.6 66.0\\nInternVL2 [74] 26B✗ 58.5 63.5 53.4 88.0 78.9 65.0\\nGPT-4o-mini [59] –✗ 57.6 67.1 56.5 85.9 75.4 65.8\\nGPT-4o [59] –✗ 62.374.5 66.088.0 73.7 71.3\\nImage Referring LMMs\\nFerret [98] 7B✗ 35.2 44.7 41.9 70.4 74.6 48.8\\nOsprey [102] 7B✗ 45.9 47.1 30.0 48.6 23.7 39.9\\nVideo Referring LMMs\\nVideoRefer [103] 7B✗ 75.468.6 59.389.478.1 71.9\\nUniPixel(Ours) 3B ✗ 73.6 70.3 60.7 88.8 78.0 72.2\\nUniPixel(Ours) 7B ✗ 68.9 73.1 64.7 88.8 83.3 73.4',\n",
       " 'Video Referring LMMs\\nVideoRefer [103] 7B✗ 75.468.6 59.389.478.1 71.9\\nUniPixel(Ours) 3B ✗ 73.6 70.3 60.7 88.8 78.0 72.2\\nUniPixel(Ours) 7B ✗ 68.9 73.1 64.7 88.8 83.3 73.4\\nVideoRefer [103] 7B✓ – 70.6 60.5 – – 72.1\\nUniPixel(Ours) 3B ✓ 75.3 70.7 62.3 87.4 77.2 72.8\\nUniPixel(Ours) 7B ✓ 70.6 74.6 64.7 88.8 82.5 74.1\\nTable 10: Evaluation results on our newly\\nintroduced PixelQA task. All the visual\\nprompts are applied in a single frame. See\\nSec. 4.2 for detailed settings.\\nMethod Size J F J&F Acc\\nPoint Prompts\\nInternVL2 [74] 26B – – – 60.8\\nQwen2-VL [77] 72B – – – 69.3\\nUniPixel(Ours) 3B 57.3 64.4 60.9 71.1\\nUniPixel(Ours) 7B 42.1 47.1 44.6 71.4\\nBox Prompts\\nInternVL2 [74] 26B – – – 61.3\\nQwen2-VL [77] 72B – – – 69.0\\nUniPixel(Ours) 3B 57.8 64.7 61.3 70.3\\nUniPixel(Ours) 7B 41.1 46.4 43.8 71.4\\nMixed (50% Points + 50% Boxes)\\nInternVL2 [74] 26B – – – 60.9\\nQwen2-VL [77] 72B – – – 69.1\\nUniPixel(Ours) 3B 57.2 64.1 60.6 70.8\\nUniPixel(Ours) 7B 42.3 47.5 44.9 71.4',\n",
       " 'Mixed (50% Points + 50% Boxes)\\nInternVL2 [74] 26B – – – 60.9\\nQwen2-VL [77] 72B – – – 69.1\\nUniPixel(Ours) 3B 57.2 64.1 60.6 70.8\\nUniPixel(Ours) 7B 42.3 47.5 44.9 71.4\\neffectively comprehend both types of prompts, and accurately respond with object-centric descriptions\\nor answers, surpassing strong models including GPT-4o [59] and VideoRefer [103].\\n4.2 Q2: Pixel-Level Video Question Answering (PixelQA)\\nWe design the new PixelQA task based on VideoRefer-Bench Q [103], where the original mask\\nprompts are replaced with more challenging point or box prompts. Given these ambiguous visual\\ncues, models are expected to correctly identify the target object according to the question and the\\nvisual prompt, then respond withboth the textual answer and the corresponding object masks.\\nWe report the mask prediction J&F and MCQ accuracy in Tab. 10. Note that none of the existing\\nmethods supports this scenario. Thus, we apply set-of-mark prompts [97] directly on video frames,',\n",
       " 'We report the mask prediction J&F and MCQ accuracy in Tab. 10. Note that none of the existing\\nmethods supports this scenario. Thus, we apply set-of-mark prompts [97] directly on video frames,\\nand evaluate the QA accuracies of two strong LMMs [77, 74] as our baselines. Aside from point- or\\nbox-only prompts, we also explore a more flexible setting that randomly chooses different prompts\\nfor different objects. The results verify that ourmemory pre-filling & injectionparadigm effectively\\nenhances the model’s reasoning capabilities. Visualizations of this task are shown in Fig. 5.\\n4.3 Q3: Key Ablation Studies\\nEffect of Task UnificationWe study the effect of task unification in Tab. 11 (a). Unifying referring\\nand segmentation capabilities into a single model and training them jointly leads to better results\\n9',\n",
       " 'If [1] continues moving forward, what is a likely future event?(A) The bear will encounter other animals (B) The bear will find a place to rest (C) The bear will start running (D) The bear will climb the stone wall\\nWhat is a likely future event with [1]?(A) He will stop and rest (B) He will start walking slowly (C) He will continue to navigate through more obstacles (D) He will sit down and take a break\\nHow is [4] related to [2]?(A) [4] is holding [2] (B) [4] is controlling [2] with a leash (C) [4] is walking away from [2] (D) [4] is ignoring [2]\\nFigure 5:Visualization of the outputs from UniPixel on PixelQA task. Star marks and boxes\\nrefer to point and box prompts, respectively. The boxed frames denote where the visual prompts are\\napplied. Given different types of visual prompts on a single frame, our method can flexibly infer the\\nrelevant object, track it across the entire video, and involve its features in reasoning.',\n",
       " 'applied. Given different types of visual prompts on a single frame, our method can flexibly infer the\\nrelevant object, track it across the entire video, and involve its features in reasoning.\\nTable 11: Key ablation studies with UniPixel-3B on PixelQA (mixed). See Sec. 4.3 for explanations.\\n(a) Task Unification\\nRefer Segment Memory J&F Acc\\n✓ – 64.6\\n✓ 47.5 –\\n✓ ✓ 48.2 67.4\\n✓ ✓ ✓ 49.0 68.5\\n(b) Object Memory Bank\\nReferring Method J&F Acc\\n①<REF> 46.8 64.5\\n②<REF><SEG> 47.8 64.9\\n③<REF><SEG>+ Pooling 47.5 66.3\\n④Object Memory Bank 49.0 68.5\\n(c) Prompt Encoder & Mask Decoder\\nEncoder Decoder J&F Acc\\nw/o Time – 44.3 63.7\\nw/ Time – 49.0 68.5\\n– Independent 46.1 66.2\\n– Propagation 49.0 68.5\\non both tasks (first three rows), demonstrating themutual reinforcement effectof such unification.\\nIncorporating memory pre-filling as an auxiliary task (last row) brings extra improvements.\\nEffect of Object Memory BankTab. 11 (b) verifies the effectiveness of object memory bank. ①',\n",
       " 'Incorporating memory pre-filling as an auxiliary task (last row) brings extra improvements.\\nEffect of Object Memory BankTab. 11 (b) verifies the effectiveness of object memory bank. ①\\nmeans using a single token for each referred object. ② means adding an extra segmentation token to\\nsegment it as an auxiliary task. ③ further appends masked-pooled visual tokens after it. The results\\nshow that (1) both adding auxiliary segmentation task and masked-pooled features help regional\\nunderstanding, and (2) decoupling them via object memory bank can further boost the performance.\\nDesign Space of Prompt Encoder & Mask DecoderWe compare different prompt encoder\\nand mask decoder designs in Tab. 11 (c). The performance significantly drops when the temporal\\nencoding in the prompt encoder is removed (first two rows). For the mask decoder (last two rows),\\nwe explore an alternative strategy that treats video frames independently (as batched images), which',\n",
       " 'encoding in the prompt encoder is removed (first two rows). For the mask decoder (last two rows),\\nwe explore an alternative strategy that treats video frames independently (as batched images), which\\ncould largely accelerate inference but lead to sub-optimal accuracies. We hypothesize that this is\\nbecause the LLM-generated <SEG> token cannot well-capture the object information in all frames,\\nthus disentangling the segmentation and tracking capabilities to an external module is reasonable.\\n5 Conclusion\\nIn this work, we proposedUniPixel, a large multi-modal model that supports flexible pixel-level\\nvisual reasoning. It unifies the internal representations of referred and segmented objects through\\na novelobject memory bank. We observe that by such unification, the performance of object\\nreferring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed',\n",
       " 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       " 'Appendix\\nIn this appendix, we provide more details about the training data, model implementation, and experi-\\nmental settings to complement the main paper. Additional analysis, ablation studies, visualizations,\\nand discussions are also incorporated. Below is the table of contents.\\nA.Model\\n1.Implementation Details\\n2.Training Recipe\\nB.Experiments\\n1.Tasks and Benchmarks\\n2.Evaluation Metrics\\n3.More Experimental Results\\n4.Ablation Studies\\n5.Qualitative Results\\nC.Discussions\\n1.Limitations & Future Work\\n2.Potential Societal Impacts\\nD.Licenses\\nA Model\\nA.1 Implementation Details\\nWe instantiate our base models with 3B and 7B versions of Qwen2.5-VL [3]. Both variants employ\\npre-trained SAM 2.1 [66] with Hiera Base+ [70] backbone as the mask decoder. The M→L projector\\nis initialized with the weights from the V→L projector of Qwen2.5-VL. The hidden size inside the\\nprompt encoder is 256. To reduce GPU memory and accelerate training, we randomly sample 8',\n",
       " 'is initialized with the weights from the V→L projector of Qwen2.5-VL. The hidden size inside the\\nprompt encoder is 256. To reduce GPU memory and accelerate training, we randomly sample 8\\nframes per video, with each frame resized to 3162 ∼448 2 pixels (128∼256 tokens per frame). The\\nframe sampling strategies follow the specifications of each benchmark during inference. The mask\\ndecoder has a fixed resolution of 768 ×768. For each segmentation sample, up to 5 objects are\\nrandomly selected to compute the mask prediction losses. During training, LoRA adapters [26] with\\nrank=128 and alpha=256 are applied to all QKVO layers in the visual encoder and LLM. The input\\nsequences are restricted to 4K tokens. We train the model with 8 RTX A6000 Ada (48G) GPUs, with\\na global batch size of 256 for stages 1 and 2, and 32 for stage 3. In the first two stages, the learning\\nrates are set to 1e-3. In the last stage, it is set to 5e-6 for the mask decoder and 2e-5 for all the other',\n",
       " 'rates are set to 1e-3. In the last stage, it is set to 5e-6 for the mask decoder and 2e-5 for all the other\\nparameters, respectively. A linear warmup in the first 3% steps followed by cosine decay is adopted\\nin all stages. The configurations of datasets are introduced in the following section.\\nA.2 Training Recipe\\nThe detailed distribution of training datasets for UniPixel is shown in Tab. 12. Within the three-stage\\ntraining recipe, we first pre-train the sparse prompt encoder using short caption samples from Inst-IT\\n[61] and VideoRefer [103]. For each sample, we randomly select a point inside the ground truth\\nmask (50%) or generate an augmented box from it (50%). This stage aims to enable the model with\\nsimple visual prompt comprehension and regional captioning capabilities on images and videos. In\\nthe second stage, we align the LLM and mask decoder using referring object segmentation datasets',\n",
       " 'simple visual prompt comprehension and regional captioning capabilities on images and videos. In\\nthe second stage, we align the LLM and mask decoder using referring object segmentation datasets\\n[29, 55, 71]. We use short caption/query samples for the first two stages to focus on alignment rather\\nthan knowledge learning. For the last stage, we collect a large-scale, high-quality corpus called\\nUniPixel-SFT-1M3 to jointly train the model on diverse pixel-level tasks. The original annotations\\nhave been rewritten using task-specific templates to incorporate instructions. All the repurposed\\ndatasets and pre-processing pipelines will be publicly available to facilitate future research.\\n3 https://huggingface.co/datasets/PolyU-ChenLab/UniPixel-SFT-1M\\n11',\n",
       " 'Table 12: The distribution of training datasets for UniPixel. We use different background colors to\\ndenote object referring , object segmentation , regional understanding , memory pre-filling , and\\ngeneral video understanding data, respectively.\\nStage Dataset\\nInputs Outputs\\n#Samples #Repeat Ratio\\nText Image Video Point Box Mask Text Mask\\n1 Inst-IT-Image-Short-Caption [61] ✓ ✓ ✓ ✓ ✓ 351K 1 41.2%\\nVideoRefer-Short-Caption [103] ✓ ✓ ✓ ✓ ✓ 500K 1 58.8%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 5 20.8%\\n2 RefCOCOg [55] ✓ ✓ ✓ ✓ 22K 5 26.8%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 5 22.0%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 3 9.5%\\n3\\nOsprey-Conversation [102] ✓ ✓ ✓ ✓ 1.4K 5 0.1%\\nOsprey-Detail-Description [102] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nOsprey-Pos-Neg [102] ✓ ✓ ✓ ✓ 20K 5 1.7%\\nVideoRefer-Detailed-Caption [103] ✓ ✓ ✓ ✓ 120K 5 10.1%\\nVideoRefer-QA [103] ✓ ✓ ✓ ✓ 69K 5 5.8%\\nInst-IT-Video-QA [61] ✓ ✓ ✓ ✓ 159K 5 13.4%\\nVideoRefer-QA-Memory [103] ✓ ✓ ✓ ✓ ✓ ✓ 69K 3 3.5%\\nInst-IT-QA-Memory [61] ✓ ✓ ✓ ✓ ✓ ✓ 158K 3 8.0%',\n",
       " 'VideoRefer-QA [103] ✓ ✓ ✓ ✓ 69K 5 5.8%\\nInst-IT-Video-QA [61] ✓ ✓ ✓ ✓ 159K 5 13.4%\\nVideoRefer-QA-Memory [103] ✓ ✓ ✓ ✓ ✓ ✓ 69K 3 3.5%\\nInst-IT-QA-Memory [61] ✓ ✓ ✓ ✓ ✓ ✓ 158K 3 8.0%\\nRefCOCO [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCO+ [29] ✓ ✓ ✓ ✓ 17K 10 2.9%\\nRefCOCOg [55] ✓ ✓ ✓ ✓ 22K 10 3.7%\\nRefClef [29] ✓ ✓ ✓ ✓ 18K 10 3.0%\\nReasonSeg [32] ✓ ✓ ✓ ✓ 1.6K 10 0.3%\\nADE20K [113] ✓ ✓ ✓ ✓ 20K 3 1.0%\\nCOCOStuff [7] ✓ ✓ ✓ ✓ 118K 3 6.0%\\nMapillary Vistas [57] ✓ ✓ ✓ ✓ 18K 3 0.9%\\nPACO-LVIS [64] ✓ ✓ ✓ ✓ 46K 3 2.3%\\nPASCAL-Part [10] ✓ ✓ ✓ ✓ 4.4K 3 0.2%\\nRef-YouTube-VOS [71] ✓ ✓ ✓ ✓ 13K 5 1.1%\\nRef-DA VIS17 [62] ✓ ✓ ✓ ✓ 0.6K 10 0.1%\\nRef-SA V [101] ✓ ✓ ✓ ✓ 56K 3 2.8%\\nMeViS [21] ✓ ✓ ✓ ✓ 23K 5 1.9%\\nLV-VIS [76] ✓ ✓ ✓ ✓ 11K 3 0.6%\\nViCaS [1] ✓ ✓ ✓ ✓ 41K 3 2.1%\\nReVOS [96] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nGroundMoRe [20] ✓ ✓ ✓ ✓ 5.6K 3 0.3%\\nLLaV A-1.5-Mix-665K [42] ✓ ✓ ✓ 647K 1 10.9%\\nVideoGPT+ Instruct [52] ✓ ✓ ✓ 573K 1 9.7%\\nB Experiments\\nB.1 Tasks and Benchmarks',\n",
       " 'ReVOS [96] ✓ ✓ ✓ ✓ 29K 5 2.5%\\nGroundMoRe [20] ✓ ✓ ✓ ✓ 5.6K 3 0.3%\\nLLaV A-1.5-Mix-665K [42] ✓ ✓ ✓ 647K 1 10.9%\\nVideoGPT+ Instruct [52] ✓ ✓ ✓ 573K 1 9.7%\\nB Experiments\\nB.1 Tasks and Benchmarks\\nOur method is extensively evaluated across 9 fine-grained image/video understanding tasks. The\\nbenchmark(s) used for each task are listed as follows:\\n1.Reasoning Video Object Segmentation:ReVOS [96]\\n2. Referring Video Object Segmentation:MeViS [ 21], Ref-YouTube-VOS [71], Ref-DA VIS17 [62], Ref-SA V [101]\\n3.Motion-Grounded Video Reasoning:GroundMoRe [20]\\n4.Referring Expression Segmentation:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n5.Reasoning Segmentation:ReasonSeg [32]\\n6.Referring Expression Comprehension:RefCOCO [29], RefCOCO+ [29], RefCOCOg [55]\\n7.Referred Video Description:VideoRefer-Bench D [103]\\n8.Referred Video Question Answering:VideoRefer-Bench Q [103]\\n9.Flexible Pixel-Level Understanding:PixelQA (Ours)\\nB.2 Evaluation Metrics',\n",
       " '7.Referred Video Description:VideoRefer-Bench D [103]\\n8.Referred Video Question Answering:VideoRefer-Bench Q [103]\\n9.Flexible Pixel-Level Understanding:PixelQA (Ours)\\nB.2 Evaluation Metrics\\nFor video segmentation tasks, we adopt J&F as the main metric to jointly consider region similarity\\nJ and contour accuracy F. Image segmentation is evaluated using cIoU (the cumulative intersection\\nover the cumulative union) and gIoU (the average of all per-image IoUs) following existing work. For\\nreferred video description and question answering tasks, we follow the official evaluation protocols to\\nreport GPT-4o [59] scores and MCQ accuracy, respectively. For referring expression comprehension,\\n12',\n",
       " 'Table 13: Performance comparison on general video question answering (VideoQA) on MVBench\\n[36]. Note that UniPixel is the only model supporting pixel-level referring & segmentation.\\nModel Size AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg.\\nGPT-4V [58] – 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.059.011.0 43.5\\nVideo-ChatGPT [53] 7B 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 39.5 48.5 29.0 33.0 29.5 26.0 35.5 32.7\\nVideo-LLaMA [105] 7B 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1\\nVideoChat [35] 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5\\nVideo-LLaV A [38] 7B 46.0 42.5 56.5 39.0 53.5 53.0 48.0 41.0 29.0 31.5 82.5 45.0 26.0 53.0 41.5 33.5 41.5 27.5 38.5 31.5 43.0\\nTimeChat [68] 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5',\n",
       " 'TimeChat [68] 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5\\nPLLaV A [94] 7B 58.0 49.0 55.5 41.0 61.0 56.0 61.0 36.0 23.5 26.0 82.0 39.5 42.0 52.0 45.0 42.0 53.5 30.5 48.0 31.0 46.6\\nST-LLM [44] 7B 66.0 53.584.044.0 58.5 80.5 73.5 38.5 42.5 31.0 86.5 36.5 56.5 78.5 43.0 44.5 46.5 34.5 41.5 58.5 54.9\\nVideoGPT+ [52] 4B 69.0 60.0 83.0 48.5 66.5 85.575.536.0 44.0 34.0 89.5 39.5 71.0 90.5 45.0 53.0 50.0 29.5 44.0 60.0 58.7\\nVideoChat2 [36] 7B 75.558.0 83.5 50.560.5 87.5 74.5 45.047.544.082.5 37.0 64.5 87.5 51.0 66.547.035.037.0 72.5 60.4\\nUniPixel(Ours) 3B 69.5 62.5 83.0 48.5 76.5 86.5 66.5 38.0 49.0 40.5 87.0 49.0 74.0 95.0 49.0 45.0 63.5 34.5 58.0 73.5 62.5\\nUniPixel(Ours) 7B 71.0 68.0 84.0 45.0 78.0 91.5 66.5 35.5 57.5 43.0 91.5 47.0 73.5 92.5 58.0 53.0 74.0 37.5 49.0 69.0 64.3\\nTable 14: Effectiveness justification of multi-stage training. The best and second-best results are',\n",
       " 'Table 14: Effectiveness justification of multi-stage training. The best and second-best results are\\nmarkedboldand underlined , respectively. The three-stage recipe leads to optimal performance.\\nStage 1 Stage 2 Stage 3\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ 58.3 63.6 61.0 54.8 61.9 58.4 71.1 71.5\\n✓ ✓ 59.0 63.4 61.2 55.2 62.1 58.7 71.8 72.3\\n✓ ✓ 59.6 63.5 61.6 55.7 62.5 59.1 71.2 71.6\\n✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nwe leverage mean accuracies, where a predicted bounding box is considered correct when it has the\\nintersection over union (IoU) with the ground truth no less than 0.5.\\nB.3 More Experimental Results\\nGeneral Video Question AnsweringWe also evaluate UniPixel on MVBench [ 36] to compare its\\ngeneral video understanding capabilities with existing methods. The results are illustrated in Tab. 13.\\nNote that our method is the only one in the table that supports referring and segmentation. By jointly',\n",
       " 'Note that our method is the only one in the table that supports referring and segmentation. By jointly\\ntraining on holistic-level and pixel-level data, UniPixel can effectively balance the capabilities under\\nboth scenarios, demonstrated by the strong performance compared with holistic-level models.\\nB.4 Ablation Studies\\nEffect of Multi-stage TrainingWe investigate the effectiveness of multi-stage training in Tab. 14.\\nAs shown in the first line, directly training the model using large-scale data only leads to sub-optimal\\nperformance, due to the unaligned representations among prompt encoder, LLM, and mask decoder.\\nWe observe that pre-training either the sparse prompt encoder or the L→M projector (the second and\\nthird lines) brings performance gains on both tasks (referring and segmentation). We hypothesize that\\nthis is because pre-aligning either of them can alleviate the burden of joint-task learning in stage 3.',\n",
       " 'this is because pre-aligning either of them can alleviate the burden of joint-task learning in stage 3.\\nThe last row verifies that the performance can be further boosted by pre-aligning both of them.\\nNumber of Hidden Tokens for Mask DecoderAs mentioned in the main paper, there is a huge\\ngap between the feature dimensions of the LLM and the mask decoder, thus splitting the<SEG> token\\ninto more hidden tokens can better preserve the object information from the LLM. We ablate this\\nmechanism in Tab. 15. According to the results, using only 1 hidden token cannot fully preserve the\\nobject information, as the mask prediction performance is sub-optimal. However, we also observe\\nthat using more than 2 hidden tokens (e.g., 4 or 8) only brings negligible performance gain. Therefore,\\nwe choose 2 hidden tokens per object in our final model.\\nTraining Strategy for the M→L projectorThe M →L projector aims to project the masked-pooled',\n",
       " 'we choose 2 hidden tokens per object in our final model.\\nTraining Strategy for the M→L projectorThe M →L projector aims to project the masked-pooled\\nobject-centric features to the LLM’s embedding space. Since the object features originate from the\\nvisual encoder, it is possible to re-use the pre-trained weights of the original V →L projector in\\nQwen2.5-VL. Its effects are studied in Tab. 16. We investigated two strategies: 1) re-using the\\nweights and 2) adding an extra pre-training stage for better alignment. The comparison shows that\\ndirectly re-using weights without extra pre-training can achieve the best results.\\n13',\n",
       " 'Table 15: Ablation study on the number of hid-\\nden tokens for each <SEG>. Performance gains\\nare negligible with more than 2 tokens/object.\\n#Tokens\\nReVOS MeViS(val u)\\nJ F J&F J F J&F\\n1 59.6 63.5 61.6 55.8 62.5 59.2\\n2 59.7 64.4 62.1 56.1 63.2 59.7\\n4 59.8 63.9 61.9 56.863.1 59.9\\n8 59.5 64.0 61.8 56.4 62.8 59.6\\nTable 16: Ablation study on M→L projector. Init\\nand PT denote weight initialization from V→L\\nprojector and extra pre-training, respectively.\\nInit PT\\nVideoRefer-BenchQ PixelQA\\nSingle-Frame Multi-Frame Mixed Acc\\n71.4 71.9 67.7\\n✓ 71.5 71.7 67.4\\n✓ 72.4 72.6 68.2\\n✓ ✓ 72.2 72.8 68.5\\nTable 17: Ablation study on training data used in stage 3. The best and second-best results are marked\\nboldand underlined, respectively. Gradually adding more pixel-level data brings performance gains.\\nRegional Segmentation Memory General\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ – – – – – – 72.1 72.0\\n✓ 58.9 63.8 61.4 56.0 63.2 59.6 – –',\n",
       " 'Regional Segmentation Memory General\\nReVOS MeViS(val u) VideoRefer-BenchQ\\nJ F J&F J F J&F Single-Frame Multi-Frame\\n✓ – – – – – – 72.1 72.0\\n✓ 58.9 63.8 61.4 56.0 63.2 59.6 – –\\n✓ ✓ 59.2 63.7 61.5 55.8 63.1 59.5 72.3 72.6\\n✓ ✓ ✓ 59.6 64.5 62.1 56.3 63.5 59.9 72.472.5\\n✓ ✓ ✓ ✓ 59.7 64.4 62.1 56.1 63.2 59.7 72.2 72.8\\nCombination of Training DataTab. 17 studies the effect of the combination of multi-task co-\\ntraining data in stage 3. Compared with training only on the regional or segmentation data, leveraging\\nboth of them leads to considerable performance on both tasks. Incorporating memory pre-filling data\\n(requiring both referring and segmentation) can further boost the performance. We also mix some\\ngeneral holistic-level video understanding data to preserve the original capabilities of the pre-trained\\nmodel, while it slightly affects the performance on pixel-level tasks.\\nB.5 Qualitative Results',\n",
       " 'general holistic-level video understanding data to preserve the original capabilities of the pre-trained\\nmodel, while it slightly affects the performance on pixel-level tasks.\\nB.5 Qualitative Results\\nFig. 6∼11 present more visualizations of outputs from UniPixel on different pixel-level understanding\\ntasks. Our method can effectively handle flexible visual prompts [103], implicit queries [32, 96], long\\nqueries [101], and motion-grounded questions [20].\\nC Discussion\\nC.1 Limitations & Future Work\\nDue to the limited computing resources, we did not further scale up the training data to incorporate\\nmore pixel-level tasks such as grounded caption generation (GCG) on images [65] or videos [56],\\nwhich are interesting scenarios and their data may bring more performance gains. Besides, the mask\\ndecoder currently predicts the first mask on the first frame and propagates it to the following frames,',\n",
       " 'decoder currently predicts the first mask on the first frame and propagates it to the following frames,\\nwhile it potentially supports predicting on the best frame (defined as the frame with the best view of\\nthe target) and propagates it to both sides of the video. We will focus in our future work to explore\\nmore pixel-level understanding tasks and more flexible mechanisms for the mask decoder.\\nC.2 Potential Societal Impacts\\nThis work introduces a new framework for pixel-level visual-language understanding, which could\\npotentially be used in education, surveillance, and healthcare industries, where flexible interactions\\nwith the users and fine-grained understanding of images & videos are required. In other scenarios\\nrequiring multi-modal assistants, our method can also serve as a more advanced alternative. To the\\nbest of our knowledge, there are no potential negative societal impacts to declare.\\nD Licenses',\n",
       " 'requiring multi-modal assistants, our method can also serve as a more advanced alternative. To the\\nbest of our knowledge, there are no potential negative societal impacts to declare.\\nD Licenses\\nOur model is built based on the pre-trained Qwen2.5-VL [3] and SAM 2.1 [66] models. They are both\\nlicensed under the Apache License 2.0 (https://www.apache.org/licenses/LICENSE-2.0).\\n14',\n",
       " \"If [1] continues to move forward, what is a likely future event involving [2]?(A) [2] will run away (B) [2] will sit down and stop moving (C) [2] will start barking (D) [2] will continue walking by the wheelchair\\nWhat action does [1] perform that involves [3]?(A) [1] extends an arm across [3]'s chest (B) [1] hands something to [3] (C) [1] talks to [3] (D) [1] ignores [3]\\nWhat is [1] wearing?(A) Blue sweatshirt and black jeans (B) Red sweatshirt and light blue jeans (C) Green t-shirt and white pants (D) Yellow hoodie and dark blue jeans\\nIf <object1><region> continues riding the bike, what is a likely future event?(A) [1] will stop (B) [1] will start running (C) [1] will change a different outfit (D) [1] will continue to challenge different high difficulty movements\\nFigure 6: Visualization of the predictions from UniPixel on PixelQA.\\nPlease segment the zebra which is younger in this video.\\nPlease segment the cow that is the furthest from the camera in this video.\",\n",
       " 'Please segment the zebra which is younger in this video.\\nPlease segment the cow that is the furthest from the camera in this video.\\nWhich goldfish is on the left side of the screen at the beginning of the video? Please provide the segmentation mask.\\nCan you find the skunk that has black fur all over its body and a tuft of white fur on its head and the tip of its tail?\\nWhere is the instrument that serves to shield from the sun or protect from rain and snow?\\nWhich ferret(s) is/are being licked by a cat consistently? Please provide the segmentation mask.\\nCan you segment the insect(s) belonging to the superfamily Papilionoidea of the Lepidoptera order in this video?\\nFigure 7: Visualization of the predictions from UniPixel on ReVOS [96].\\n15',\n",
       " 'Where is the man wearing a cap and shorts in this video? Respond with the segmentation mask.\\nCan you find the blue wooden car in the frames?\\nSegment and track the green motorbike in this video.\\nPlease segment the black swan in this video.\\nWhere is the rope? Give me the segmentation results directly.\\nFigure 8: Visualization of the predictions from UniPixel on Ref-DA VIS17 [62].\\nQ: Who might not open the cooler if not for feeding the walrus a fish?                                                       A: The woman.\\nQ: Who opens the ziploc bag to transfer the crushed Oreo cookies into the bowl?                                                  A: The girl.\\nQ: Who dribbles the ball before he shoots it?                                                                      A: The man in the black shorts.\\nQ: Who asked if the little girl could carry the box before she picked it up?                                                 A: The man.',\n",
       " 'Q: Who asked if the little girl could carry the box before she picked it up?                                                 A: The man.\\nQ: What might not be given to the woman by the man if he did not eat by himself? A: The bag.\\nQ: Who kicks the ball into the goal? A: The boy.\\nFigure 9: Visualization of the predictions from UniPixel on GroundMoRe [20].\\n16',\n",
       " \"Findtheobjectaccordingtothedescription:Theobjectisadark-coloredbackpackwithlight-coloredaccents,featuringmultiplecompartmentsandpockets,securelyfastenedtoanindividual'sback.Thepersonisdressedindarkclothingandascendinganescalatorinapublicsetting,likelyamallortransportationhub.Thebackpackhasadjustablestrapsandatophandle,appearingfunctionalforcarryingvariousitems.Theindividualmovessteadilyuptheescalator,indicatingapurposefuljourney.\\nAnalyzethefollowingsentencesandprovidethecorrespondingsegmentationmask:Theobjectisadark-coloredsedan,likelyblueorblack,parkedonanunpavedsurface,possiblyadirtroadoranareawithloosesoil.Ithasfourdoors,avisiblerearspoileronthetrunk,silverwheels,andtintedwindows.Thecarisslightlytilted,suggestingitmightbeparkedonunevengroundorexperiencingsomeformofimbalance.Throughoutthevideo,thesedanremainsstationary,withnoindicationofmovementoractionsbeingperformedbythevehicle.\",\n",
       " 'Pleasesegmenttheobjectaccordingtothedescription:Theobjectisapersonwithlongdarkhair,wearingadarktopandapatternedskirtwithgeometricdesigns.Thisindividualisstationaryormovingveryslowlyinthebackgroundofaretailstore,possiblyafurnitureorhomegoodsstore.Thepersonremainsincloseproximitytoanothershopperpushingashoppingcart,suggestingtheymightbetogetherorinteracting.Thescenecapturesatypicalshoppingexperience.\\nFigure 10: Visualization of the predictions from UniPixel on Ref-SA V [101].\\nFind the lens that is more suitable for photographing nearby objects.\\n Where is the goat nearest to the bottom stone? Give me the segmentation mask.\\nIn some rural areas, horse-drawn carts are still used for transportation and carrying goods. What is the main source of power that drives the cart in the picture?\\nWhat item in the picture can provide information to help guide travelers through this rugged terrain that can be challenging to navigate?',\n",
       " 'What item in the picture can provide information to help guide travelers through this rugged terrain that can be challenging to navigate?\\nPlease localize the place where piano players should sit in this image.\\nWhere is the place where the garbage should be put? Please respond with the segmentation mask.\\nWhich part of the vehicle must be used to display identifying information as required by law? Segment the target directly.\\nSegment the place where the patient lies down to receive examination in this image.\\nFigure 11: Visualization of the predictions from UniPixel on ReasonSeg [32].\\n17',\n",
       " 'References\\n[1] Ali Athar, Xueqing Deng, and Liang-Chieh Chen. Vicas: A dataset for combining holistic and pixel-level\\nvideo understanding using captions with grounded segmentation.arXiv:2412.09754, 2024.\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\\nand Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text\\nreading, and beyond. 2023.\\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\\nWang, Jun Tang, et al. Qwen2.5-vl technical report.arXiv:2502.13923, 2025.\\n[4] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng\\nShou. One token to seg them all: Language instructed reasoning segmentation in videos. InNeurIPS,\\npages 6833–6859, 2024.\\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation\\nwith multimodal transformers. InCVPR, pages 4985–4995, 2022.',\n",
       " 'pages 6833–6859, 2024.\\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation\\nwith multimodal transformers. InCVPR, pages 4985–4995, 2022.\\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\\nlarge-scale video benchmark for human activity understanding. InCVPR, pages 961–970, 2015.\\n[7] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\\nCVPR, pages 1209–1218, 2018.\\n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krish-\\nnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model\\nas a unified interface for vision-language multi-task learning.arXiv:2310.09478, 2023.\\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic.arXiv:2306.15195, 2023.',\n",
       " '[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic.arXiv:2306.15195, 2023.\\n[10] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect\\nwhat you can: Detecting and representing objects using holistic models and body parts. InCVPR, pages\\n1971–1978, 2014.\\n[11] Yuan Chen, Zi-han Ding, Ziqin Wang, Yan Wang, Lijun Zhang, and Si Liu. Asynchronous large language\\nmodel enhanced planner for autonomous driving. InECCV, pages 22–38, 2024.\\n[12] Zewen Chen, Juan Wang, Wen Wang, Sunhan Xu, Hang Xiong, Yun Zeng, Jian Guo, Shuxun Wang,\\nChunfeng Yuan, Bing Li, othersShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei\\nZhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-\\ninterest.arXiv:2307.03601, 2023.\\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,',\n",
       " 'interest.arXiv:2307.03601, 2023.\\n[13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\\nHao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models\\nwith model, data, and test-time scaling.arXiv:2412.05271, 2024.\\n[14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,\\nXizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic\\nvisual-linguistic tasks.arXiv:2312.14238, 2023.\\n[15] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an\\natkinson-shiffrin memory model. InECCV, pages 640–658, 2022.\\n[16] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu,\\nZichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous\\ndriving. InWACV, pages 958–979, 2024.',\n",
       " 'Zichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous\\ndriving. InWACV, pages 958–979, 2024.\\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\\nLi, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\\ninstruction tuning. InNeurIPS, 2024.\\n[18] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era, 2024.\\n[19] Google DeepMind. Gemini 2.5: Our most intelligent ai model, 2025.\\n[20] Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed\\nMian, Mohit Bansal, and Chen Chen. Motion-grounded video reasoning: Understanding and perceiving\\nmotion at pixel level.arXiv:2411.09921, 2024.\\n18',\n",
       " '[21] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: A large-scale\\nbenchmark for video segmentation with motion expressions. InICCV, pages 2694–2703, 2023.\\n[22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan\\nTompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language\\nmodel.arXiv:2303.03378, 2023.\\n[23] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified\\npixel-level vision llm for understanding, generating, segmenting, editing.arXiv:2412.19806, 2024.\\n[24] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\\nYunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark\\nof multi-modal llms in video analysis.arXiv:2405.21075, 2024.\\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong',\n",
       " 'of multi-modal llms in video analysis.arXiv:2405.21075, 2024.\\n[25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning.arXiv:2501.12948, 2025.\\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models.ICLR, 2022.\\n[27] Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, and Dae-Shik Kim. Mmr: A large-scale\\nbenchmark dataset for multi-target and multi-granularity reasoning segmentation.arXiv:2503.13881,\\n2025.\\n[28] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-\\ntemporal reasoning in visual question answering. InCVPR, pages 2758–2766, 2017.\\n[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects',\n",
       " 'temporal reasoning in visual question answering. InCVPR, pages 2758–2766, 2017.\\n[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\\nin photographs of natural scenes. InEMNLP, pages 787–798, 2014.\\n[30] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael\\nRafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-\\naction model.arXiv:2406.09246, 2024.\\n[31] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\\nSpencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. InICCV, pages 4015–4026,\\n2023.\\n[32] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning\\nsegmentation via large language model. InCVPR, pages 9579–9589, 2024.\\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,',\n",
       " 'segmentation via large language model. InCVPR, pages 9579–9589, 2024.\\n[33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer.arXiv:2408.03326, 2024.\\n[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. InICML, pages 19730–19742, 2023.\\n[35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\\nYu Qiao. Videochat: Chat-centric video understanding.arXiv:2305.06355, 2023.\\n[36] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\\nLuo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. InCVPR, pages\\n22195–22206, 2024.\\n[37] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli',\n",
       " '22195–22206, 2024.\\n[37] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli\\nZhao, and Wenbing Huang. Star-r1: Spacial transformation reasoning by reinforcing multimodal llms.\\narXiv:2505.15804, 2025.\\n[38] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\\nrepresentation by alignment before projection.arXiv:2311.10122, 2023.\\n[39] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into a\\nsingle large language model for video segmentation.arXiv:2504.07962, 2025.\\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. InECCV, pages 740–755, 2014.\\n[41] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In\\nCVPR, pages 23592–23601, 2023.\\n19',\n",
       " '[42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\\ntuning. InCVPR, pages 26296–26306, 2024.\\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. InNeurIPS,\\npages 34892–34916, 2023.\\n[44] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models\\nare effective temporal learners. InECCV, pages 1–18, 2024.\\n[45] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen.\\nr2-tuning: Efficient image-to-video transfer learning for video temporal grounding. InECCV, 2024.\\n[46] Ye Liu, Huifang Li, Chao Hu, Shuang Luo, Yan Luo, and Chang Wen Chen. Learning to aggregate\\nmulti-scale context for instance segmentation in remote sensing images.IEEE Transactions on Neural\\nNetworks and Learning Systems, 36(1):595–609, 2024.\\n[47] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal',\n",
       " 'Networks and Learning Systems, 36(1):595–609, 2024.\\n[47] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal\\ntransformers for joint video moment retrieval and highlight detection. InCVPR, pages 3042–3051, 2022.\\n[48] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: A chain-of-lora\\nagent for long video reasoning.arXiv:2503.13444, 2025.\\n[49] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang W Chen. E.t. bench: Towards\\nopen-ended event-level video-language understanding. InNeurIPS, pages 32076–32110, 2024.\\n[50] Ye Liu, Junsong Yuan, and Chang Wen Chen. Consnet: Learning consistency graph for zero-shot\\nhuman-object interaction detection. InACM MM, pages 4235–4243, 2020.\\n[51] Zongyang Ma, Yuxin Chen, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Shaojie Zhu, Chengxiang\\nZhuo, Bing Li, Ye Liu, Zang Li, Ying Shan, and Weiming Hu. Visionmath: Vision-form mathematical\\nproblem-solving. InICCV, 2025.',\n",
       " 'Zhuo, Bing Li, Ye Liu, Zang Li, Ying Shan, and Weiming Hu. Visionmath: Vision-form mathematical\\nproblem-solving. InICCV, 2025.\\n[52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and\\nvideo encoders for enhanced video understanding.arXiv:2406.09418, 2024.\\n[53] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\\ndetailed video understanding via large vision and language models.arXiv:2306.05424, 2023.\\n[54] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark\\nfor very long-form video language understanding. InNeurIPS, 2024.\\n[55] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\\nGeneration and comprehension of unambiguous object descriptions. InCVPR, pages 11–20, 2016.\\n[56] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and',\n",
       " 'Generation and comprehension of unambiguous object descriptions. InCVPR, pages 11–20, 2016.\\n[56] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and\\nSalman Khan. Videoglamm: A large multimodal model for pixel-level visual grounding in videos.\\narXiv:2411.04923, 2024.\\n[57] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas\\ndataset for semantic understanding of street scenes. InICCV, pages 4990–4999, 2017.\\n[58] OpenAI. Gpt-4v(ision) system card, 2023.\\n[59] OpenAI. Gpt-4o system card, 2024.\\n[60] OpenAI. Openai o1 system card, 2024.\\n[61] Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu,\\nZuxuan Wu, and Yu-Gang Jiang. Inst-it: Boosting multimodal instance understanding via explicit visual\\nprompt instruction tuning.arXiv:2412.03565, 2024.\\n[62] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc',\n",
       " 'prompt instruction tuning.arXiv:2412.03565, 2024.\\n[62] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc\\nVan Gool. The 2017 davis challenge on video object segmentation.arXiv:1704.00675, 2017.\\n[63] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye,\\nand Yunjie Tian. Artemis: Towards referential understanding in complex videos. InNeurIPS, 2024.\\n[64] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang,\\nAaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects.\\nInCVPR, pages 7141–7151, 2023.\\n20',\n",
       " '[65] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham\\nCholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding\\nlarge multimodal model. InCVPR, pages 13009–13018, 2024.\\n[66] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham\\nKhedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and\\nvideos.arXiv:2408.00714, 2024.\\n[67] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste\\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of context.arXiv:2403.05530, 2024.\\n[68] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large\\nlanguage model for long video understanding. InCVPR, pages 14313–14323, 2024.',\n",
       " '[68] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large\\nlanguage model for long video understanding. InCVPR, pages 14313–14323, 2024.\\n[69] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin.\\nPixellm: Pixel reasoning with large multimodal model. InCVPR, pages 26374–26383, 2024.\\n[70] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal,\\nArkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical vision transformer\\nwithout the bells-and-whistles. InICML, pages 29441–29454, 2023.\\n[71] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation\\nnetwork with a large-scale benchmark. InECCV, pages 208–223, 2020.\\n[72] Nicholas Stroh. Trackgpt–a generative pre-trained transformer for cross-domain entity trajectory forecast-\\ning.arXiv:2402.00066, 2024.',\n",
       " '[72] Nicholas Stroh. Trackgpt–a generative pre-trained transformer for cross-domain entity trajectory forecast-\\ning.arXiv:2402.00066, 2024.\\n[73] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high\\nfrequency functions in low dimensional domains. InNeurIPS, pages 7537–7547, 2020.\\n[74] OpenGVLab Team. Internvl2: Better than the best—expanding performance boundaries of open-source\\nmultimodal models with the progressive scaling strategy, 2024.\\n[75] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level\\nperception in videos via mllm. InECCV, pages 166–185, 2024.\\n[76] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios\\nGavves. Towards open-vocabulary video instance segmentation. InICCV, pages 4057–4066, 2023.',\n",
       " '[76] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, Xu Tang, Yao Hu, Weidi Xie, and Efstratios\\nGavves. Towards open-vocabulary video instance segmentation. InICCV, pages 4057–4066, 2023.\\n[77] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\\nresolution.arXiv:2409.12191, 2024.\\n[78] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-\\nto-sequence learning framework. InICML, pages 23318–23340, 2022.\\n[79] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and\\nJose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception,\\nreasoning and planning.arXiv:2405.01533, 2024.',\n",
       " 'Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception,\\nreasoning and planning.arXiv:2405.01533, 2024.\\n[80] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. Lasagna: Language-based segmentation\\nassistant for complex queries.arXiv:2404.08506, 2024.\\n[81] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. Hyperseg:\\nTowards universal visual segmentation with large language model.arXiv:2411.17606, 2024.\\n[82] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, and Yujiu Yang. Instructseg:\\nUnifying instructed visual segmentation with multi-modal large language models.arXiv:2412.14006,\\n2024.\\n[83] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: A simple\\nonline baseline for referring video object segmentation. InICCV, pages 2761–2770, 2023.\\n[84] Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, and Chang Wen Chen. A survey',\n",
       " 'online baseline for referring video object segmentation. InICCV, pages 2761–2770, 2023.\\n[84] Jianlong Wu, Wei Liu, Ye Liu, Meng Liu, Liqiang Nie, Zhouchen Lin, and Chang Wen Chen. A survey\\non video temporal grounding with multimodal large language model.arXiv:2508.10922, 2025.\\n21',\n",
       " '[85] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video\\nobject segmentation. InCVPR, pages 4974–4984, 2022.\\n[86] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Segment every reference\\nobject in spatial and temporal spaces. InICCV, pages 2538–2550, 2023.\\n[87] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized\\nsegmentation via multimodal large language models. InCVPR, pages 3858–3869, 2024.\\n[88] Linshan Xie, Xuehong Lin, Xunda Wang, Junjian Wen, Teng Ma, Jiahao Hu, Peng Cao, Alex TL Leong,\\nand Ed X Wu. Simultaneous eeg-fmri reveals spontaneous neural oscillatory activity in cingulate cortex\\nunderlying transient rsfmri network dynamics. InISMRM, 2024.\\n[89] Linshan Xie, Xunda Wang, Xuehong Lin, Teng Ma, Junjian Wen, Peng Cao, Alex TL Leong, and Ed X\\nWu. Single-pulse optogenetic perturbation of thalamo-cortical networks reveals functional architecture of',\n",
       " 'Wu. Single-pulse optogenetic perturbation of thalamo-cortical networks reveals functional architecture of\\nrsfmri networks. InISMRM, 2024.\\n[90] Linshan Xie, Xunda Wang, Xuehong Lin, Junjian Wen, Teng Ma, Alex TL Leong, and Ed X Wu.\\nBrain-wide resting-state fmri network dynamics elicited by activation of single thalamic input.Nature\\nCommunications, 2025.\\n[91] Linshan Xie, Xunda Wang, Teng Ma, Pit Shan Chong, Lee Wei Lim, Peng Cao, Pek-Lan Khong, Ed X Wu,\\nand Alex TL Leong. Are topographically segregated excitatory neurons in visual thalamus functionally\\ndiverse? an optogenetic fmri study. InISMRM, 2022.\\n[92] Linshan Xie, Xunda Wang, Teng Ma, Hang Zeng, Junjian Wen, Peng Cao, Ed X Wu, and Alex TL Leong.\\nShort single pulse optogenetic fmri mapping of downstream targets in thalamo-cortical pathways. In\\nISMRM, 2023.\\n[93] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video',\n",
       " 'ISMRM, 2023.\\n[93] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\\nquestion answering via gradually refined attention over appearance and motion. InACM MM, pages\\n1645–1653, 2017.\\n[94] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free\\nllava extension from images to videos for video dense captioning.arXiv:2404.16994, 2024.\\n[95] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\\nperception as object discovery and retrieval. InCVPR, pages 15325–15336, 2023.\\n[96] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios\\nGavves. Visa: Reasoning video object segmentation via large language models. InECCV, pages 98–115,\\n2024.\\n[97] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\\nunleashes extraordinary visual grounding in gpt-4v.arXiv:2310.11441, 2023.',\n",
       " '2024.\\n[97] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\\nunleashes extraordinary visual grounding in gpt-4v.arXiv:2310.11441, 2023.\\n[98] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao,\\nShih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity.\\narXiv:2310.07704, 2023.\\n[99] Samson Yu, Kelvin Lin, Anxing Xiao, Jiafei Duan, and Harold Soh. Octopi: Object property reasoning\\nwith large tactile-language models.arXiv:2405.02794, 2024.\\n[100] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A\\ndataset for understanding complex web videos via question answering. InAAAI, pages 9127–9134, 2019.\\n[101] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi\\nFeng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of',\n",
       " 'Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of\\nimages and videos.arXiv:2501.04001, 2025.\\n[102] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.\\nOsprey: Pixel understanding with visual instruction tuning. InCVPR, pages 28202–28211, 2024.\\n[103] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao,\\nWenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding\\nwith video llm.arXiv:2501.00599, 2024.\\n[104] Ao Zhang, Yuan Yao, Wei Ji, Zhiyuan Liu, and Tat-Seng Chua. Next-chat: An lmm for chat, detection\\nand segmentation.arXiv:2311.04498, 2023.\\n22',\n",
       " '[105] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\\nfor video understanding.arXiv:2306.02858, 2023.\\n[106] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation\\nfor autonomous vehicles. InCVPR, pages 15459–15469, 2024.\\n[107] Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi\\nZhang, and Lianwen Jin. Aesthetics is cheap, show me the text: An empirical evaluation of state-of-the-art\\ngenerative models for ocr.arXiv:2507.15085, 2025.\\n[108] Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, and Lianwen Jin. Smaller but better: Unifying\\nlayout generation with smaller large language models.International Journal of Computer Vision,\\n133:3891–3917, 2025.\\n[109] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and',\n",
       " '133:3891–3917, 2025.\\n[109] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and\\nShuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding.\\nInNeurIPS, pages 71737–71767, 2024.\\n[110] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:\\nGrounding large language models to holistic segmentation. InCVPR, pages 14227–14238, 2024.\\n[111] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat\\nwith the environment: Interactive multimodal perception using large language models. InIROS, pages\\n3590–3596, 2023.\\n[112] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, and Hengshuang Zhao. Villa: Video\\nreasoning segmentation with large language model.arXiv:2407.14500, 2024.\\n[113] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. InCVPR, pages 633–641, 2017.',\n",
       " '[113] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. InCVPR, pages 633–641, 2017.\\n[114] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web\\ninstructional videos. InAAAI, 2018.\\n[115] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models.arXiv:2304.10592, 2023.\\n[116] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl,\\nJianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. InCVPR, pages\\n15116–15127, 2023.\\n[117] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao,\\nand Yong Jae Lee. Segment everything everywhere all at once. InNeurIPS, pages 19769–19782, 2023.\\n23',\n",
       " 'YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL\\nENHANCEMENTS\\nRahima Khanam* and Muhammad Hussain\\nDepartment of Computer Science, Huddersfield University, Queensgate, Huddersfield HD1 3DH, UK;\\n*Correspondence: rahima.khanam@hud.ac.uk;\\nOctober 24, 2024\\nABSTRACT\\nThis study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only\\nLook Once) series of object detection models. We examine the models architectural innovations,\\nincluding the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object',\n",
       " 'feature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object\\ndetection (OBB). We review the model’s performance improvements in terms of mean Average\\nPrecision (mAP) and computational efficiency compared to its predecessors, with a focus on the\\ntrade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11’s\\nversatility across different model sizes, from nano to extra-large, catering to diverse application needs\\nfrom edge devices to high-performance computing environments. Our research provides insights into\\nYOLOv11’s position within the broader landscape of object detection and its potential impact on\\nreal-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction',\n",
       " 'real-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction\\nComputer vision, a rapidly advancing field, enables machines to interpret and understand visual data [ 1]. A crucial\\naspect of this domain is object detection[2], which involves the precise identification and localization of objects within\\nimages or video streams[3]. Recent years have witnessed remarkable progress in algorithmic approaches to address this\\nchallenge [4].\\nA pivotal breakthrough in object detection came with the introduction of the You Only Look Once (YOLO) algorithm\\nby Redmon et al. in 2015 [5]. This innovative approach, as its name suggests, processes the entire image in a single pass\\nto detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by',\n",
       " 'to detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by\\nframing object detection as a regression problem [5]. It employs a single convolutional neural network to simultaneously\\npredict bounding boxes and class probabilities across the entire image [6], streamlining the detection pipeline compared\\nto more complex traditional methods.\\nYOLOv11 is the latest iteration in the YOLO series, building upon the foundation established by YOLOv1. Unveiled at\\nthe YOLO Vision 2024 (YV24) conference, YOLOv11 represents a significant leap forward in real-time object detection\\ntechnology. This new version introduces substantial enhancements in both architecture and training methodologies,\\npushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail',\n",
       " 'pushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail\\ncapture while maintaining a lean parameter count. This results in improved accuracy across a diverse range of computer\\narXiv:2410.17725v1  [cs.CV]  23 Oct 2024',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nvision (CV) tasks, from object detection to classification. Furthermore, YOLOv11 achieves remarkable gains in\\nprocessing speed, substantially enhancing real-time performance capabilities.\\nIn the following sections, this paper will provide a comprehensive analysis of YOLOv11’s architecture, exploring its\\nkey components and innovations. We will examine the evolution of YOLO models, leading up to the development\\nof YOLOv11. The study will delve into the model’s expanded capabilities across various CV tasks, including object\\ndetection, instance segmentation, pose estimation, and oriented object detection. We will also review YOLOv11’s\\nperformance improvements in terms of accuracy and computational efficiency compared to its predecessors, with a\\nparticular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11',\n",
       " 'particular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11\\non real-time CV applications and its position within the broader landscape of object detection technologies.\\n2 Evolution of YOLO models\\nTable 1 illustrates the progression of YOLO models from their inception to the most recent versions. Each iteration has\\nbrought significant improvements in object detection capabilities, computational efficiency, and versatility in handling\\nvarious CV tasks.\\nTable 1: YOLO: Evolution of models\\nRelease Year Tasks Contributions Framework\\nYOLO [5] 2015 Object Detection, Basic Classifica-\\ntion\\nSingle-stage object detector Darknet\\nYOLOv2 [7] 2016 Object Detection, Improved Classi-\\nfication\\nMulti-scale training, dimension clus-\\ntering\\nDarknet\\nYOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone',\n",
       " 'YOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone\\nDarknet\\nYOLOv5 [10] 2020 Object Detection, Basic Instance\\nSegmentation (via custom modifica-\\ntions)\\nAnchor-free detection, SWISH acti-\\nvation, PANet\\nPyTorch\\nYOLOv6 [11] 2022 Object Detection, Instance Segmen-\\ntation\\nSelf-attention, anchor-free OD PyTorch\\nYOLOv7 [12] 2022 Object Detection, Object Tracking,\\nInstance Segmentation\\nTransformers, E-ELAN reparame-\\nterisation\\nPyTorch\\nYOLOv8 [13] 2023 Object Detection, Instance Segmen-\\ntation, Panoptic Segmentation, Key-\\npoint Estimation\\nGANs, anchor-free detection PyTorch\\nYOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch',\n",
       " 'YOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch\\nThis evolution showcases the rapid advancement in object detection technologies, with each version introducing novel\\nfeatures and expanding the range of supported tasks. From the original YOLO’s groundbreaking single-stage detection\\nto YOLOv10’s NMS-free training, the series has consistently pushed the boundaries of real-time object detection.\\nThe latest iteration, YOLO11, builds upon this legacy with further enhancements in feature extraction, efficiency,\\nand multi-task capabilities. Our subsequent analysis will delve into YOLO11’s architectural innovations, including\\nits improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?',\n",
       " 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach',\n",
       " 'simultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing',\n",
       " 'Building on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while',\n",
       " 'image. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nspatial attention in the feature maps. This spatial attention mechanism allows the model to focus more effectively on\\nimportant regions within the image. By pooling features spatially, the C2PSA block enables YOLO11 to concentrate on\\nspecific areas of interest, potentially improving detection accuracy for objects of varying sizes and positions.\\n4.2 Neck\\nThe neck combines features at different scales and transmits them to the head for prediction. This process typically\\ninvolves upsampling and concatenation of feature maps from different levels, enabling the model to capture multi-scale\\ninformation effectively.\\n4.2.1 C3k2 Block\\nYOLO11 introduces a significant change by replacing the C2f block in the neck with the C3k2 block. The C3k2 block\\nis designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After',\n",
       " 'is designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After\\nupsampling and concatenation, the neck in YOLO11 incorporates this improved block, resulting in enhanced speed and\\nperformance [18].\\n4.2.2 Attention Mechanism\\nA notable addition to YOLO11 is its increased focus on spatial attention through the C2PSA module. This attention\\nmechanism enables the model to concentrate on key regions within the image, potentially leading to more accurate\\ndetection, especially for smaller or partially occluded objects. The inclusion of C2PSA sets YOLO11 apart from its\\npredecessor, YOLOv8, which lacks this specific attention mechanism [18].\\n4.3 Head\\nThe head of YOLOv11 is responsible for generating the final predictions in terms of object detection and classification.\\nIt processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block',\n",
       " 'It processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block\\nIn the head section, YOLOv11 utilizes multiple C3k2 blocks to efficiently process and refine the feature maps. The\\nC3k2 blocks are placed in several pathways within the head, functioning to process multi-scale features at different\\ndepths. The C3k2 block exhibits flexibility depending on the value of the c3k parameter:\\n• When c3k = False, the C3k2 module behaves similarly to the C2f block, utilizing a standard bottleneck\\nstructure.\\n• When c3k = True, the bottleneck structure is replaced by the C3 module, which allows for deeper and more\\ncomplex feature extraction.\\nKey characteristics of the C3k2 block:\\n• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.',\n",
       " '• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.\\n• Parameter efficiency: C3k2 is a more compact version of the CSP bottleneck, making the architecture more\\nefficient in terms of the number of trainable parameters.\\nAnother notable addition is the C3k block, which offers enhanced flexibility by allowing customizable kernel sizes. The\\nadaptability of C3k is particularly useful for extracting more detailed features from images, contributing to improved\\ndetection accuracy.\\n4.3.2 CBS Blocks\\nThe head of YOLOv11 includes several CBS (Convolution-BatchNorm-Silu) [19] layers after the C3k2 blocks. These\\nlayers further refine the feature maps by:\\n• Extracting relevant features for accurate object detection.\\n• Stabilizing and normalizing the data flow through batch normalization.\\n4',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n• Utilizing the Sigmoid Linear Unit (SiLU) activation function for non-linearity, which improves model perfor-\\nmance.\\nCBS blocks serve as foundational components in both feature extraction and the detection process, ensuring that the\\nrefined feature maps are passed to the subsequent layers for bounding box and classification predictions.\\n4.3.3 Final Convolutional Layers and Detect Layer\\nEach detection branch ends with a set of Conv2D layers, which reduce the features to the required number of outputs for\\nbounding box coordinates and class predictions. The final Detect layer consolidates these predictions, which include:\\n• Bounding box coordinates for localizing objects in the image.\\n• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11',\n",
       " '• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11\\nYOLO11 supports a diverse range of CV tasks, showcasing its versatility and power in various applications. Here’s an\\noverview of the key tasks:\\n1. Object Detection: YOLO11 excels in identifying and localizing objects within images or video frames,\\nproviding bounding boxes for each detected item [ 20]. This capability finds applications in surveillance\\nsystems, autonomous vehicles, and retail analytics, where precise object identification is crucial [21].\\n2. Instance Segmentation: Going beyond simple detection, YOLO11 can identify and separate individual\\nobjects within an image down to the pixel level [20]. This fine-grained segmentation is particularly valuable in\\nmedical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].',\n",
       " 'medical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].\\n3. Image Classification: YOLOv11 is capable of classifying entire images into predetermined categories,\\nmaking it ideal for applications like product categorization in e-commerce platforms or wildlife monitoring in\\necological studies [21].\\n4. Pose Estimation: The model can detect specific key points within images or video frames to track movements\\nor poses. This capability is beneficial for fitness tracking applications, sports performance analysis, and various\\nhealthcare applications requiring motion assessment [21].\\n5. Oriented Object Detection (OBB): YOLO11 introduces the ability to detect objects with an orientation angle,\\nallowing for more precise localization of rotated objects. This feature is especially valuable in aerial imagery\\nanalysis, robotics, and warehouse automation tasks where object orientation is crucial [21].',\n",
       " 'analysis, robotics, and warehouse automation tasks where object orientation is crucial [21].\\n6. Object Tracking: It identifies and traces the path of objects in a sequence of images or video frames[ 21].\\nThis real-time tracking capability is essential for applications such as traffic monitoring, sports analysis, and\\nsecurity systems.\\nTable 2 outlines the YOLOv11 model variants and their corresponding tasks. Each variant is designed for specific\\nuse cases, from object detection to pose estimation. Moreover, all variants support core functionalities like inference,\\nvalidation, training, and export, making YOLOv11 a versatile tool for various CV applications.\\n6 Advancements and Key Features of YOLOv11\\nYOLOv11 represents a significant advancement in object detection technology, building upon the foundations laid by\\nits predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics',\n",
       " 'its predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics\\nshowcases enhanced architectural designs, more sophisticated feature extraction techniques, and refined training\\nmethodologies. The synergy of YOLOv11’s rapid processing, high accuracy, and computational efficiency positions it\\nas one of the most formidable models in Ultralytics’ portfolio to date [22]. A key strength of YOLOv11 lies in its refined\\narchitecture, which facilitates the detection of subtle details even in challenging scenarios. The model’s improved\\nfeature extraction capabilities allow it to identify and process a broader range of patterns and intricate elements within\\nimages. Compared to earlier versions, YOLOv11 introduces several notable enhancements:\\n5',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nTable 2: YOLOv11 Model Variants and Tasks\\nModel Variants Task Inference Validation Training Export\\nYOLOv11 yolo11-nano yolo11-small\\nyolo11-medium yolo11-\\nlarge yolo11-xlarge\\nDetection ✓ ✓ ✓ ✓\\nYOLOv11-seg yolo11-nano-seg yolo11-\\nsmall-seg yolo11-medium-\\nseg yolo11-large-seg\\nyolo11-xlarge-seg\\nInstance Segmen-\\ntation\\n✓ ✓ ✓ ✓\\nYOLOv11-pose yolo11-nano-pose yolo11-\\nsmall-pose yolo11-medium-\\npose yolo11-large-pose\\nyolo11-xlarge-pose\\nPose/Keypoints ✓ ✓ ✓ ✓\\nYOLOv11-obb yolo11-nano-obb yolo11-\\nsmall-obb yolo1-medium-\\nobb yolo11-large-obb\\nyolo11-xlarge-obb\\nOriented Detec-\\ntion\\n✓ ✓ ✓ ✓\\nYOLOv11-cls yolo11-nano-cls yolo11-\\nsmall-cls yolo11-medium-\\ncls yolo11-large-cls yolo11-\\nxlarge-cls\\nClassification ✓ ✓ ✓ ✓\\n1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m',\n",
       " '1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m\\ncounterpart, demonstrating improved computational efficiency without compromising accuracy [23].\\n2. Versatility in CV tasks: YOLOv11 exhibits proficiency across a diverse array of CV applications, including\\npose estimation, object recognition, image classification, instance segmentation, and oriented bounding box\\n(OBB) detection [23].\\n3. Optimized speed and performance: Through refined architectural designs and streamlined training pipelines,\\nYOLOv11 achieves faster processing speeds while maintaining a balance between accuracy and computational\\nefficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].',\n",
       " 'efficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].\\n5. Advanced feature extraction: YOLOv11 incorporates improvements in both its backbone and neck architec-\\ntures, resulting in enhanced feature extraction capabilities and, consequently, more precise object detection\\n[23].\\n6. Contextual adaptability: YOLOv11 demonstrates versatility across various deployment scenarios, including\\ncloud platforms, edge devices, and systems optimized for NVIDIA GPUs [23].\\nYOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11',\n",
       " 'such as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].\\nThe performance comparison graph depicted in Figure 2 overs several key insights. The YOLOv11 variants (11n, 11s,\\n11m, and 11x) form a distinct performance frontier, with each model achieving higher COCO mAP 50−95 scores at\\ntheir respective latency points. Notably, the YOLOv11x achieves approximately 54.5% mAP50−95 at 13ms latency,\\nsurpassing all previous YOLO iterations. The intermediate variants, particularly YOLOv11m, demonstrate exceptional\\nefficiency by achieving comparable accuracy to larger models from previous generations while requiring significantly\\nless processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s',\n",
       " 'less processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s\\nmaintains high accuracy (approximately 47% mAP50−95) while operating at speeds previously associated with much\\nless accurate models. This represents a crucial advancement for real-time applications where both speed and accuracy\\nare critical. The improvement curve of YOLOv11 also shows better scaling characteristics across its model variants,\\nsuggesting more efficient utilization of additional computational resources compared to previous generations.\\n6',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nFigure 2: Benchmarking YOLOv11 Against Previous Versions [23]\\n7 Discussion\\nYOLO11 marks a significant leap forward in object detection technology, building upon its predecessors while\\nintroducing innovative enhancements. This latest iteration demonstrates remarkable versatility and efficiency across\\nvarious CV tasks.\\n1. Efficiency and Scalability: YOLO11 introduces a range of model sizes, from nano to extra-large, catering\\nto diverse application needs. This scalability allows for deployment in scenarios ranging from resource-\\nconstrained edge devices to high-performance computing environments. The nano variant, in particular,\\nshowcases impressive speed and efficiency improvements over its predecessor, making it ideal for real-time\\napplications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature',\n",
       " 'applications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature\\nextraction and processing capabilities. The incorporation of novel elements such as the C3k2 block, SPPF, and\\nC2PSA contributes to more effective feature extraction and processing. These enhancements allow the model\\nto better analyze and interpret complex visual information, potentially leading to improved detection accuracy\\nacross various scenarios.\\n3. Multi-Task Proficiency: YOLO11’s versatility extends beyond object detection, encompassing tasks such as\\ninstance segmentation, image classification, pose estimation, and oriented object detection. This multi-faceted\\napproach positions YOLO11 as a comprehensive solution for diverse CV challenges.\\n4. Enhanced Attention Mechanisms: A key advancement in YOLO11 is the integration of sophisticated spatial\\nattention mechanisms, particularly the C2PSA component. This feature enables the model to focus more',\n",
       " 'attention mechanisms, particularly the C2PSA component. This feature enables the model to focus more\\neffectively on critical regions within an image, enhancing its ability to detect and analyze objects. The\\nimproved attention capability is especially beneficial for identifying complex or partially occluded objects,\\naddressing a common challenge in object detection tasks. This refinement in spatial awareness contributes to\\nYOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.',\n",
       " 'speed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.\\n6. Implications for Real-World Applications: The advancements in YOLO11 have significant implications\\nfor various industries. Its improved efficiency and multi-task capabilities make it particularly suitable for\\napplications in autonomous vehicles, surveillance systems, and industrial automation. The model’s ability to\\nperform well across different scales also opens up new possibilities for deployment in resource-constrained\\nenvironments without compromising on performance.\\n7',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n8 Conclusion\\nYOLOv11 represents a significant advancement in the field of CV , offering a compelling combination of enhanced\\nperformance and versatility. This latest iteration of the YOLO architecture demonstrates marked improvements in\\naccuracy and processing speed, while simultaneously reducing the number of parameters required. Such optimizations\\nmake YOLOv11 particularly well-suited for a wide range of applications, from edge computing to cloud-based analysis.\\nThe model’s adaptability across various tasks, including object detection, instance segmentation, and pose estimation,\\npositions it as a valuable tool for diverse industries such as emotion detection [26], healthcare [27] and various other\\nindustries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses',\n",
       " 'industries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses\\nseeking to implement or upgrade their CV systems. In summary, YOLOv11’s blend of enhanced feature extraction,\\noptimized performance, and broad task support establishes it as a formidable solution for addressing complex visual\\nrecognition challenges in both research and practical applications.\\nReferences\\n[1] Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image processing, analysis and machine vision. Springer, 2013.\\n[2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\\nProceedings of the IEEE, 111(3):257–276, 2023.\\n[3] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review.\\nIEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-',\n",
       " 'IEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-\\nvoltaic defect detection. In Solar, volume 4, pages 351–386. MDPI, 2024.\\n[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,\\n2016.\\n[6] Juan Du. Understanding of object detection based on cnn family and yolo. In Journal of Physics: Conference\\nSeries, volume 1004, page 012029. IOP Publishing, 2018.\\n[7] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.',\n",
       " 'computer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\\n[10] Roboflow Blog Jacob Solawetz. What is yolov5? a guide for beginners., 2020. Accessed: 21 October 2024.\\n[11] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint\\narXiv:2209.02976, 2022.\\n[12] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new\\nstate-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.',\n",
       " 'state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.\\n[13] Francesco Jacob Solawetz. What is yolov8? the ultimate guide, 2023. Accessed: 21 October 2024.\\n[14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using\\nprogrammable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time\\nend-to-end object detection. arXiv preprint arXiv:2405.14458, 2024.\\n[16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024.\\n[17] Rahima Khanam, Muhammad Hussain, Richard Hill, and Paul Allen. A comprehensive review of convolutional\\nneural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.',\n",
       " 'neural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.\\n[19] Jingwen Feng, Qiaofeng An, Jiahao Zhang, Shuxun Zhou, Guangwei Du, and Kai Yang. Application of yolov7-tiny\\nin the detection of steel surface defects. In 2024 5th International Seminar on Artificial Intelligence, Networking\\nand Information Technology (AINIT), pages 2241–2245. IEEE, 2024.\\n8',\n",
       " 'R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n[20] Ultralytics. Instance segmentation and tracking, 2024. Accessed: 2024-10-21.\\n[21] Ultralytics Abirami Vina. Ultralytics yolo11 has arrived: Redefine what’s possible in ai, 2024. Accessed:\\n2024-10-21.\\n[22] Viso.AI Gaudenz Boesch. Yolov11: A new iteration of “you only look once. https://viso.ai/\\ncomputer-vision/yolov11/, 2024. Accessed: 2024-10-21.\\n[23] Ultralytics. Ultralytics yolov11. https://docs.ultralytics.com/models/yolo11/s, 2024. Accessed:\\n21-Oct-2024.\\n[24] Rahima Khanam and Muhammad Hussain. What is yolov5: A deep look into the internal features of the popular\\nobject detector. arXiv preprint arXiv:2407.20892, 2024.\\n[25] DigitalOcean. What’s new in yolov11 transforming object detection once again part 1, 2024. Accessed: 2024-10-\\n21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.',\n",
       " '21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.\\nIn Kids Cybersecurity Using Computational Intelligence Techniques, pages 165–174. Springer, 2023.\\n[27] Burcu Ataer Aydin, Muhammad Hussain, Richard Hill, and Hussain Al-Aqrabi. Domain modelling for a\\nlightweight convolutional network focused on automated exudate detection in retinal fundus images. In 2023 9th\\nInternational Conference on Information Technology Trends (ITT), pages 145–150. IEEE, 2023.\\n9']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [doc.page_content for doc in chunks]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08d1dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 147 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 147 documents to vector store...\n",
      "Successfully added 147 documents to vector store\n",
      "Total documents in collection: 2205\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_manager.generate_embeddings(text)\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2910a62",
   "metadata": {},
   "source": [
    "Retreival Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "378da086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetreivalPipeline:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingVectorDB):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self,query:str,top_k:int=5,score_threshold:float=0.0)->List[Dict[str,Any]]:\n",
    "        # Generate embedding for the query\n",
    "        print(f\"Generating embedding for query: {query}\")\n",
    "        print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()], \n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            retreive_docs = []  \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                 documents = results['documents'][0]\n",
    "                 metadatas = results['metadatas'][0]\n",
    "                 distances = results['distances'][0]\n",
    "                 ids = results['ids'][0]\n",
    "\n",
    "                 for i, (doc_id, document, metadata, distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retreive_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                    print(f\"Retrieved {len(retreive_docs)} documents above the score threshold.\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retreive_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "        \n",
    "retreival_pipeline = RetreivalPipeline(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac2e52d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RetreivalPipeline at 0x738d2a4a4500>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreival_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f41b0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is yolov11 ?\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 documents above the score threshold.\n",
      "Retrieved 2 documents above the score threshold.\n",
      "Retrieved 3 documents above the score threshold.\n",
      "Retrieved 4 documents above the score threshold.\n",
      "Retrieved 5 documents above the score threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_5d894304_116',\n",
       "  'content': 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       "  'metadata': {'source': '../data/pdf/2410.17725v1.pdf',\n",
       "   'doc_index': 116,\n",
       "   'moddate': '2024-10-24T00:37:53+00:00',\n",
       "   'keywords': '',\n",
       "   'page_label': '2',\n",
       "   'content_length': 755,\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': '2410.17725v1.pdf',\n",
       "   'total_pages': 9,\n",
       "   'creationdate': '2024-10-24T00:37:53+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'subject': '',\n",
       "   'page': 1,\n",
       "   'author': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'title': ''},\n",
       "  'similarity_score': 0.3190048933029175,\n",
       "  'distance': 0.6809951066970825,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_ac122705_116',\n",
       "  'content': 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       "  'metadata': {'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source_file': '2410.17725v1.pdf',\n",
       "   'total_pages': 9,\n",
       "   'creationdate': '2024-10-24T00:37:53+00:00',\n",
       "   'source': '../data/pdf/2410.17725v1.pdf',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2024-10-24T00:37:53+00:00',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'title': '',\n",
       "   'content_length': 755,\n",
       "   'page': 1,\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'doc_index': 116,\n",
       "   'page_label': '2'},\n",
       "  'similarity_score': 0.3190048933029175,\n",
       "  'distance': 0.6809951066970825,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_22afb6b5_116',\n",
       "  'content': 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'doc_index': 116,\n",
       "   'content_length': 755,\n",
       "   'moddate': '2024-10-24T00:37:53+00:00',\n",
       "   'trapped': '/False',\n",
       "   'creationdate': '2024-10-24T00:37:53+00:00',\n",
       "   'subject': '',\n",
       "   'page': 1,\n",
       "   'author': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'title': '',\n",
       "   'source': '../data/pdf/2410.17725v1.pdf',\n",
       "   'source_file': '2410.17725v1.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 9,\n",
       "   'page_label': '2'},\n",
       "  'similarity_score': 0.3190048933029175,\n",
       "  'distance': 0.6809951066970825,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_09293bf5_116',\n",
       "  'content': 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       "  'metadata': {'source': '../data/pdf/2410.17725v1.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'content_length': 755,\n",
       "   'page_label': '2',\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'title': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'page': 1,\n",
       "   'keywords': '',\n",
       "   'total_pages': 9,\n",
       "   'doc_index': 116,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'moddate': '2024-10-24T00:37:53+00:00',\n",
       "   'source_file': '2410.17725v1.pdf',\n",
       "   'subject': '',\n",
       "   'creationdate': '2024-10-24T00:37:53+00:00'},\n",
       "  'similarity_score': 0.3190048933029175,\n",
       "  'distance': 0.6809951066970825,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_7ca87b98_116',\n",
       "  'content': 'its improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2',\n",
       "  'metadata': {'page': 1,\n",
       "   'page_label': '2',\n",
       "   'trapped': '/False',\n",
       "   'source': '../data/pdf/2410.17725v1.pdf',\n",
       "   'title': '',\n",
       "   'doc_index': 116,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'moddate': '2024-10-24T00:37:53+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'creationdate': '2024-10-24T00:37:53+00:00',\n",
       "   'content_length': 755,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'total_pages': 9,\n",
       "   'source_file': '2410.17725v1.pdf'},\n",
       "  'similarity_score': 0.3190048933029175,\n",
       "  'distance': 0.6809951066970825,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreival_pipeline.retrieve(\"What is yolov11 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5ca1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query: What is Visual Referring and Segmentation ?\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 documents above the score threshold.\n",
      "Retrieved 2 documents above the score threshold.\n",
      "Retrieved 3 documents above the score threshold.\n",
      "Retrieved 4 documents above the score threshold.\n",
      "Retrieved 5 documents above the score threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_671c3c9b_51',\n",
       "  'content': 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       "  'metadata': {'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning',\n",
       "   'trapped': '/False',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2509.18094',\n",
       "   'total_pages': 23,\n",
       "   'creationdate': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       "   'arxivid': 'https://arxiv.org/abs/2509.18094v3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'content_length': 651,\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '10',\n",
       "   'doc_index': 51,\n",
       "   'page': 9,\n",
       "   'source_file': '2509.18094v3.pdf',\n",
       "   'source': '../data/pdf/2509.18094v3.pdf'},\n",
       "  'similarity_score': 0.23901617527008057,\n",
       "  'distance': 0.7609838247299194,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_9929fda0_51',\n",
       "  'content': 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       "  'metadata': {'source_file': '2509.18094v3.pdf',\n",
       "   'trapped': '/False',\n",
       "   'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen',\n",
       "   'creationdate': '',\n",
       "   'doc_index': 51,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       "   'arxivid': 'https://arxiv.org/abs/2509.18094v3',\n",
       "   'page_label': '10',\n",
       "   'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 9,\n",
       "   'total_pages': 23,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2509.18094',\n",
       "   'source': '../data/pdf/2509.18094v3.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1',\n",
       "   'content_length': 651},\n",
       "  'similarity_score': 0.23901617527008057,\n",
       "  'distance': 0.7609838247299194,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_c3e1ff02_51',\n",
       "  'content': 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       "  'metadata': {'content_length': 651,\n",
       "   'source_file': '2509.18094v3.pdf',\n",
       "   'source': '../data/pdf/2509.18094v3.pdf',\n",
       "   'doc_index': 51,\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 23,\n",
       "   'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       "   'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen',\n",
       "   'page': 9,\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'creationdate': '',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1',\n",
       "   'arxivid': 'https://arxiv.org/abs/2509.18094v3',\n",
       "   'page_label': '10',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2509.18094'},\n",
       "  'similarity_score': 0.23901617527008057,\n",
       "  'distance': 0.7609838247299194,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_55ad9a59_51',\n",
       "  'content': 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       "  'metadata': {'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2509.18094',\n",
       "   'page_label': '10',\n",
       "   'source': '../data/pdf/2509.18094v3.pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'creationdate': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen',\n",
       "   'source_file': '2509.18094v3.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2509.18094v3',\n",
       "   'trapped': '/False',\n",
       "   'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning',\n",
       "   'page': 9,\n",
       "   'content_length': 651,\n",
       "   'total_pages': 23,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 51},\n",
       "  'similarity_score': 0.23901617527008057,\n",
       "  'distance': 0.7609838247299194,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_fa4cd5ae_51',\n",
       "  'content': 'referring and segmentation can be jointly enhanced. Extensive experiments on diverse pixel-level\\nunderstanding tasks, including thePixelQAtask, demonstrate the significance of the proposed\\nmethod. We hope this work inspires future advancements in pixel-level visual understanding.\\nAcknowledgements\\nThis study was supported by The Hong Kong RGC Grant (15229423) and a financial support from\\nARC Lab, Tencent PCG (ZGG9). We also acknowledge The University Research Facility in Big Data\\nAnalytics (UBDA) at The Hong Kong Polytechnic University for providing computing resources that\\nhave contributed to the research results reported within this paper.\\n10',\n",
       "  'metadata': {'author': 'Ye Liu; Zongyang Ma; Junfu Pu; Zhongang Qi; Yang Wu; Ying Shan; Chang Wen Chen',\n",
       "   'content_length': 651,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'creationdate': '',\n",
       "   'page': 9,\n",
       "   'source_file': '2509.18094v3.pdf',\n",
       "   'page_label': '10',\n",
       "   'source': '../data/pdf/2509.18094v3.pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'title': 'UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning',\n",
       "   'license': 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       "   'arxivid': 'https://arxiv.org/abs/2509.18094v3',\n",
       "   'total_pages': 23,\n",
       "   'doc_index': 51,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2509.18094',\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.23901617527008057,\n",
       "  'distance': 0.7609838247299194,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreival_pipeline.retrieve(\"What is Visual Referring and Segmentation ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc009a",
   "metadata": {},
   "source": [
    "Adding LLM with rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "afcc5eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv(\"groq_secret_key\")\n",
    "print(os.getenv(\"groq_secret_key\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3112ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "32862bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqAILLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.1-70b-versatile\", api_key: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"groq_secret_key\")\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"OpenAI API key not found. Please set the 'groq_secret_key' environment variable.\"\n",
    "            )\n",
    "\n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        print(f\"Initialized ChatGroq with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "            template=\"\"\"\n",
    "You are a helpful AI assistant. Use the following context to answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        formatted_prompt = prompt_template.format(\n",
    "            context=context,\n",
    "            query=query\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"I'm sorry, I couldn't generate a response.\"\n",
    "\n",
    "    def generate_simple_response(self, query: str, context: str) -> str:\n",
    "        simple_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"Error occurred.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cb949670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ChatGroq with model: llama-3.1-70b-versatile\n",
      "Successfully initialized ChatGroq model.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chat_model = GroqAILLM(api_key=os.getenv(\"groq_secret_key\"))\n",
    "    print(\"Successfully initialized ChatGroq model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChatOpenAI: {e}\")\n",
    "    chat_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba83a83",
   "metadata": {},
   "source": [
    "Integrating VectorDb context pipeline with llm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2f36bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"groq_secret_key\")\n",
    "# Initialize LLM\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-70b-versatile\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    # Get relevant documents\n",
    "    results = retriever.retrieve(\n",
    "        query,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt = f\"\"\"\n",
    "     Use the following context to answer the question concisely.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
